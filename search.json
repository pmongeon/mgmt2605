[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Working with data",
    "section": "",
    "text": "Course overview\nThis book is a work-in-progress intended to support the course MGMT2605 - Working with Data offered at the Faculty of Management of Dalhousie University. The course introduces this knowledge and skillset through lectures and hands-on labs. Students will learn to use a variety of tools to find, manage, assess, extract value from, and visualize raw data.",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "ch2.html",
    "href": "ch2.html",
    "title": "2  Data structures",
    "section": "",
    "text": "2.1 From unstructured to structured data\nData can take many forms that we can situate along a continuum with unstructured data at one extreme, structured data at the other, and semi-structured data in between. In this section, we briefly explore what these concepts mean.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "ch3.html",
    "href": "ch3.html",
    "title": "3  Data Types",
    "section": "",
    "text": "3.1 Why data types matter\nStorage efficiency: This is not going to be a concern in the context of this course, but data, like any other objects stored on a computer, takes up memory space. Typically, numbers (especially small numbers) take up less memory space than text, for instance. When dealing with very large databases that support, for example, the operations of a large multinational organization, or a data intensive organization like Meta, using a sub-optimal data type for a column in the data can have significant implications for the data storage costs.\nData integrity: Because Excel is a very flexible tool, it does not do a good job at ensuring the integrity of the data. For example, a column can contain both text, numbers, and dates. For example, if you have a column called age and set the data type to number, you will still be able to enter the text Banana in that column, and Excel will automatically convert that cell, and only that cell, to the general (undefined)data type instead of throwing an error (see example below).\nSo when working with Excel documents, it is a good idea to explore the data set and try to spot and fix errors. It fact it is always a good idea to check the integrity of the data no matter what software is used to access it. Has the saying goes: garbage in, garbage out. We certainly don’t want to make important business or life decision based on erroneous data, as these errors depending on their frequency and nature, can drastically affect the results of analysis and have lead to decisions with devastating effects.\nData analysis: Data types have important implications for analysis, because you cannot perform the same operations on text and on numbers. When we learn about data analysis and data visualization later in the course, we will see that the data type is the main factor that determines how we can analyze or visualize the data to gain insights from it. For now, it is enough to understand that some operations are possible (or logical) for some data types but not for others.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "4  Collecting data",
    "section": "",
    "text": "Coming soon…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Collecting data</span>"
    ]
  },
  {
    "objectID": "ch8.html",
    "href": "ch8.html",
    "title": "8  Summarizing numerical data",
    "section": "",
    "text": "Coming soon…",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing numerical data</span>"
    ]
  },
  {
    "objectID": "ch9.html",
    "href": "ch9.html",
    "title": "9  Measuring the relationship between two variables",
    "section": "",
    "text": "Coming soon…",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Measuring the relationship between two variables</span>"
    ]
  },
  {
    "objectID": "ch11.html",
    "href": "ch11.html",
    "title": "11  Visualizing single variables",
    "section": "",
    "text": "Coming soon…",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Visualizing single variables</span>"
    ]
  },
  {
    "objectID": "ch12.html",
    "href": "ch12.html",
    "title": "12  Visualizing multiple variables",
    "section": "",
    "text": "Coming soon…",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualizing multiple variables</span>"
    ]
  },
  {
    "objectID": "ch7.html",
    "href": "ch7.html",
    "title": "7  Summarizing textual data",
    "section": "",
    "text": "Coming soon…",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing textual data</span>"
    ]
  },
  {
    "objectID": "ch10.html",
    "href": "ch10.html",
    "title": "10  Open research",
    "section": "",
    "text": "10.1 Introduction\nThis chapter introduces the concept of Open Access (OA) as well as data sources that can be used to search for OA literature or get data on the OA status of research publications. The content of the chapter will be complemented by an in-class presentation on scholarly publishing and open access.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open research</span>"
    ]
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "1  What is data",
    "section": "",
    "text": "1.1 Learning objectives",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data</span>"
    ]
  },
  {
    "objectID": "ch1.html#slides",
    "href": "ch1.html#slides",
    "title": "1  Bibliometrics and scholarly communication: An introduction",
    "section": "1.2 Slides",
    "text": "1.2 Slides\nSlides will be posted right before the first class begins.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bibliometrics and scholarly communication: An introduction</span>"
    ]
  },
  {
    "objectID": "ch1.html#homework",
    "href": "ch1.html#homework",
    "title": "1  Bibliometrics and scholarly communication: An introduction",
    "section": "1.3 Homework",
    "text": "1.3 Homework\nYour homework is to explore the bibliometrics literature to get a better sense of what the field is about, and then choose an article to review for assignment #1 (the detailed instructions are on Brightspace).\nQuantitative Science Studies, the official journal of the International Society for Scientometrics and Informetrics (ISSI) may be a good place to start for this task, and it is an open access journal so you will be able to access the articles by using the link provided here. There are other sources mentioned in the assignment description on Brightspace (eg. Scientometrics, Journal of Informetrics). You can browse the title and abstracts of the papers in these journals but you might need to go through the Dalhousie library websites to access to the full text of the articles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bibliometrics and scholarly communication: An introduction</span>"
    ]
  },
  {
    "objectID": "ch2.html#mertonian-framework",
    "href": "ch2.html#mertonian-framework",
    "title": "2  The social structure of science",
    "section": "2.2 Mertonian framework",
    "text": "2.2 Mertonian framework\nMerton was a highly influential American sociologist who is said to have essentially created the modern sociology of science. For Merton, the institutional goal of science is the extension of certified knowledge, and the ethos of science comprises that goal and a set of norms that ensure its fulfillment. The following sections provide an overview of the part of his work, as well as the work of some of his students and close collaborators, through which they sought to understand the conditions under which science as a social system operates in accordance with the ethos of science and sometimes deviate from it.\n\n2.2.1 The norms of science\nMerton (1942) defined four norms of science which are binding to all scientists:\n\nCommunism: Science is a common good. This emphasizes the importance of the dissemination of discoveries, which is necessary for their inclusion in the common stock of knowledge.\nUniversalism: Contributions to the advancement of knowledge need to be judged for their own merit irrespective of the characteristics of the individuals or groups involved. Similarly, access to scientific careers must be based on relevant criteria and not sociodemographic characteristics or other irrelevant factors.\nDisinterestedness refers to the pursuit of knowledge for its own sake and for the benefit of humankind, and not for personal gains. Scientists are characterized by a passion for knowledge, curiosity, and the desire to ameliorate the human condition. Merton emphasizes that if scientists, as a group, disproportionally exhibit these characteristics, it’s because the scientific system rewards disinterestedness and not so much because naturally disinterested people decide to become scientists.\nOrganized skepticism:This norm operates at two levels. First, the institution of science exercises a collective form of skepticism towards beliefs and truth claims and seeks to subject them to thorough examination and empirical validation. Second, every scientific contribution must be subjected to the unbiased scrutiny of peers before being accepted as such and incorporated into the common stock of scientific knowledge.\n\n\n\n2.2.2 Reward system of science\nBecause of the norms of communism and disinterestedness, the motivation of researchers is the recognition they receive from their peers (Zuckerman 1977a). As J. R. Cole and Cole (1973) put it:\n\nBecause recognition is so important to scientists, there must be a reward system that identifies and honors scientific excellence wherever it is found. If a scientist desires to acquire “property”, he can only do so through recognition by the system, since there are no other legitimate ways to obtain property in science. (p. 46)\n\nBecause the role of researchers is to advance knowledge, the scientific community gradually developed a reward system through which those who best fulfill this role are compensated (Merton 1957). This system works well when those who deserve recognition receive it, and the most promising researchers are provided with the resources they need to realize their potential. According to Merton (1973) this benefits both the individual researchers and science as a whole.\nThe reward system comprises many reward mechanisms that form a hierarchy. For Merton (1957), the most permanent and prestigious form of institutionalized recognition in science is eponymic reward: the practice of attaching a researcher’s name to a discovery (e.g., Zipf law, Planck’s constant, or Copernican system), a field (Comte, father of sociology), or a period (e.g., the Freudian era, the Darwinian era). Other prestigious forms of reward include honorific awards like the Nobel prize, membership to academies of science, or other nobility titles. Those who receive such prestigious titles are often seen as the scientific elite and are at the top of the stratified social structure of science (J. R. Cole and Cole 1973).\nBecause there are limited spots at the top of the hierarchy, there are many researchers whose contributions may be just as extraordinary (if not more) than those of Nobel prize winners but are not awarded such honours (Merton 1968). The scientific community thus creates less prestigious awards and prizes to highlight these accomplishments. However, these are still few in number and only bestowed upon a minority of researchers (Zuckerman 1977b; S. Cole and Cole 1967). So again, there are other mechanisms to reward accomplished scientists, such as nomination to important positions in scientific institutions or as editors of scholarly journals. At the bottom of the hierarchy, visibility is the most basic form of scientific reward: being published and cited by peers (J. R. Cole and Cole 1973).\n\n\n2.2.3 Social stratification\nThe social stratification of science is a systematic effect of the differentiation and evaluation of researchers who are positioned in a structure that determines their access to resources and opportunities Merton (1968) based on their recognized contributions or potential. Prestigious institutions with more resources recruit Individuals with recognized potential which confers to these individuals (and to the institutions) a competitive advantage. And so does the stratified social structure of science take form through a mixture of self and social selection (Merton 1979). Through their contributions to science, the researchers will then accumulate power and authority, maintain or grow their advantage, and contribute to the asymmetric distribution of resources, productivity, visibility and prestige (J. R. Cole and Cole 1973).\n\n\n2.2.4 Accumulation of advantages and disadvantages\nThe social stratification of science can generate the accumulation of advantages and disadvantages by scientists, thus increasing the gap between the haves and the have-nots (Zuckerman 1998). Identifying promising researchers and providing them with a certain advantage has a short and long-term effect on their careers. When it is working optimally, the divide between the haves and the have-nots grows exponentially. The best performing researchers obtaining more resources, which they use to perform even better and in return obtain even more resources, and so on. According to Zuckerman (1998), the system would become dysfunctional if resource allocation were based on criteria that don’t relate to performance (e.g., gender, religion, ethnicity) or if resources were equally or randomly distributed. Overall, the accumulation of advantages and disadvantages contributes to the social stratification of science which, according to Zuckerman (1998), is essential for the optimization of the scientific system and the advancement of knowledge.\n\n2.2.4.1 Matthew effect\nThe value of a scientific contribution and the recognition that the researchers obtain in return is inevitably based on the subjective judgment of their peers, which can introduce some dysfunctions in the reward system, such as what Merton (1968) called the Matthew Effect, defined as “the accruing of greater increments of recognition for particular scientific contributions to scientists of considerable repute and the withholding of such recognition from scientists who have not yet made their mark” (p. 53). According to J. R. Cole and Cole (1973), the Matthew effect is a direct effect of the social stratification of science, which amplifies the accumulation of advantages and disadvantages by overestimating the merit of some researchers at the detriment of others. The Matthew effect occurs at many levels, such as the recognition of one’s accomplishments, in peer-review, awards, research funding and so on.\n\n\n2.2.4.2 Mathilda effect\nRossiter (1993) showed three decades ago that women in science tended to be disadvantaged by being deprived partly or totally of recognition for their scientific contributions. Like the Matthew effect, this violates the norm of universalism. Some of the most significant scientific discoveries made by women led to their male collaborators winning the Nobel Prize. Even if today the gap between men and women in science has narrowed, women remain less likely then men to obtain awards or important positions Moss-Racusin et al. (2012) and tend to be less cited (Larivière et al. 2013). It should be acknowledged here that these theories were developed before the generalized recognition of gender as a non-binary concept, and that the disadvantages discussed here have been shown to apply to minorities and equity-deserving groups more broadly. I should also mention here that still to this day, bibliometric studies tend to to operationalize gender as a binary concept, which is mostly due to the fact that bibliographic databases do not usually provide the gender of authors, and we must then rely on other available data like first names and geographical location to guess the gender of authors."
  },
  {
    "objectID": "ch2.html#bourdieusian-framework",
    "href": "ch2.html#bourdieusian-framework",
    "title": "2  The social structure of science",
    "section": "2.3 Bourdieusian framework",
    "text": "2.3 Bourdieusian framework\nWe now turn our attention to the theories developed by French sociologist Pierre Bourdieu, whose perspective on science is fundamentally different than Merton’s as it is centred not around a set of institutional goals and norms but around power struggles between pragmatic self-interested agents.\n\n2.3.1 Field and agents\nAccording to Bourdieu (1975), the social space is divided in several distinct fields that are relatively autonomous and have specific interests and stakes. Science can be understood as a field with the specific goal of advancing knowledge. The field is composed of agents that possess different forms of capital (discussed below) and can be defined as entities recognized by their peers and who internalize the goals and norms of the field. There are two key concepts in this definition. First, the concept of recognition here highlights that certain conditions must be met for an agent to integrate a field, and the concept of internalization, which is tied to the concept of habitus discussed further..\nThe structure of a field is determined by the relative position occupied by all of the agents in the field based on the type and amount of capital they possess. This structure is dynamic because agents within a field constantly compete and mobilize their capital to acquire more capital, increase their symbolic power, and dominate the field.\nThere are also meta-fields which have some degree of influence on many other fields. The state is an example of a meta-field since it can, to some degree, impose regulations on other fields and play an important role in distributing resources within fields (e.g., funding corporations and universities). Bourdieu’s theory also includes a field of power composed of agents with a lot of capital that they can use to gain influence in other fields. Think, for example, of movie stars who can mobilize that stardom to gain political influence.\nThe concept of sub-fields is also important. For instance, the scientific community is formed by a set of agents that share a goal (advancing knowledge), but it is also composed of many sub-fields with their own specific goals (e.g., the production of knowledge in a certain area), practices and norms. The existence of meta-fields and sub-fields highlights the limited autonomy of fields.\n\n\n2.3.2 Capital\nBourdieu’s theory of capital incorporates Durkheim’s concept of cultural capital and Marx’s concept of economic capital, to which it adds the concepts of social capital and symbolic capital. Every field also has its form of capital (e.g., scientific capital) that can only be acquired within the field and has limited utility outside it.\n\n2.3.2.1 Economic capital\nEconomic capital comprises an agent’s financial resources, material assets, revenues, and means of production. This capital can be transformed into other types of capital. For instance, in the scientific field, a researcher might invest various forms of economic capital (research funds, work, equipment) to produce a contribution to knowledge and thus obtain some scientific capital.\n\n\n2.3.2.2 Social capital\nBourdieu (1980) defines social capital as resources available or potentially available to an agent by virtue of their belonging to a group, to a network of agents that recognize the links that unite them. The social capital of an agent is thus the extent of the network (number of nodes), the type and amount of capital that the connected agents possess, and the capacity of the agent to mobilize this capital. Social capital is acquired through efforts to create and maintain relationships through events (e.g., conferences), places (e.g., research laboratories), and practices (e.g., collaboration, peer review).\n\n\n2.3.2.3 Cultural capital\nAccording to Bourdieu (1979), cultural capital can be embodied, objectified, or institutionalized. Embodied cultural capital is the knowledge of an agent. Objectified cultural capital can be things like books, works of art, or musical instruments. Finally, an example of institutionalized cultural capital is a diploma. While agents may not recognize embodied capital within a field, diplomas and similar credentials can often guarantee a minimal degree of recognition within a field.\n\n\n2.3.2.4 Symbolic capital\nThe knowledge and recognition by agents of a field of the different forms of capital than an agent possesses confers symbolic capital to that agent Bourdieu (1987). Thus, symbolic capital is a form of meta-capital that is the supreme objective of agents’ actions in a field, as it is the recognition of one’s position within it. The distribution of symbolic capital ultimately determines the power structure at play in the field.\n\n\n2.3.2.5 Scientific capital\nScientific capital is a form of symbolic capital specific to the scientific field which determines its structure (Bourdieu 1975). It is the recognition by peers of one’s contribution to scientific progress (Bourdieu 1997). It provides its owner scientific authority and legitimacy in scientific matters. Scientific capital is provided by peers, which are also competitors. This is one of the most important features of science (peer review) and is related to the organized skepticism norm. Furthermore, not all contributions are equal, so the amount of scientific capital an agent may possess is determined by the value and originality of their contribution. The scientific article (or other forms of scholarly publications) materializes that contribution and is an example of what Bourdieu (1971) calls “symbolic goods”: objects that have some value in symbolic capital within a specific field.\n\n\n\n2.3.3 Habitus\nThe habitus of an individual is their mental representation of the world and of their position and the position of others within it. It is the structure of the field in its embodied form Bourdieu (1989). Agents acquire the habitus of a specific field through learning and experience within it. Through this process, agents eventually recognize the goals, norms, and structure of the field and are thus able to invest their capital strategically. The field and the habitus are thus interdependent. The habitus is structured by the field of which it is an embodied form, but it also structures the field since it guides the action of agents, which alter the structure of the field.\nThe habitus produces some regularity in the action of agents because it is derived from an objective structure common to all. However, Bourdieu’s theory insists that agents are guided by pragmatism more than by norms or rules. As Bourdieu (1986) points out, “we must avoid seeing in agent’s behaviour more logic than there is, because the logic of pragmatism is to be logical to a point where being logical ceases to be practical”. (p. 41, my translation)."
  },
  {
    "objectID": "ch2.html#summary",
    "href": "ch2.html#summary",
    "title": "2  The social structure of science",
    "section": "2.4 Summary",
    "text": "2.4 Summary\nThis chapter explored science as a social system with a set of goals, norms, and a distribution of capital that together define the system’s structure. By looking at both the Mertonian and Bourdieusian perspectives, we can understand this system as one where agents are at the same time guided by institutional norms common to all and by their strategies aimed at improving their position in the field. We described some mechanisms through which the system recognizes and rewards researchers for their contributions, this recognition being the main driver of one’s ascension in the stratified structure of science. We also explored some factors that can distort these recognition mechanisms, such as the Matthew effect and the Mathilda effect. These remind us that, ultimately, recognition and reward are subjective processes.\nDespite their differences, Merton and Bourdieu’s theories have a lot in common, most importantly the principle of social stratification providing some researchers with an advantage over others. Furthermore, both theories highlight the importance of accumulating peer recognition (Merton 1957) or symbolic capital (Bourdieu 1987) for researchers. This recognition is mainly achieved by contributing to the advancement of knowledge and playing by the rules of the game, at least as long as one sees this as the best advancement strategy.\n\n\n\n\nBourdieu, Pierre. 1971. “Le Marché Des Biens Symboliques.” L’Année Sociologique 3 (22): 49–126. https://doi.org/10.2307/27887912.\n\n\n———. 1975. “The Specificity of the Scientific Field and the Social Conditions of the Progress of Reason.” Social Science Information 14 (6): 19–47. https://doi.org/10.1177/053901847501400602.\n\n\n———. 1979. “Les Trois États Du Capital Culturel.” Actes de La Recherche En Sciences Sociales 30 (1): 3–6. https://doi.org/10.3406/arss.1979.2654.\n\n\n———. 1980. “Le Capital Social. Notes Provisoires.” Actes de La Recherche En Sciences Sociales 31 (1): 2–3.\n\n\n———. 1986. “The Forms of Capital.” In, edited by John G. Richardson, 241–58. New York: Greenwood.\n\n\n———. 1987. Choses Dites. Paris: Editions de minuit.\n\n\n———. 1989. La Noblesse d’état: Grandes Écoles Et Esprit de Corps. Les Editions de minuit.\n\n\n———. 1997. Les usages sociaux de la science: pour une sociologie clinique du champ scientifique ; une conférence-débat organisée par le groupe Sciences en questions Paris, INRA, 11 mars 1997. Sciences en questions. Paris: Institut national de la recherche agronomique.\n\n\nCole, Jonathan R, and Stephen Cole. 1973. Social Stratification in Science. Chicago, IL: University of Chicago Press.\n\n\nCole, Stephen, and Jonathan R Cole. 1967. “Scientific Output and Recognition: A Study in the Operation of the Reward System in Science.” American Sociological Review 32 (3): 377–90. http://www.jstor.org/stable/2091085.\n\n\nLarivière, Vincent, Chaoqun Ni, Yves Gingras, Blaise Cronin, and Cassidy R. Sugimoto. 2013. “Bibliometrics: Global Gender Disparities in Science.” Nature 504 (7479): 211–13. https://doi.org/10.1038/504211a.\n\n\nLincoln, Anne E., Stephanie Pincus, Janet Bandows Koster, and Phoebe S. Leboy. 2012. “The Matilda Effect in Science: Awards and Prizes in the US, 1990s and 2000s.” Social Studies of Science 42 (2): 307–20. https://doi.org/10.1177/0306312711435830.\n\n\nMerton, Robert K. 1942. “A Note on Science and Democracy.” Journal of Legal and Political Sociology, no. 1-2: 115–26. https://doi.org/2027/mdp.39015008014428.\n\n\n———. 1957. “Priorities in Scientific Discovery: A Chapter in the Sociology of Science.” American Sociological Review 22 (6): 635–35. https://doi.org/10.2307/2089193.\n\n\n———. 1968. “The Matthew Effect in Science.” Science, New series, 159 (3810): 56–63. https://doi.org/10.2307/1723414.\n\n\n———. 1973. “Recognition’and ’Excellence’: Instructive Ambiguities.” RK Merton, The Sociology of Science. Theoretical and …. http://scholar.google.ca/scholar?hl=fr&q=recognition+and+excellence+merton&btnG=&lr=#0.\n\n\n———. 1979. The Sociology of Science: An Episodic Memoir. Carbondale, IL: Southern Illinois University Press.\n\n\nMoss-Racusin, Corinne A, John F Dovidio, Victoria L Brescoll, Mark J Graham, and Jo Handelsman. 2012. “Science Faculty’s Subtle Gender Biases Favor Male Students.” Proceedings of the National Academy of Sciences 109 (41): 16474–79. https://doi.org/10.1073/pnas.1211286109.\n\n\nRossiter, Margaret W. 1993. “The Matthew Matilda Effect in Science.” Social Studies of Science 23 (2): 325–41. https://doi.org/10.2307/285482.\n\n\nZuckerman, Harriet A. 1977a. “Scientific Elite: Nobel Laureates in the United States.” http://books.google.ca/books?hl=fr&lr=&id=HAHCzJfmD5IC&oi=fnd&pg=PR13&dq=zukerman+nobel+prize&ots=5P05DL9tA2&sig=KcXglSng7NvCOudkwv5uCK3eo3I.\n\n\n———. 1977b. “Scientific Elite: Nobel Laureates in the United States.” http://books.google.ca/books?hl=fr&lr=&id=HAHCzJfmD5IC&oi=fnd&pg=PR13&dq=zukerman+nobel+prize&ots=5P05DL9tA2&sig=KcXglSng7NvCOudkwv5uCK3eo3I.\n\n\n———. 1998. “Accumulation of Advantage and Disadvantage: The Theory and Its Intellectual Biography.” In, edited by Carlo Mongardini and Simonetta Tabboni, 139–61. New Brunswick, N.J.: Transaction Publishers."
  },
  {
    "objectID": "ch3.html#the-organization-of-research",
    "href": "ch3.html#the-organization-of-research",
    "title": "3  Journals, peer-review, and the organization and evaluation of research",
    "section": "3.2 The organization of research",
    "text": "3.2 The organization of research\nToday’s research is largely done in universities, but it was not always so. Before the adoption of the Humboldt model of universities (as institutions of research and teaching) in Germany in the early 19th century and it’s subsequent generalization, research was mainly organized by academies, which initially operated like clubs funded by their members: an international group of science amateurs. I highlight the word amateur here since the professional status of the scientist as it is understood today is a relatively recent historical development.\nAlthough there is a stereotype of the scientist as a mad professor alone his laboratory, history shows that science has always been a collective effort. The first scientific societies – which were more like private clubs – were formed in the 1600s in some of Europe’s major cities. Most of these clubs evaluated research and technology on behalf of their monarch, such as The Royal Society of London, which is the oldest scientific academy to still exist today. Founded in 1660, the Royal Society became a publishing venue for scientific ideas and reports, and was critical in the formation of a scholarly communication network throughout Europe.\nAlso founded in 1660 was the Académie des Sciences in Paris. Unlike London’s Royal Society, which was not financially supported by the monarchy, the Paris academy awarded government stipends to a small group of scientists. These stipends helped legitimize science as a profession while drawing upon the vast resources of the kingdom, and by the 1700s, being elected to the Académie des Sciences carried great prestige and financial assurances. As with the Royal Society, the Académie des Sciences paved the way for scientific publication by publishing proceedings from its meetings.\nDespite these scientists’ supposed interest in attaining enlightenment, they were as discriminatory as anybody else from back then, allowing mostly prominent citizens from noble families into these early societies. The first woman was admitted into the Royal Society as late as 1945. There’s no way to measure how much greater our knowledge would be if early scientific clubs had been open to all citizens.\nNonetheless, the emergence of the Royal Academy and the Académie des Sciences inspired other aspiring scientists throughout Europe. Societies became a crucial part of science, allowing researchers to coordinate and evaluate their efforts.\n\n\n\n\n\n\nSuggested reading\n\n\n\nIf you want to know more about the Royal Society of London, you can check out their website: https://royalsociety.org/about-us/history/\n\n\nThe institutionalization of science took place gradually over the last 2 centuries and greatly accelerated after World War I and World War II, which brought scientific development (which played a decisive role in the outcome of the war) to the forefront of national priorities. While the international scientific community had long existed and transcended geopolitical boundaries, governments came do realize the importance of developing a national research capacity to decrease their reliance on other countries for the production and application of new knowledge. This contributed to the massification of research and higher education, as well as the proliferation of universities and university professors, effectively operating a full transition from science as a hobby to science as a profession."
  },
  {
    "objectID": "ch3.html#scholarly-communications-before-scholarly-journals",
    "href": "ch3.html#scholarly-communications-before-scholarly-journals",
    "title": "3  Journals, peer-review, and the organization and evaluation of research",
    "section": "3.2 Scholarly communications before scholarly journals",
    "text": "3.2 Scholarly communications before scholarly journals\n\n\n\n\n\n\nBenefits of the written work\n\n\n\nWritten works can be disseminated, copied, verified and referenced. they can contain detailed information, including images and figures. They establish priority in discoveries, can be used to generate a record of the scientific knowledge that does not die with the scientists.\n\n\nIf you have ever wondered what people did all day before the internet, you probably have the impression (maybe from Bridgerton) that they just wrote letters all day long. Thus, it is not surprising that the earliest scholarly communication took place through letter writing (“personal correspondence,” if you’re fancy.)\nScientists spread the news of observations and ongoing experiments through letters written to other scientists. Letters were a part of daily life due to their convenience, low cost, and lack of censorship. In the 17th century, letters could be sent across Europe within weeks – great speed for the time.\nHowever, even back then it was too much work to write a letter to each of your scientist friends announcing your most recent discovery. Instead, there were middlemen who collected and distributed these letters to other scientists. One of the most famous of these information gatekeepers was Henry Oldenburg, the secretary of London’s Royal Society. At Society meetings, Oldenburg would read aloud the correspondence he received that pertained to scientific matters. For example, the minutes of a Society meeting from 1667 reads as follows:\n\n“During the recess of the society Mr. OLDENBURG kept up his correspondences with several of the learned men abroad, and particularly HEVELIUS; the letters which passed between them being extent in the LetterBook….\nMr. OLDENBURG read an extract of Monsr. AUZOUT’s letter to him from Paris, Decemb. 28.1666. N.S. mentioning a new method esteemed by him better than any hitherto practised, of taking the diameters of the planets to seconds, and of knowing the parallax of the moon by means of her diameter.” (Fjällbrant 1997)\n\nThese meeting minutes show indications of an international scholarly communication network dating back to the 17th century.\nAnother prominent figure was Father Marin Mersenne, who was at the center of a correspondence network. By visualizing these networks, we are able to observe a part of what we could call the “scientific community” of the time."
  },
  {
    "objectID": "ch3.html#scholarly-journals",
    "href": "ch3.html#scholarly-journals",
    "title": "3  Journals, peer-review, and the organization and evaluation of research",
    "section": "3.3 Scholarly journals",
    "text": "3.3 Scholarly journals\nEven though it was preceded by a few months by the Journal des Sçavans that eventually became the main dissemination platform of members of the Académie des Sciences, the Philosophical Transactions first published in 1665 by the Royal Society (and still published today) is generally referred to as the first scholarly journal. Its editor-in-chief was Harry Oldenburg. Eventually, these journals came to formalize and organize the process of dissemination of knowledge that had been before relying on informal networks and people like Father Mersenne.\n\n3.3.1 Organizing knowledge and research communities\nWith the growth of science came increased specialization, the creation of new disciplines, scholarly associations, and journals with varying degree of specialization, as well as national journals to accommodate the growth of national science systems that we mentioned above. In this way, journals play an important role in shaping and structuring research communities, and the knowledge that they produce.\n\n\n3.3.2 Quality control (peer review)\nAnother role of journals that may come to mind is the evaluation of research, which is done through a process known as peer review. Peer-review is one of the most central and defining feature of modern science and the scholarly communication process. It is directly related to the Mertonian norm of organized skepticism, and can be defined as “the evaluation of work by one or more people with similar competencies as the producers of the work” (Wikipedia). One job of the journal editor or their team is to invite experts (typically two) to evaluate the article and determine if it is suitable for publication in the journal. The peer review process can be single-blind (the identify of the author known to the reviewer, but the reviewer is anonymous), double-blind (both the reviewer and the author remain anonymous), or open (the identities of all parties are revealed). In recent years, the idea of post-publication peer review has been gaining some steam. This type of peer review, as the name suggests, occurs only after the article is published (usually on some online platform), and is another form of open and transparent peer review. The main appeal of post-publication peer-review is that it allows for the faster dissemination of knowledge (with the traditional pre-publication peer review can take months or years before a submitted article eventually gets published). The COVID-19 pandemic might have contributed to the recent hype around post-publication peer-review. Since the world needed to have access to research results to do things like developing and approving vaccines. However, letting everyone publish their work before the peer review process occurs can have perverse effects, such as the potential flooding of the web with bad science. Peer review is a quality control mechanism, after all. This short article in Nature discusses some of the challenges encountered by pre-preprint servers bioRxiv.org during the pandemic: https://www.nature.com/articles/d41586-020-01394-6.\n\n\n\n\n\n\nDid you know?\n\n\n\nPeer review as it is conducted today (by external reviewers invited by the editor) is a relatively new thing in science. For hundreds of years following the creation of the Philosophical Transaction the journal editors were the ones who handled the quality control of the works they published. However, as science became more and more institutionalized and the number of researchers and research areas grew, it became impossible for editors to handle all that work. They could also not be expert in all the areas of research that their journals would publish. So, they turned to external reviewers for help.\nSome eminent researchers, like Albert Einstein, were not huge fans of this new process. When the Physical Review journal editor rejected one of his papers based on the external reviewers’ comments, this was Einstein’s response:\n\nDear Sir,\nWe (Mr. Rosen and I) had sent you our manuscript for publication and had not authorized you to show it to specialists before it is printed. I see no reason to address the—in any case erroneous—comments of your anonymous expert. On the basis of this incident I prefer to publish the paper elsewhere.\nRespectfully,\nP.S. Mr. Rosen, who has left for the Soviet Union, has authorized me to represent him in this matter.\n\nBy the way, the reviewer turned out to be right, and Einstein ended up correcting the paper before it was published in another journal.\n\n\nEven though peer review is now a standard and widely accepted element of the scholarly communication process, there are still a lot of researchers who are critical of the process. Here are some of the main critics of peer review.\n\nPeer review is arbitrary. The outcome of the peer review is difficult to predict and the same publication might receive a completely different assessment depending on the reviewers involved.\nPeer review is biased. There are all kinds of ways in which reviews may be biased, prominent scientists get more favorable reviews than those who are unknown. Gender, ethnicity, topics, methods, and innovativeness are all factors that may affect the outcome of the review in one way or another.\nPeer review does not detect error or fraud. In the last two decades, we have seen a surge in the retractions of peer reviewed articles that were found to be erroneous or fraudulent, leading some people to argue that the fact that these papers made it past peer review demonstrates that the process does not work.\nPeer review is exploitative. Peer review is performed voluntarily by members of the scientific community. It also provides little returns in symbolic capital for the reviewer. However, they help increase the quality and reputation of the journals, many of which are owned by corporations who extract large profits from the reputation of their journals.\nPeer review is slow. It may take time for an editor to find suitable reviewers for a paper. Then reviewers are given usually about a month for their review. Then the comments are sent to the authors who are invited to submit a response to the reviewers and an updated manuscript. This process is repeated until the manuscript gets accepted for publication. Here is a meme to illustrate just how slow peer review can be:\n\n\n\n.. (AcademicChatter?) #AcademicTwitter #phdlife #phdchat #ecrchat #acwri #AcademicChatter (WoeAcademia?) (AcaGrumbles?) pic.twitter.com/RAqLnvdFDT\n\n— Reviewer 2 ((GrumpyReviewer2?)) September 21, 2019\n\n\n\n3.3.2.1 Reviewer 2\nThis is probably a good time to introduce the dreaded and infamous reviewer 2. Indeed, if it takes such a long time for a paper to get published, it is a well-known fact that it is partly because of reviewer 2. But who is reviewer 2?\n\n\n\n\n\nEveryone fears reviewer 2, but what they fear even more is being reviewer 2.\n\n\n\n\n\n\n\nDid you know?\n\n\n\nPeer review is not only a part of the scholarly communication process but occurs in almost all stages of the research process (e.g., research funding) and throughout the reward system of science (hiring, tenure, promotion, honorific awards, etc.).\n\n\n\n\n\n3.3.3 Other uses (or misuses) of journals\n\n3.3.3.1 Research evaluation\nJournals having different levels of prestige (symbolic capital), they are often used as proxy for the quality of publications, researchers, and institutions. Publishing in some specific journals can be determinant and even make or break scientific careers, bring universities up in university rankings, etc. For example, the Financial Times produces a business schools ranking in which they use the number of articles published in 50 top journals as an indicator of research excellence.\nIn bibliometrics, the Journal Impact Factor (JIF) is one way in which the prestige of a journal is quantified and measured (based on average number of citations papers published in the journal have received). To appreciate the importance of a journal, one does not have to be a member of the community or be able to recognize the prominence of the editorial board members or the quality of the content published by the journal. One just needs to know 5 is a higher number than 4 and hang on to the belief that this number is a true representation of a journal’s quality or prestige, so that a journal with a JIF of 5 is better than a journal with a JIF of 4. By extension, a scientist who publishes their work in a journal with a JIF of 5 has to be a better scientist than one who publishes in a journal with a JIF of 4. This of course is a widely criticized use of the JIF, which were actually designed to help academic librarians with collection development, not to evaluate science or scientists.\n\n\n3.3.3.2 Generating profits\nMost of the journals published today are owned by a handful of large corporations (Larivière, Haustein, and Mongeon 2015) that control what is one of the most profitable industries of modern times. This is done largely by selling back to the scientific community (mainly through academic libraries) the product of its free labor (research, peer-review). We will address this topic again in a later chapter about open access publishing.\n\n\n\n\n\n\nSuggested readings\n\n\n\nThis article published in the Guardian is a fascinating dive into the recent history of the scholarly publishing industry and how it came to be one of the most profitable industry of modern times.\nThis blog post discusses the profits generated through the article processing charges (APCs) that publishers typically charge to make research articles openly accessible."
  },
  {
    "objectID": "ch3.html#peer-review",
    "href": "ch3.html#peer-review",
    "title": "3  Journals, peer-review, and the organization and evaluation of research",
    "section": "3.5 Peer-review",
    "text": "3.5 Peer-review\nPeer-review is one of the most central and defining feature of modern science and the scholarly communication process. It is directly related to the Mertonian norm of organized skepticism, and can be defined as “the evaluation of work by one or more people with similar competencies as the producers of the work” (Wikipedia). Peer-review occurs at many stages of the reward system of science (hiring, tenure, promotion, honorific awards, etc.) and the scholarly communication process (funding, publishing)\n\n3.5.1"
  },
  {
    "objectID": "ch3.html#references",
    "href": "ch3.html#references",
    "title": "3  Journals, peer-review, and the organization and evaluation of research",
    "section": "3.4 References",
    "text": "3.4 References\n\n\n\n\nFjällbrant, Nancy. 1997. “Scholarly Communication - Historical Development and New Possibilities.” In.\n\n\nLarivière, Vincent, Stefanie Haustein, and Philippe Mongeon. 2015. “The Oligopoly of Academic Publishers in the Digital Era.” Edited by Wolfgang Glanzel. PLOS ONE 10 (6): e0127502. https://doi.org/10.1371/journal.pone.0127502."
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5  Assessing data",
    "section": "",
    "text": "5.1 Cleaning: data is messy\nData may be transforming everything about our lives, though it is amazing how little the public knows about it. In 2012, analysts estimated that 90% of the world’s data had come into existence in the previous two years [10], and by 2019 the World Economic Forum estimated that there were 44 zettabytes of data in existence, which is around 40 times more bytes than there are stars in the observable universe [11]. It’s almost as if the amount of data being generated is expanding faster than the universe.\nBut why is this the case? Why is there so much data being generated, and so quickly? In the previous chapter, we discussed how a prior explosion of software has fundamentally changed our society. With the advent of the software revolution, and especially mobile computing, there are now hundreds of thousands of sensors, processes, and devices generating data every second [12]. Software has consumed our lives, and as a result, is generating unfathomable amounts of data.\nFrom a manager’s perspective, the data that is usually most interesting to us are those data generated by our information systems. Whether these systems are e-commerce platforms like Shopify, customer-relationship management (CRM) tools like Salesforce, or enterprise resource planning (ERP) systems like SAP; data is being generated through our interactions with these systems. In the context of the 2020s, this data is not only being generated but is largely generated on the internet using cloud software. Gartner predicts that in 2023, the market for end users of cloud software will be just over $600 billion, representing almost 50% of all information technology spending [13].\nOne advantage of modern cloud computing is that it makes data highly accessible to managers who wish to use it. Supported by modern data exchange protocols, most enterprise applications seamlessly integrate with business intelligence software, including Excel, Power BI, and Tableau. Business intelligence is also usually supported through an intermediary analytics layer that is either maintained by an organization or is part of an enterprise software suite. Figure 2.1 illustrates this concept.\nWhile modern technology is great, the ease of data exchange also raises new pressures on people who make data-driven decisions. Much of the data that is available to you will be formatted in ways that are difficult to interpret or with frustrating errors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing data</span>"
    ]
  },
  {
    "objectID": "ch6.html",
    "href": "ch6.html",
    "title": "6  Processing data",
    "section": "",
    "text": "Coming soon…",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Processing data</span>"
    ]
  },
  {
    "objectID": "ch4.html#web-of-science",
    "href": "ch4.html#web-of-science",
    "title": "4  Bibliometric data sources",
    "section": "4.2 Web of Science",
    "text": "4.2 Web of Science\nThe Web of Science is the oldest bibliometric database. It was created in 1963 by Eugene Garfield and his company, the Institute for Scientific Information (ISI). It took 33 years before a competitor emerged (Scopus). In fact, the Web of Science is a collection of citation indexes. The main ones generally used in bibliometric research are the Science Citation Index Expanded (SCIE), the Social Sciences Citation Index (SSCI), and the Arts & Humanities Citation Index (AHCI).\nBecause it’s been around so long, the Web of Science (and because Elsevier was not making its database available for large-scale research until very recently) the Web of Science has long been the main database used for large scale bibliometric studies.\nIts main advantages are its long coverage period (although the competitors are catching up), the consistency of its metadata, and the clear criteria for the inclusion of sources in the database. (you can read about the editorial selection process here). However, the Web of Science’s focus on quality and consistency makes it one of the databases with the lowest coverage among its competitors.\nIf you’re interested in what a citation index looked like before the internet, I recommend the three videos below, produced by the ISI in 1967.\n\n\n\n\n\n\n\n\n\n\n\n\nWeb of Science is not available at Dalhousie\n\n\n\nUnfortunately, the Dalhousie Libraries do not have a subscription to the Web of Science, so you will not be able to use it as a data source for the course.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch4.html#scopus-and-scival",
    "href": "ch4.html#scopus-and-scival",
    "title": "4  Bibliometric data sources and indicators",
    "section": "4.3 Scopus and SciVal",
    "text": "4.3 Scopus and SciVal\nScopus was created in 1996 by Elsevier."
  },
  {
    "objectID": "ch4.html#dimensions",
    "href": "ch4.html#dimensions",
    "title": "4  Bibliometric data sources",
    "section": "4.4 Dimensions",
    "text": "4.4 Dimensions\nDimensions is a relatively new player. It is a database like Web of Science and Scopus developed by a company called Digital Science, which happens to be owned by another large publishing company: Springer-Nature. Unlike Web of Science and Scopus, which are accessible via subscription only (and they are not cheap!), Dimensions has a free online search interface that provides access to its publication data (which is great because that is what you’ll need 95% of the time).\nAgain, this search engine is mostly useful for information retrieval. However, Like Scopus and Web of Science, it is possible for researchers to submit research proposals and get access to the full data infrastructure for research.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch4.html#openalex",
    "href": "ch4.html#openalex",
    "title": "4  Bibliometric data sources",
    "section": "4.6 OpenAlex",
    "text": "4.6 OpenAlex\nOpenAlex is a new and entirely open platform managed by a non-profit organization called OurResearch. At the core of OpenAlex is the Microsoft Academic Graph (MAG), an open database that Microsoft stopped supporting in 2021, as well as Crossref, but OpenAlex also includes data from a variety of other sources in an attempt to provide the most comprehensive, fully open database of scholarly entities and outputs. Here is the OpenAlex database schema:\n\nOpenAlex does not (yet) have a website that can be used to search and retrieve data. However, it has a free API, and the entire database can also be downloaded for free (don’t do it, it’s huge).\n\n\n\n\n\n\nSpecial access for INFO6850 students\n\n\n\nThe Maritime Institute for Science, Technology, and Society (MISTS) provides direct access to the OpenAlex database through its PostgreSQL server. If you would like to make use of this access, please contact your instructor or TA, and you will be provided with a username, password, and log in instructions.\nOnce you have your connection information, I recommend using DBeaver to connect and query the MISTS database, but any PostgreSQL client will do.\nPlease: be careful when running SQL queries on the server. The MISTS servers do not run large resources, so poorly constructed queries might use up all the resources and slow the server down for everyone. Some of the OpenAlex tables are very large, and if your query uses a non-indexed column, it might take days to execute. As a rule of thumb, if your query has been executing for more than 30 minutes, cancel it and check with your instructor or TA to see if something can be done to make the query execute faster.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch4.html#google-scholar",
    "href": "ch4.html#google-scholar",
    "title": "4  Bibliometric data sources",
    "section": "4.7 Google Scholar",
    "text": "4.7 Google Scholar\nGoogle Scholar (https://scholar.google.ca/) has the best coverage of all databases. Unlike databases like Scopus and Web of Science which index specific journals that meet some criteria, Google Scholar crawls the web and collects everything that Google robots consider to be a scholarly output. This may be good for coverage, but at the expense of some data quality issues. Google Scholar has many shortcomings, some as direct results of its lack of selection criteria:\n\nThe Google Scholar process is a black box. We do not have a list of included sources or know how the robots are doing the indexing.\nData is not easy to access and download for analysis.\nThe metadata is much more limited than in other databases.\nLimited data extraction capabilities.\n\n\n4.7.1 Google Scholar profiles\nResearchers’ profiles on Google Scholar is one of the most powerful and useful feature of the site. One of the reasons is that most researchers take ownership of their Google Scholar profile, verify it, and curate it. Thus, verified Google Scholar profiles are one of the best way to avoid the challenge of author name disambiguation, which we will discuss in more detail in Chapter 6.\nThis is a screenshot of a Google Scholar profile (and this is the URL: https://scholar.google.ca/citations?user=wvX4S8sAAAAJ)\nThis is all nice, but we’re still not able to easily collect the data for some analysis. There is free software for this called Publish or Perish that can help, but its capacities are also quite limited. In chapter 6, a short demo will be provided to show how to make effective use of publish or perish to collect publications and citation data for individual researchers.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch4.html#section",
    "href": "ch4.html#section",
    "title": "4  Bibliometric data sources and indicators",
    "section": "4.7 ",
    "text": "4.7"
  },
  {
    "objectID": "ch4.html#elseviers-suite-scopus-scival-and-the-icsr-lab.",
    "href": "ch4.html#elseviers-suite-scopus-scival-and-the-icsr-lab.",
    "title": "4  Bibliometric data sources",
    "section": "4.3 Elsevier’s suite: Scopus, SciVal, and the ICSR lab.",
    "text": "4.3 Elsevier’s suite: Scopus, SciVal, and the ICSR lab.\n\n4.3.1 Scopus\nElsevier launched Scopus in 1996 as the first competitor for the Web of Science for information retrieval purposes. Unlike the Web of Science, which made its raw data available for research (although not for free) for decades, Scopus data was for a long time only available through a subscription to its web platform (which only allows you to download 2000 records at a time and makes large scale bibliometric analyses difficult if not impossible).\nThere is no free access to Scopus, but you can access Scopus through the Dalhousie libraries.\n\n\n4.3.2 SciVal\nScival uses the same data as Scopus, but it is designed for research evaluation purposes. You can also access SciVal through the Dalhousie Libraries (Note: you will need to create an account and log in to use SciVal).\nThe good thing about SciVal is that it provides an interface that you can use to calculate a variety of indicators, produce rankings, visualize trends, etc. However, the interface can be a little confusing and clunky. It can also lead you to use whatever indicator or visualization the platform provides rather than other indicators and visualizations that may fit your needs better.\n\n\n4.3.3 International Center for the Study of Research (ICSR) Lab\nPossibly in an attempt to compete with the Web of Science, and especially with Dimensions (see below), which are offering access to their full database to research, Elsevier recently launched the International Center for the Study of Research (ICSR) Lab. The lab allows researchers to submit research proposals and get access to their data infrastructure. (details here: https://www.elsevier.com/icsr/icsrlab/features).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch4.html#crossref",
    "href": "ch4.html#crossref",
    "title": "4  Bibliometric data sources",
    "section": "4.5 Crossref",
    "text": "4.5 Crossref\nCrossref is a non-profit organization run by the Publishers International Linking Association (PILA). It is also a major Digital Object Identifier (DOI) registration agency. Crossref’s main focus is on creating an open and free database of linking research objects (articles, books, datasets, etc.). However, its search engine is not very good, and Crossref is usually not the best choice for searching the literature.\nThat said, Crossref provides free access to its entire database through its application programming interface (API) and the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH). Crossref also makes the entire Crossref database available for download (don’t do it. It’s huge). Details here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch4.html#appendix-a---scopus-metadata-export",
    "href": "ch4.html#appendix-a---scopus-metadata-export",
    "title": "4  Bibliometric data sources",
    "section": "4.8 Appendix A - Scopus metadata export",
    "text": "4.8 Appendix A - Scopus metadata export"
  },
  {
    "objectID": "ch4.html#appendix-b---dimensions-metadata-export",
    "href": "ch4.html#appendix-b---dimensions-metadata-export",
    "title": "4  Bibliometric data sources",
    "section": "4.9 Appendix B - Dimensions metadata export",
    "text": "4.9 Appendix B - Dimensions metadata export"
  },
  {
    "objectID": "ch4.html#appendix-c---google-scholar-export",
    "href": "ch4.html#appendix-c---google-scholar-export",
    "title": "4  Bibliometric data sources",
    "section": "4.10 Appendix C - Google Scholar export",
    "text": "4.10 Appendix C - Google Scholar export"
  },
  {
    "objectID": "ch4.html#how-to-choose-a-data-source",
    "href": "ch4.html#how-to-choose-a-data-source",
    "title": "4  Bibliometric data sources",
    "section": "4.8 How to choose a data source?",
    "text": "4.8 How to choose a data source?\nYou should always, always start with some idea of what your goal is and what data you need to achieve that goal. So the first question is:\n\nWhat is the goal of my analysis? What do I want to know?\n\nOnce you know what you want to know, then you are ready to ask the second most important question:\n\nWhat data do I need to know what I want to know?\n\nThe point is that some databases may be more generous than others in terms of metadata elements and the possibilities they offer, but that will not help you if you don’t need those extra features. In other words, you don’t want the best database. You want the best database for your needs. So start by identifying what those needs are.\nThen you can start exploring the databases and ask the questions below to help you guide your decision.\n\nWhat metadata does the database provide?\n\nDoes it include author identifiers to help with disambiguation?\nDoes it include all the author’s institutional addresses?\nDoes it include all cited sources?\nDoes it include the abstract?\nDoes it include the authors’ keywords?\nDoes it include funding acknowledgements?\nDoes it include some disciplinary classification?\nEtc.\n\nHow easily accessible is the data?\n\nCan I easily download all the data I will need for my analysis?\nWhat interface/methods are available to retrieve data?\nCan I retrieve that data in a format that I know how to use?\n\nHow structured is the data?\n\nWill I have to do a lot of data processing?\nDo I have the skills to do that data processing?\n\nDoes the database adequately cover the field/countries/languages I want to analyze?\n\nSome databases cover non-English research and certain disciplines like Arts & Humanities better than others.\n\n\nIf you’re working on a small scale (trying to build a comprehensive database of a person or a small unit’s research output, you might want to combine multiple databases to achieve the best results.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch4.html#what-skills-do-i-need-to-do-bibliometrics",
    "href": "ch4.html#what-skills-do-i-need-to-do-bibliometrics",
    "title": "4  Bibliometric data sources",
    "section": "4.9 What skills do I need to do bibliometrics?",
    "text": "4.9 What skills do I need to do bibliometrics?\nNo, you do not need to know any programming languages like SQL, R or Python to do bibliometrics. There are plenty of free tools that can help you, and most data processing and analyses can be done in Excel too. That said, programming skills definitely can help you perform more complex tasks, use a broader range of databases, or just save time with data collection, cleaning, processing and analysis.\nIn this course, you are free to chose whatever tool you wish to use. Your instructor and TA will be there to help no matter what choices you make, so don’t be afraid to challenge yourself."
  },
  {
    "objectID": "ch4.html#further-readings",
    "href": "ch4.html#further-readings",
    "title": "4  Bibliometric data sources",
    "section": "4.10 Further readings",
    "text": "4.10 Further readings\nIf you are curious, you can check out this recent study that compared some of the databases mentioned in this chapter:\nVisser, M., van Eck, N. J., & Waltman, L. (2021). Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. Quantitative Science Studies, 2(1), 20–41. https://doi.org/10.1162/qss_a_00112",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch4.html#what-skills-do-i-need-to-do-this",
    "href": "ch4.html#what-skills-do-i-need-to-do-this",
    "title": "4  Bibliometric data sources",
    "section": "4.9 What skills do I need to do this?",
    "text": "4.9 What skills do I need to do this?\nNo, you do not need to know any programming languages like SQL, R or Python to do bibliometrics. There are plenty of free tools that can help you, and most data processing and analyses can be done in Excel too. That said, programming skills definitely can help you perform more complex tasks, use a broader range of databases, or just save time with data collection, cleaning, processing and analysis.\nIn this course, you are free to choose whatever tool you wish to use. Your instructor and TA will be there to help no matter what choices you make, so don’t be afraid to challenge yourself.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch6.html#what-is-an-author",
    "href": "ch6.html#what-is-an-author",
    "title": "6  Measuring research output",
    "section": "6.2 What is an author?",
    "text": "6.2 What is an author?\nAuthorship is at the core of the reward system of science. It links every contribution to a field (a publication) to the individual (or individuals) who produced them. As we will see in this chapter, the author status is not necessarily attributed to everyone who participated to a study. This chapter presents the functions of authorship in research as well as authorship attribution practices, norms, and guidelines."
  },
  {
    "objectID": "ch6.html#functions-of-authorship",
    "href": "ch6.html#functions-of-authorship",
    "title": "6  Measuring research output",
    "section": "6.3 Functions of authorship",
    "text": "6.3 Functions of authorship\nAccording to Birnholtz (2006), the functions of authorship in science are to assign credit, ownership, and responsibility for discoveries, as well as to enable the existence of a reputation economy.\n\n6.3.1 Credit\nAs we have seen, peer recognition is the main reward obtained by researchers, this symbolic capital then allowing them to obtain other rewards with symbolic but also financial value. It is therefore important for researchers to obtain this credit for their contributions and the codes of ethics of disciplinary associations (and, to a lesser extent, the editorial policies of journals) emphasize the importance of giving all contributors the credit due to them. To take up some elements of the theory of Bourdieu presented earlier, the credit is in fact the symbolic capital obtained by the researchers in their struggle for domination of the scientific field (Bourdieu, 1976). It is thanks to the status of authorship that the system of recognition of science can function, granting a form institutionalized credit (in the form of honorary awards, positions, for example) to researchers who have distinguished themselves as authors of important contributions to the advancement knowledge.\n\n\n6.3.2 Ownership and responsibility\nThe notion of ownership linked to authorship applies less to science than to other domains (e.g. the literary domain). Indeed, property rights in science boil down to recognition of the researcher’s contribution to the reported results (Merton, 1957). According Biagioli (2003), only the original expression of scientific discoveries, generally under textual form, is the property of the authors, which allows them to be protected against plagiarism. The notion of responsibility goes hand in hand with that of ownership and has a particular in science (Birnholtz, 2006; Rennie and Flanagin, 1994). In a system of recognition based on the contribution of researchers to the advancement of knowledge, the good functioning of the system depends on both its ability to reward merit and respect standards and its ability to punish, or at least not reward, demerit and non-compliance with standards. Scientific fraud (e.g. fabrication or falsification of scientific results research, or plagiarism) is an example of non-compliance with the standards of science having a negative impact on the career of the researchers involved (Azoulay, Bonatti and Krieger, 2015; Lu, Jin, Uzzi and Jones, 2013; Mongeon and Larivière, 2016), their institutions and even all their discipline (Azoulay, Furman, Krieger and Murray, 2015). Although they remain relatively rare compared to all scientific research, cases of fraud scientists have greatly increased over the last twenty years (Steen, Casadevall and Fang, 2013). This has contributed to publishers’ interest in developing guidelines on the attribution of the status of author which, as we will see later, particularly emphasis on the responsibility of researchers for the validity of the content of articles (Resnik and Masters, 2011).\n\n\n6.3.3 Reputation economy\nThe notions of reputation and the economics of reputation (Whitley, 2000) are closely linked to that of symbolic capital. Indeed, authorship, among other things, provides from scientific capital to the researcher (Birnholtz, 2006). Insofar as this capital is known and recognized by other researchers, it is converted into symbolic capital and thus confers on the seeking a certain power in the field (Bourdieu, 1987). The economy of reputation is also evident in the social stratification of the scientific field as well as at all levels of the hierarchy of rewards, from the simple citation to obtaining the Nobel Prize, form institutionalized symbolic capital and reputation of the researcher (Cole and Cole, 1973)."
  },
  {
    "objectID": "ch6.html#authorship-norms-and-practices-in-a-collaborative-context",
    "href": "ch6.html#authorship-norms-and-practices-in-a-collaborative-context",
    "title": "6  Measuring research output",
    "section": "6.4 Authorship norms and practices in a collaborative context",
    "text": "6.4 Authorship norms and practices in a collaborative context\nResearch is an increasingly collaborative activity, in all disciplines the average number of authors in each disciplines increased continuously over the last century (Larivière et al., 2015; Wuchty et al., 2007). There are many factors explaining that trend, including the fact that research is more complex and expensive, leading researchers to pool together there expertise and resources (Katz and Martin 1997).\nAssigning authorship can be difficult when they are the outcome of the work of many individuals with different status, role and contributions. Decisions have to be made about 1) who will be an author (not all contributions lead to authorship) and 2) the order of the names, which is usually meant to indicate what share of the credit each one deserves (D. Pontille 2006). It can be extremely difficult for an external observer to determine who did what or what share of credit everyone deserves (Rennie et al., 1997).\nUtlimately, the byline depends of two elements:\n\nThe nature of the collaboration and the division of labour between members of the team(who did what?)\nThe decisions made about naming and ordering authors (who will be an author, and in what order will the names be listed\n\n\n6.4.1 Collaboration and division of labour\nLaudel (2002) identified six types de collaboration related to the types of contributions that individuals can make to a given research project:\n\nCollaboration involving a division of intellectual labour. Collaborators with a shared goal who make substantial intellectual contribution towards that goal. They are to some degree co-leading the research.\nService collaboration. Researchers who are called upon to produce routine work that require a specific expertise.\nProvision of access to research equipment. The collaborator does not perform any tasks related to the project but provides material, equipment, data, etc. used for the research.\nTransmission of know-how. The non-creative transmission of information stored in memory that is useful for the research.\nMutual stimulation: The stimulation and engagement through informal interactions that helps researchers develop their ideas.\nTrusted assessorship:Providing feedback on the work.\n\nNot all these forms of collaboration lead to authorship. Laudel (2002) found that authorship is usually attributed to collaborators of the first two types only.\nSubramanyam (1983) proposed four types of collaboration related to the hierarchical status of the involved individuals:\n\nSame-status collaborations.\nProfessor-student collaborations.\nSupervisor-assistant collaborations.\nResearcher-consultant collaborations.\n\nThe number of people required for a given project can have an influence on the types of collaboration that will take place. According to Walsh and Lee (2015), larger teams tend to lead to more bureaucratic organizational structures, which is characterized by increased division of labour, specialization and standardization of tasks, hierarchical relationships, and decentralized decision-making (Walsh et Lee, 2015).\nLes formes de collaboration privilégiées par les chercheurs ne sont pas seulement déterminées par leurs préférences personnelles, mais aussi par la nature des travaux de recherche effectués. Celle-ci est définie par l'objectif institutionnel de production de connaissances dont découlent des impératifs techniques et moraux (Merton, 1942). Cet objectif commun ou, selon Bourdieu (1976), les intérêts des agents dominants définissent les enjeux scientifiques (les objets de recherche) qui sont d'intérêts pour les agents du champ qui recherchent les investissements les plus payants en capital symbolique (Bourdieu, 1976). L'objet de recherche définit les ressources humaines et matérielles ainsi que les savoirs et savoir-faire requis, ce qui peut en quelque sorte imposer aux chercheurs certains types de collaboration et de division du travail. Cette idée est bien illustrée par la notion de culture épistémique (Knorr Cetina, 1999) qui désigne la particularité des pratiques de création des connaissances dans une discipline.\n\n\n6.4.2 Naming authors\nUnlike the concept of author in literature, which is linked with writing, the concept of author in science is not and other forms of contributions can lead to authorship. Moreover, as D. Pontille (2006) points out, the act of writing is not necessarily sufficient to obtain the author status, and it is also possible to be an author without writing. So what kind of contributions do lead to authorship? Contributions involving the division of intellectual labour (first type of collaboration identified by Laudel (2002)). This linked between the substantial intellectual contribution and authorship generates little to no debate in the literature. However, whether authorship should be awarded to individuals with technical, routine, or less substantial contributions is not so clear.\nHagstrom (1964) tackles the question of authorship for technical contributions by looking at the historical context and distinguishing traditional collaboration from modern collaboration. He portrays the traditional technician as having little qualifications and being involved in the search for solutions to scientific problems but performing simple tasks designed and assigned by the scientist in exchange for economic capital. In contrast, the modern technician is a qualified professional performing complex tasks that the researcher employing them may not know how to perform themselves. This professionalization of the technician is nicely illustrated by Knorr-Cetina (1999) who tells the story of an established researcher who mastered the theories of his field but was unable to execute the technical tasks their research required. This type of relationship between the scientist and the technician is not generalized and technical contributions do not always lead to authorship today Laudel (2002).\nThe type of contribution is not always the only determinant. The same task performed by a paid technician or by another researcher may lead to authorship for the latter but not the former Shibayama, Walsh, and Baba (2012).\nThe importance of a discovery can also affect authorship attribution. Jabbehdari et Walsh (2017) found that the individuals who performed technical or less subtantial contributions are more often excluded from the byline of highly cited work.\nThe relationship between the nature and extent of a contribution and authorship is also determined by implicit disciplinary norms. Some type of contributions that often lead to authorship in a discipline may not (and may even be considered unethical) in another (Bozeman and Youtie 2016). For example, in Sociology, writing remains one of the most important contribution and tends to be required to be an author David. Pontille (2004). However, Pontille distinguishes French Sociology from American Sociology, where authorship norms are more inclusive. This highlights that the disciplinary context is not entirely independent from the local context. In physics, we find the special case of mega collaborations leading to articles signed by hundreds (or thousands) of authors; a phenomena that Cronin (2001) calls “hyperauthorsip”. In this context, it is the affiliation to the collective, and not a specific contribution to the publication, that justifies authorship Knorr-Cetina (1999).\n\n6.4.2.1 Guest and ghost authors\nDans certains cas, il peut exister une relation très faible, voire inexistante entre la contribution effectuée et l'obtention du statut d'auteur. Il est fréquent, par exemple, que des auteurs honorifiques (guest ou honorary authors) signent des articles alors que leur contribution était très superficielle ou encore nulle, alors que d'autres, les auteurs fantômes (ghost authors), ne signent pas alors qu'ils ont réalisé une partie importante du travail (Bates, Anić, Marušić et Marušić, 2004; Flanagin et al., 1998; Godlee, 1996; Huth, 1986b; Katz et Martin, 2007; Mowatt et al., 2002; Sismondo, 2009; Wager, 2007a; Wislar, Flanagin, Fontanarosa et Deangelis, 2011).\n\n\n6.4.2.2 Acknowledgements\nDe plus, il est fréquent que les individus ainsi exclus de la liste des auteurs soient remerciés dans l'article (Cronin et Overfelt, 1994; Laudel, 2002). Les différences disciplinaires observées dans le nombre d'auteurs moyen par article sont d'ailleurs grandement réduites lorsqu'on tient compte à la fois des auteurs et des individus remerciés (Paul-Hus et al., 2017), ce qui suggère que ces différences ne découlent pas seulement du nombre de chercheurs ayant contribué à la recherche, mais peut-être aussi de différences disciplinaires dans les pratiques d'attribution du statut d'auteur, possiblement en ce qui concerne la valorisation des contributions des techniciens ou des assistants de recherche. Il importe ici de souligner la distinction entre l'auteur fantôme et le remercié. Le premier désigne l'individu qui n'a pas obtenu le statut d'auteur alors que sa contribution aurait normalement justifié une telle reconnaissance. Les remerciements sont plutôt utilisés, en principe, pour souligner une contribution dont l'ampleur ne justifie pas l'attribution du statut d'auteur (Kassirer et Angell, 1991). L'existence des auteurs honorifiques, des auteurs fantômes et des contributeurs remerciés met en évidence le fait que la liste des auteurs peut dans certains cas mener à une sous-estimation (Moulopoulos, Sideris et Georgilis, 1983; Paul-Hus, Mongeon, Sainte-Marie et Larivière, 2017; Rennie et Flanagin, 1994) ou à une surestimation (King, 2000) du nombre de chercheurs ayant contribué à une recherche.\n\n\n\n6.4.3 Ordering authors\nAuthorship decisions are not only about who will be an author but also in what order the names will be listed, which is usually intended to reflect the importance of individual contributions to the work (Zuckerman 1968). In this section, we survey ordering approaches used in science.\n\n6.4.3.1 Alphabetical order\nAlphabetical order is sometimes used to distribute credit equally between members of the research team. It is also standard practice in some fields like Mathematics and Economics. This mode of ordering is also often used in hyperauthorship situations where the specific individual contributions are difficult to capture and order Knorr-Cetina (1999). Sometimes the authors will be grouped by institution or country first, and then ordered alphabetically within each group. The use of alphabetical in some cases but not always can generate some ambiguity since the first (or last) authors might be perceived has having made more important contributions than others even if that is not the case. Furthermore, it is possible for alphabetical order to occur by chance, sending a signal that authors may have contributed equally despite it not being the case (Zuckerman 1968).\n\n\n6.4.3.2 Decreasing order of contribution importance\nAuthors can also be listed based on the importance of their contribution, the first authors having typically contributed the most. This is a dominant mode of name ordering in Social Sciences and Humanities where writing is key (David. Pontille 2004). One limitation of this approach is that there is only one author at any position in the list. This issue is sometimes addressed by adding a note in the byline indicating that some authors contributed equally to the work (Hu 2009). According to Zuckerman (1967), prominent researchers sometimes let there less well-known collaborators take first authorship even if they may have contributed less.\n\n\n6.4.3.3 From the outside in\nUne variante de l'ordre basé sur la contribution décrit au paragraphe précédent est souvent utilisée dans certaines disciplines des sciences naturelles ou biomédicales où le travail de recherche s'effectue typiquement dans des laboratoires. La différence est que les deux extrémités de la liste des auteurs (la première et la dernière position) sont les plus importantes (Pontille, 2004, 2006). Le premier auteur, qui est généralement un doctorant ou un chercheur postdoctoral responsable de l'expérimentation. Le dernier auteur est généralement le directeur du laboratoire; celui qui détient l'autorité. Les autres contributeurs sont listés entre ces deux extrêmes en fonction de l'importance de leur contribution. Les contributions les moins importantes se retrouvant au centre de la liste (Pontille, 2004, 2006).\n\n\n6.4.3.4 Hybrid (partial alphabetical order)\nZuckerman (1968) décrit un dernier type d'ordonnancement que nous appellerons « ordre alphabétique partiel » et selon lequel un sous-ensemble des auteurs est en ordre alphabétique afin de mettre en évidence la contribution particulière d'un auteur (normalement le premier ou le dernier). Cette idée a été reprise par Mongeon et al. (2017) qui ont mesuré la prévalence des cas où seul un sous-ensemble des auteurs situés au centre de la liste est en ordre alphabétique. L'ordre alphabétique est ici utilisé pour créer une distinction entre les auteurs principaux (au début de la liste), les auteurs ayant joué un rôle de supervision (à la fin de la liste) et les autres (au milieu de la liste, en ordre alphabétique), dont la contribution est généralement moindre (Mongeon et al., 2017). Bien que les modes d'ordonnancement des auteurs présentés ci-dessus soient les plus fréquemment utilisés, il arrive aussi que les chercheurs utilisent d'autres critères, parfois arbitraires ou ludiques, pour déterminer l'ordre des auteurs (Cabanac, 2015).\n\n\n\n6.4.4 Categorizing authors\nCertains auteurs ont proposé de diviser les auteurs en trois groupes. Par exemple, Perneger et al. (2017) ont analysé les contributions individuelles des auteurs de 1139 articles du domaine biomédical et distingué trois types de profils : 1) le penseur (thinker), qui conçoit l'étude, obtient le financement, et révise l'article, 2) le soldat, qui fournit du matériel, du soutien administratif ou technique, ou participe à la collecte des données, et 3) le scribe, qui analyse et interprète les résultats et rédige l'article. De leur côté, Baerlocher, Newton, Gautam, Tomlinson et Detsky (2007) distinguent les trois types d'auteurs suivants :\n1. Les auteurs principaux (primary) : ils participent à la planification et à la réalisation de l'étude ainsi qu'à la rédaction de l'article. Ils sont en mesure d'expliquer l'ensemble des résultats de l'étude, et se portent garants de l'exactitude des informations rapportées dans l'article. Ils apparaissent généralement au début de la liste des auteurs.\n2. Les auteurs seniors/superviseurs (senior/supervisory) : ils participent à la planification et à la supervision de l'étude ainsi qu'à la révision substantielle de l'article. Comme les auteurs principaux, ils sont en mesure d'expliquer l'ensemble des résultats de l'étude et se portent aussi garants de l'exactitude des informations rapportées dans l'article. Ils apparaissent généralement à la fin de la liste des auteurs.\n3. Les auteurs contributeurs (contributing): Ils ont fait une contribution substantielle à la recherche, mais ils ne remplissent pas les critères pour l'obtention du statut d'auteur principal ou superviseur décrit ci-dessus. Ils n'ont pas à être en mesure d'expliquer l'ensemble des résultats de l'étude et ne sont pas responsables de l'exactitude des informations rapportées dans l'article. Ils apparaissent généralement au milieu de la liste des auteurs.\nUn parallèle peut être fait entre les typologies de Perneger et al. (2017) et de Baerlocher et al. (2007), les scribes s'apparentant aux auteurs principaux, les penseurs aux auteurs superviseurs, et les soldats aux auteurs contributeurs. De plus, les soldats et les auteurs contributeurs s'apparentent aux techniciens auxquels font référence Shapin (1989) et Hagstrom (1964), entre autres. Il est important toutefois de noter que le technicien selon la définition de Hagstrom (1964) est typiquement un salarié n'ayant pas d'intérêt dans les enjeux scientifiques, alors que les soldats et les auteurs contributeurs peuvent être d'autres chercheurs impliqués dans ce que Laudel (2002) appelle une collaboration de service."
  },
  {
    "objectID": "ch6.html#authorship-guidelines",
    "href": "ch6.html#authorship-guidelines",
    "title": "6  Measuring research output",
    "section": "6.4 Authorship guidelines",
    "text": "6.4 Authorship guidelines\nScholarly societies and journal editors have developed authorship guidelines in response to the lack of standards in authorship practices (Osborne and Holland 2009), the increased number of authors on articles, and the increase in cases of scientific misconduct Steen (2011) bringing forth the relationship between authorship and responsibility (Rennie and Flanagin 1994). Large research teams are most frequent in Natural Sciences, Engineering, and Health Sciences (Wuchty, Jones, and Uzzi 2007), and misconduct is most frequent in the Health Sciences (Fang, Steen, and Casadevall 2012). It is thus not surprising that these are research areas where authorship guidelines are frequently found Wager (2007).\nJournals rarely create their own authorship guidelines. Instead, they refer to the code of ethics of professional associations or publishers’ guidelines, which tend to refer to the recommendations of the International Committee of Medical Journal Editors (ICMJE). The ICMJE recommendations include four criteria that must be met by all authors of a paper:\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work;\nDrafting the work or revising it critically for important intellectual content;\nFinal approval of the version to be published;\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\nEven though there adoption is not generalized even in the Medical field (Bošnjak and Marušić 2012) these recommendation have a wide influence beyond this field. For instane, the Committee on Publication Ethics (COPE), the European Association of Science Editors (EASE), and major publishers like Elsevier, Springer-Nature, and Wiley-Blackwell refer to it.\nDespite the existence of these guidelines, there is no consensus on what constitutes a substantial contribution that should lead to authorship (Claxton 2005). Even when they do mention the type of tasks that matter for authorship, guidelines remain vague regarding the magnitude of the contribution required, simply stating that it should be substantial. Editors themselves are not convinced of the efficacy of the guidelines for reducing practices like guest and ghost authorship (Wager 2009). In sum, while guidelines might be able to help research teams with their authorship decisions, they seem to have a limited effect on these practices, and to be unable to address unethical authorship issues."
  },
  {
    "objectID": "ch6.html#collaboration",
    "href": "ch6.html#collaboration",
    "title": "6  Measuring research output",
    "section": "6.6 Collaboration",
    "text": "6.6 Collaboration\n\n\n\n\n\n\nBibliometrics in practice\n\n\n\n\nCollaboration (dichotomous)\nInterinstitutionnal collaboration\nInternational collaboration\nInterdisciplinary collaboration\nTeam size"
  },
  {
    "objectID": "ch6.html#dividing-credit-between-co-authors",
    "href": "ch6.html#dividing-credit-between-co-authors",
    "title": "6  Measuring research output",
    "section": "6.7 Dividing credit between co-authors",
    "text": "6.7 Dividing credit between co-authors\n\n\n\n\n\n\nBibliometrics in practice\n\n\n\n\nFull counting\nFractional counting\nHarmonic counting\n\n\n\n\n\n\n\n\n\n\nBibliometrics in practice\n\n\n\n\nFirst author\nLast author\nCorresponding author\n\n\n\n\n6.7.1 Multiple first authors and other equal contributors"
  },
  {
    "objectID": "ch6.html#hyper-authorship",
    "href": "ch6.html#hyper-authorship",
    "title": "6  Measuring research output",
    "section": "6.8 Hyper authorship",
    "text": "6.8 Hyper authorship"
  },
  {
    "objectID": "ch6.html#ghost-authorship",
    "href": "ch6.html#ghost-authorship",
    "title": "6  Measuring research output",
    "section": "6.9 Ghost authorship",
    "text": "6.9 Ghost authorship"
  },
  {
    "objectID": "ch6.html#guest-authorship",
    "href": "ch6.html#guest-authorship",
    "title": "6  Measuring research output",
    "section": "6.10 Guest authorship",
    "text": "6.10 Guest authorship"
  },
  {
    "objectID": "ch6.html#contributorship",
    "href": "ch6.html#contributorship",
    "title": "6  Measuring research output",
    "section": "6.11 Contributorship",
    "text": "6.11 Contributorship"
  },
  {
    "objectID": "ch6.html#authorship-guidelines-1",
    "href": "ch6.html#authorship-guidelines-1",
    "title": "6  Measuring research output",
    "section": "6.12 Authorship guidelines",
    "text": "6.12 Authorship guidelines\nPartly in reaction to the International Committee of Medical Journal Editors (ICMJE)\n\nCreteria #1\nCriteria #2\nCriteria #3\nCriteria #4\n\n\n\n\n\nBiagioli, Mario, Judith Crane, Pamela Derish, Mark Gruber, Drummond Rennie, and Richard Horton. 2003. “CSE Task Force on Authorship.” Draft White Paper [Sitio En Internet]. Encontrado En: Http://Www. Councilscienceeditors. Org/Services/Atf_whitepaper. Cfm.\n\n\nBirnholtz, Jeremy P. 2006. “What Does It Mean to Be an Author? The Intersection of Credit, Contribution, and Collaboration in Science.” Journal of the American Society for Information Science and Technology 57 (13): 1758–70. https://doi.org/10.1002/asi.20380.\n\n\nBozeman, Barry, and Jan Youtie. 2016. “Trouble in Paradise: Problems in Academic Research Co-Authoring.” Science and Engineering Ethics 22 (6): 1717–43. https://doi.org/10.1007/s11948-015-9722-5.\n\n\nCronin, Blaise. 2001. “Hyperauthorship: A Postmodern Perversion or Evidence of a Structural Shift in Scholarly Communication Practices?” Journal of the American Society for Information Science and Technology 52 (7): 558–69. https://doi.org/10.1002/asi.1097.\n\n\nHaeussler, C., and H. Sauermann. 2015. “The Anatomy of Teams: Division of Labor in Collaborative Knowledge Production.” Academy of Management Proceedings 2015 (1): 11383–83. https://doi.org/10.5465/AMBPP.2015.11383abstract.\n\n\nHagstrom, Warren O. 1964. “Traditional and Modern Forms of Scientific Teamwork.” Administrative Science Quarterly 9 (3): 241. https://doi.org/10.2307/2391440.\n\n\nHu, Xiaojun. 2009. “Loads of Special Authorship Functions: Linear Growth in the Percentage of “Equal First Authors” and Corresponding Authors.” Journal of the American Society for Information Science and Technology 60 (11): 2378–81. https://doi.org/10.1002/asi.21164.\n\n\nKatz, J.Sylvan, and Ben R Martin. 1997. “What Is Research Collaboration?” Research Policy 26 (1): 1–18. https://doi.org/10.1016/S0048-7333(96)00917-1.\n\n\nKnorr-Cetina, K. 1999. Epistemic Cultures: How the Sciences Make Knowledge. Cambridge, Mass: Harvard University Press.\n\n\nLarivière, Vincent, Nadine Desrochers, Benoît Macaluso, Philippe Mongeon, Adèle Paul-Hus, and Cassidy R. Sugimoto. 2016. “Contributorship and Division of Labor in Knowledge Production.” Social Studies of Science 46 (3): 417435. https://doi.org/10.1177/0306312716650046.\n\n\nLaudel, Grit. 2002. “What Do We Measure by Co-Authorships?” Research Evaluation 11 (1): 3–15. https://doi.org/10.3152/147154402781776961.\n\n\nPontille, David. 2006. “Qu’est-Ce Qu’un Auteur Scientifique.” Sciences de La Société 67: 77–93.\n\n\n———. 2016. Signer Ensemble: Contribution Et Évaluation En Sciences. Paris: Economica.\n\n\nPontille, David. 2004. La Signature Scientifique : Une Sociologie Pragmatique de l’attribution. Paris: CNRS.\n\n\nShibayama, Sotaro, John P. Walsh, and Yasunori Baba. 2012. “Academic Entrepreneurship and Exchange of Scientific Resources: Material Transfer in Life and Materials Sciences in Japanese Universities.” American Sociological Review 77 (5): 804–30. https://doi.org/10.1177/0003122412452874.\n\n\nSubramanyam, Krishnappa. 1983. “Bibliometric Studies of Research Collaboration: A Review.” Journal of Information Science 6 (1): 33–38.\n\n\nWalsh, John P., and You-Na Lee. 2015. “The Bureaucratization of Science.” Research Policy 44 (8): 1584–1600. https://doi.org/10.1016/j.respol.2015.04.010.\n\n\nZuckerman, Harriet A. 1967. “The Sociology of the Nobel Prizes.” Scientific American. http://europepmc.org/abstract/MED/6065922.\n\n\n———. 1968. “Patterns of Name Ordering Among Authors of Scientific Papers: A Study of Social Symbolism and Its Ambiguity.” American Journal of Sociology 74 (3): 276–91. https://doi.org/10.2307/2775535."
  },
  {
    "objectID": "ch6.html#the-functions-of-scientific-authorship",
    "href": "ch6.html#the-functions-of-scientific-authorship",
    "title": "6  Measuring research output",
    "section": "6.2 The functions of scientific authorship",
    "text": "6.2 The functions of scientific authorship\nAccording to Birnholtz (2006), the functions of authorship in science are to assign credit, ownership, and responsibility for discoveries, as well as to enable the existence of a reputation economy.\n\n6.2.1 Credit\nPeer recognition is the main reward obtained by researchers; this symbolic capital then allows them to obtain other rewards with symbolic but also economic value. It is therefore important for researchers to obtain this credit for their contributions and the codes of ethics of disciplinary associations (and, to a lesser extent, the editorial policies of journals) emphasize the importance of giving all contributors the credit due to them. It is thanks to authorship that the system of recognition of science can function, by granting an institutionalized form of credit (awards and positions, for instance) to researchers who have made important contributions to the advancement of knowledge.\n\n\n6.2.2 Ownership and responsibility\nThe link between authorship and ownership applies less to science than to other domains (e.g. the literary domain), because of the Mertonian norm of disinterestedness. According to Biagioli et al. (2003), only the original expression of scientific discoveries, generally under textual form, is the property of the authors, which grants them protection against plagiarism. Responsibility, which can be considered the other side of the ownership coin, has more relevance in science. The proper functioning of the scientific reward system relies on its ability to reward those who contribute to the goal and follow the norms, and its ability to punish (or at least not reward) those who act against the goals and norms of the system. Scientific fraud (e.g. fabrication, falsification, or plagiarism) is an example of non-compliance with the standards of science having a negative impact on the career of the researchers involved (Mongeon 2015), and even on their institutions and their disciplines (Azoulay et al. 2015). Although they remain relatively rare, the increase in cases of scientific fraud has contributed to the development of authorship guidelines which, as we will see later, emphasize the responsibility of researchers for the validity of the articles’ content.\n\n\n6.2.3 Reputation economy\nAuthorship, among other things, provides scientific capital to the researcher. Once known and recognized by the other researchers in the field, this scientific capital is converted into symbolic capital (Bourdieu 1987). Authorship is the basis of the social stratification of scientific fields, and all the layers of the hierarchy of rewards, from the simple citation to the Nobel Prize (Cole and Cole 1973). There is no social structure of science without authorship."
  },
  {
    "objectID": "ch6.html#authorship-in-a-collaborative-context",
    "href": "ch6.html#authorship-in-a-collaborative-context",
    "title": "6  Measuring research output",
    "section": "6.3 Authorship in a collaborative context",
    "text": "6.3 Authorship in a collaborative context\nThe byline of an article ultimately depends on two elements:\n\nThe nature and extent of the contribution of those involved in the works (who did what?)\nThe decisions made about naming and ordering authors (who will be an author, and in what order will the names be listed?)\n\nThese questions can be difficult to answer in a context where science is increasingly complex and collaborative Wuchty, Jones, and Uzzi (2007).\n\n6.3.1 Collaboration and division of labour\nLaudel (2002) identified six types de collaboration related to the types of contributions that individuals can make to a given research project:\n\nCollaboration involving a division of intellectual labour. Collaborators with a shared goal who make substantial intellectual contribution towards that goal. They are to some degree co-leading the research.\nService collaboration. Researchers who are called upon to produce routine work that require a specific expertise.\nProvision of access to research equipment. The collaborator does not perform any tasks related to the project but provides material, equipment, data, etc. used for the research.\nTransmission of know-how. The non-creative transmission of information stored in memory that is useful for the research.\nMutual stimulation: The stimulation and engagement through informal interactions that helps researchers develop their ideas.\nTrusted assessorship:Providing feedback on the work.\n\nNot all these forms of collaboration lead to authorship. Laudel (2002) found that authorship is usually attributed to collaborators of the first two types only.\nSubramanyam (1983) proposed four types of collaboration related to the hierarchical status of the involved individuals:\n\nSame-status collaborations.\nProfessor-student collaborations.\nSupervisor-assistant collaborations.\nResearcher-consultant collaborations.\n\nThe number of people required for a given project can have an influence on the types of collaboration that will take place. According to Walsh and Lee (2015), larger teams tend to lead to more bureaucratic organizational structures, which is characterized by increased division of labour, specialization and standardization of tasks, hierarchical relationships, and decentralized decision-making (Walsh et Lee, 2015).\nLes formes de collaboration privilégiées par les chercheurs ne sont pas seulement déterminées par leurs préférences personnelles, mais aussi par la nature des travaux de recherche effectués. Celle-ci est définie par l'objectif institutionnel de production de connaissances dont découlent des impératifs techniques et moraux (Merton, 1942). Cet objectif commun ou, selon Bourdieu (1976), les intérêts des agents dominants définissent les enjeux scientifiques (les objets de recherche) qui sont d'intérêts pour les agents du champ qui recherchent les investissements les plus payants en capital symbolique (Bourdieu, 1976). L'objet de recherche définit les ressources humaines et matérielles ainsi que les savoirs et savoir-faire requis, ce qui peut en quelque sorte imposer aux chercheurs certains types de collaboration et de division du travail. Cette idée est bien illustrée par la notion de culture épistémique (Knorr Cetina, 1999) qui désigne la particularité des pratiques de création des connaissances dans une discipline.\nAssigning authorship can be difficult when they are the outcome of the work of many individuals with different status, role and contributions. Decisions have to be made about 1) who will be an author (not all contributions lead to authorship) and 2) the order of the names, which is usually meant to indicate what share of the credit each one deserves (D. Pontille 2006). It can be extremely difficult for an external observer to determine who did what or what share of credit everyone deserves Rennie, Yank, and Emanuel (1997) .\n\n\n6.3.2 Naming authors\nUnlike the concept of author in literature, which is linked with writing, the concept of author in science is not and other forms of contributions can lead to authorship. Moreover, as D. Pontille (2006) points out, the act of writing is not necessarily sufficient to obtain the author status, and it is also possible to be an author without writing. So what kind of contributions do lead to authorship? Contributions involving the division of intellectual labour (first type of collaboration identified by Laudel (2002)). This linked between the substantial intellectual contribution and authorship generates little to no debate in the literature. However, whether authorship should be awarded to individuals with technical, routine, or less substantial contributions is not so clear.\nHagstrom (1964) tackles the question of authorship for technical contributions by looking at the historical context and distinguishing traditional collaboration from modern collaboration. He portrays the traditional technician as having little qualifications and being involved in the search for solutions to scientific problems but performing simple tasks designed and assigned by the scientist in exchange for economic capital. In contrast, the modern technician is a qualified professional performing complex tasks that the researcher employing them may not know how to perform themselves. This professionalization of the technician is nicely illustrated by Knorr-Cetina (1999) who tells the story of an established researcher who mastered the theories of his field but was unable to execute the technical tasks their research required. This type of relationship between the scientist and the technician is not generalized and technical contributions do not always lead to authorship today Laudel (2002).\nThe type of contribution is not always the only determinant. The same task performed by a paid technician or by another researcher may lead to authorship for the latter but not the former Shibayama, Walsh, and Baba (2012).\nThe importance of a discovery can also affect authorship attribution. Jabbehdari et Walsh (2017) found that the individuals who performed technical or less subtantial contributions are more often excluded from the byline of highly cited work.\nThe relationship between the nature and extent of a contribution and authorship is also determined by implicit disciplinary norms. Some type of contributions that often lead to authorship in a discipline may not (and may even be considered unethical) in another (Bozeman and Youtie 2016). For example, in Sociology, writing remains one of the most important contribution and tends to be required to be an author David. Pontille (2004). However, Pontille distinguishes French Sociology from American Sociology, where authorship norms are more inclusive. This highlights that the disciplinary context is not entirely independent from the local context. In physics, we find the special case of mega collaborations leading to articles signed by hundreds (or thousands) of authors; a phenomena that Cronin (2001) calls “hyperauthorsip”. In this context, it is the affiliation to the collective, and not a specific contribution to the publication, that justifies authorship Knorr-Cetina (1999).\n\n6.3.2.1 Guest and ghost authors\nDans certains cas, il peut exister une relation très faible, voire inexistante entre la contribution effectuée et l'obtention du statut d'auteur. Il est fréquent, par exemple, que des auteurs honorifiques (guest ou honorary authors) signent des articles alors que leur contribution était très superficielle ou encore nulle, alors que d'autres, les auteurs fantômes (ghost authors), ne signent pas alors qu'ils ont réalisé une partie importante du travail (Bates, Anić, Marušić et Marušić, 2004; Flanagin et al., 1998; Godlee, 1996; Huth, 1986b; Katz et Martin, 2007; Mowatt et al., 2002; Sismondo, 2009; Wager, 2007a; Wislar, Flanagin, Fontanarosa et Deangelis, 2011).\n\n\n6.3.2.2 Acknowledgements\nDe plus, il est fréquent que les individus ainsi exclus de la liste des auteurs soient remerciés dans l'article (Cronin et Overfelt, 1994; Laudel, 2002). Les différences disciplinaires observées dans le nombre d'auteurs moyen par article sont d'ailleurs grandement réduites lorsqu'on tient compte à la fois des auteurs et des individus remerciés (Paul-Hus et al., 2017), ce qui suggère que ces différences ne découlent pas seulement du nombre de chercheurs ayant contribué à la recherche, mais peut-être aussi de différences disciplinaires dans les pratiques d'attribution du statut d'auteur, possiblement en ce qui concerne la valorisation des contributions des techniciens ou des assistants de recherche. Il importe ici de souligner la distinction entre l'auteur fantôme et le remercié. Le premier désigne l'individu qui n'a pas obtenu le statut d'auteur alors que sa contribution aurait normalement justifié une telle reconnaissance. Les remerciements sont plutôt utilisés, en principe, pour souligner une contribution dont l'ampleur ne justifie pas l'attribution du statut d'auteur (Kassirer et Angell, 1991). L'existence des auteurs honorifiques, des auteurs fantômes et des contributeurs remerciés met en évidence le fait que la liste des auteurs peut dans certains cas mener à une sous-estimation (Moulopoulos, Sideris et Georgilis, 1983; Paul-Hus, Mongeon, Sainte-Marie et Larivière, 2017; Rennie et Flanagin, 1994) ou à une surestimation (King, 2000) du nombre de chercheurs ayant contribué à une recherche.\n\n\n\n6.3.3 Ordering authors\nAuthorship decisions are not only about who will be an author but also in what order the names will be listed, which is usually intended to reflect the importance of individual contributions to the work (Zuckerman 1968). In this section, we survey ordering approaches used in science.\n\n6.3.3.1 Alphabetical order\nAlphabetical order is sometimes used to distribute credit equally between members of the research team. It is also standard practice in some fields like Mathematics and Economics. This mode of ordering is also often used in hyperauthorship situations where the specific individual contributions are difficult to capture and order Knorr-Cetina (1999). Sometimes the authors will be grouped by institution or country first, and then ordered alphabetically within each group. The use of alphabetical in some cases but not always can generate some ambiguity since the first (or last) authors might be perceived has having made more important contributions than others even if that is not the case. Furthermore, it is possible for alphabetical order to occur by chance, sending a signal that authors may have contributed equally despite it not being the case (Zuckerman 1968).\n\n\n6.3.3.2 Decreasing order of contribution\nAuthors can also be listed based on the importance of their contribution, the first authors having typically contributed the most. This is a dominant mode of name ordering in Social Sciences and Humanities where writing is key (David. Pontille 2004). One limitation of this approach is that there is only one author at any position in the list. This issue is sometimes addressed by adding a note in the byline indicating that some authors contributed equally to the work (Hu 2009). According to Zuckerman (1967), prominent researchers sometimes let there less well-known collaborators take first authorship even if they may have contributed less.\n\n\n6.3.3.3 From the outside in\nA variant of the decreasing order of contribution exists predominantly some fields of Natural or Biomedical Sciences where research often occurs in laboratories. The difference is that the two ends of the byline (the first and last author positions) are the most important. The first author tends to be a junior researcher (a PhD student or postdoc) who lead the experiment. The last author is the lab director. Other contributors are listed in decreasing order of contribution between these two poles, those who contributed the least being in the middle of the byline (D. Pontille 2006).\n\n\n6.3.3.4 Hybrid (partial alphabetical order)\nZuckerman (1968) décrit un dernier type d'ordonnancement que nous appellerons « ordre alphabétique partiel » et selon lequel un sous-ensemble des auteurs est en ordre alphabétique afin de mettre en évidence la contribution particulière d'un auteur (normalement le premier ou le dernier). Cette idée a été reprise par Mongeon et al. (2017) qui ont mesuré la prévalence des cas où seul un sous-ensemble des auteurs situés au centre de la liste est en ordre alphabétique. L'ordre alphabétique est ici utilisé pour créer une distinction entre les auteurs principaux (au début de la liste), les auteurs ayant joué un rôle de supervision (à la fin de la liste) et les autres (au milieu de la liste, en ordre alphabétique), dont la contribution est généralement moindre (Mongeon et al., 2017). Bien que les modes d'ordonnancement des auteurs présentés ci-dessus soient les plus fréquemment utilisés, il arrive aussi que les chercheurs utilisent d'autres critères, parfois arbitraires ou ludiques, pour déterminer l'ordre des auteurs (Cabanac, 2015).\n\n\n\n6.3.4 Categorizing authors\nSome scholars have proposed to divide authors in categories. Perneger et al. (2017) identified three types of authors in biomedical research:\n\nThe thinker who designs the work, acquires funding for it, and revises the article.\nThe soldier who provides material, administrative or technical support or participates in the data collection.\nThe scribe who analyzes and interprets the results and drafts the article.\n\nSimilarly, Baerlocher et al. (2007) proposed the following categories:\n\nPrimary authors participate to the planing and execution of the study, and to drafting the article. They are able to explain all the findings and share responsibility for the exactitude of the information reported in the article. They generally appear at the beginning of the byline.\nSenior/supervisory authors plan and supervise the study, and subtantialy revise the draft. Like primary authors, they can explain all the findings in the study and share responsibility for the exactitude of the infomration reported in the article. They are generally at the end of the byline.\nContributing authors make a substantial contribution to the work but do not meat the criteria or primary and senior/supervisory authorship. They are not necessarily able to explain all of the findings of the study and they are not responsible for the exactitude of the information reported in the article. They generally appear in the middle of the byline."
  },
  {
    "objectID": "ch6.html#contribution-statements",
    "href": "ch6.html#contribution-statements",
    "title": "6  Measuring research output",
    "section": "6.5 Contribution statements",
    "text": "6.5 Contribution statements\nRennie, Yank, and Emanuel (1997) proposed a model that would replace authors with a list of contributors and guarantors. In their model, contributors include everyone who contributed, no matter the nature and extent of their contribution, and guarantors are those contributors who are responsible for the integrity of the work as a whole. In addition, the article should include a description of the work done by each contributor. The model is recommended (but not required) by the ICMJE guidelines and the Council of Science Editors, and it is applied (although partially) by an increasingly large number of journals requiring that the respective contributions of the authors be indicated, typically in a contribution statement.\nHowever, the informative value of these statements remain limited. Firstly, only the type (and not the extent) of contributions are indicated. According to Ilakovac et al. (2006), contribution statements are also imprecise as they rely on the limited memory (and limited motivation) of researchers who are asked to produce them. A third of the participants to a survey indicated that contribution statements provide are not more informative than the order in which the authors are listed (Sauermann and Haeussler 2017)."
  },
  {
    "objectID": "ch6.html#references",
    "href": "ch6.html#references",
    "title": "6  Measuring research output",
    "section": "6.5 References",
    "text": "6.5 References\n\n\n\n\nAzoulay, Pierre, Jeffrey L Furman, Krieger, and Fiona Murray. 2015. “Retractions.” Review of Economics and Statistics 97 (5): 1118–36. https://doi.org/10.1162/REST_a_00469.\n\n\nBaerlocher, Mark Otto, Marshall Newton, Tina Gautam, George Tomlinson, and Allan S Detsky. 2007. “The Meaning of Author Order in Medical Research.” Journal of Investigative Medicine 55 (4): 174–80. https://doi.org/10.2310/6650.2007.06044.\n\n\nBates, Tamara, Ante Anić, Matko Marušić, and Ana Marušić. 2004. “Authorship Criteria and Disclosure of Contributions.” JAMA 292 (1): 86–88. https://doi.org/10.1001/jama.292.1.86.\n\n\nBiagioli, Mario, Judith Crane, Pamela Derish, Mark Gruber, Drummond Rennie, and Richard Horton. 2003. “CSE Task Force on Authorship.” Draft White Paper [Sitio En Internet]. Encontrado En: Http://Www. Councilscienceeditors. Org/Services/Atf_whitepaper. Cfm.\n\n\nBirnholtz, Jeremy P. 2006. “What Does It Mean to Be an Author? The Intersection of Credit, Contribution, and Collaboration in Science.” Journal of the American Society for Information Science and Technology 57 (13): 1758–70. https://doi.org/10.1002/asi.20380.\n\n\nBošnjak, Lana, and Ana Marušić. 2012. “Prescribed Practices of Authorship: Review of Codes of Ethics from Professional Bodies and Journal Guidelines Across Disciplines.” Scientometrics 93 (3): 751–63. https://doi.org/10.1007/s11192-012-0773-y.\n\n\nBourdieu, Pierre. 1987. Choses Dites. Paris: Editions de minuit.\n\n\nBozeman, Barry, and Jan Youtie. 2016. “Trouble in Paradise: Problems in Academic Research Co-Authoring.” Science and Engineering Ethics 22 (6): 1717–43. https://doi.org/10.1007/s11948-015-9722-5.\n\n\nClaxton, Larry D. 2005. “Scientific Authorship. Part 1. A Window into Scientific Fraud?” Mutation Research 589 (1): 17–30. https://doi.org/10.1016/j.mrrev.2004.07.003.\n\n\nCole, Jonathan R, and Stephen Cole. 1973. Social Stratification in Science. Chicago, IL: University of Chicago Press.\n\n\nCronin, Blaise. 2001. “Hyperauthorship: A Postmodern Perversion or Evidence of a Structural Shift in Scholarly Communication Practices?” Journal of the American Society for Information Science and Technology 52 (7): 558–69. https://doi.org/10.1002/asi.1097.\n\n\nFang, FC Ferric C, R Grant Steen, and Arturo Casadevall. 2012. “Misconduct Accounts for the Majority of Retracted Scientific Publications.” Proceedings of the National Academy of Sciences 109 (42): 17028–33. https://doi.org/10.1073/pnas.1212247109.\n\n\nFlanagin, Annette, Lisa A Carey, Phil B Fontanarosa, Stephanie G Phillips, Brian P Pace, George D Lundberg, and Drummond Rennie. 1998. “Prevalence of Articles with Honorary Authors and Ghost Authors in Peer-Reviewed Medical Journals.” JAMA 280 (3): 222–24. https://doi.org/10.1001/jama.280.3.222.\n\n\nHaeussler, C., and H. Sauermann. 2015. “The Anatomy of Teams: Division of Labor in Collaborative Knowledge Production.” Academy of Management Proceedings 2015 (1): 11383–83. https://doi.org/10.5465/AMBPP.2015.11383abstract.\n\n\nHagen, Nils T. 2010. “Harmonic Publication and Citation Counting: Sharing Authorship Credit Equitably - Not Equally, Geometrically or Arithmetically.” Scientometrics 84 (3): 785–93. https://doi.org/10.1007/s11192-009-0129-4.\n\n\nHagstrom, Warren O. 1964. “Traditional and Modern Forms of Scientific Teamwork.” Administrative Science Quarterly 9 (3): 241. https://doi.org/10.2307/2391440.\n\n\nHu, Xiaojun. 2009. “Loads of Special Authorship Functions: Linear Growth in the Percentage of “Equal First Authors” and Corresponding Authors.” Journal of the American Society for Information Science and Technology 60 (11): 2378–81. https://doi.org/10.1002/asi.21164.\n\n\nIlakovac, V., K. Fister, M. Marusic, and A. Marusic. 2006. “Reliability of Disclosure Forms of Authors’ Contributions.” Canadian Medical Association Journal 176 (1): 41–46. https://doi.org/10.1503/cmaj.060687.\n\n\nJabbehdari, Sahra, and John P. Walsh. 2017. “Authorship Norms and Project Structures in Science.” Science, Technology, & Human Values, March, 016224391769719–19. https://doi.org/10.1177/0162243917697192.\n\n\nKassirer, Jerome P., and Marcia Angell. 1991. “On Authorship and Acknowledgments.” New England Journal of Medicine 325 (21): 1510–12. https://doi.org/10.1056/NEJM199111213252112.\n\n\nKnorr-Cetina, K. 1999. Epistemic Cultures: How the Sciences Make Knowledge. Cambridge, Mass: Harvard University Press.\n\n\nLarivière, Vincent, Nadine Desrochers, Benoît Macaluso, Philippe Mongeon, Adèle Paul-Hus, and Cassidy R. Sugimoto. 2016. “Contributorship and Division of Labor in Knowledge Production.” Social Studies of Science 46 (3): 417435. https://doi.org/10.1177/0306312716650046.\n\n\nLaudel, Grit. 2002. “What Do We Measure by Co-Authorships?” Research Evaluation 11 (1): 3–15. https://doi.org/10.3152/147154402781776961.\n\n\nMongeon, Philippe. 2015. “Costly Collaborations.” JASIST 15 (2): 125–45.\n\n\nMongeon, Philippe, Elise Smith, Bruno Joyal, and Vincent Larivière. 2017. “The Rise of the Middle Author: Investigating Collaboration and Division of Labor in Biomedical Research Using Partial Alphabetical Authorship.” Edited by Miguel A. Andrade-Navarro. PLOS ONE 12 (9): e0184601. https://doi.org/10.1371/journal.pone.0184601.\n\n\nMowatt, Graham, Liz Shirran, Jeremy M Grimshaw, Drummond Rennie, Annette Flanagin, Veronica Yank, Graeme MacLennan, Peter C Gøtzsche, and Lisa A Bero. 2002. “Prevalence of Honorary and Ghost Authorship in Cochrane Reviews.” JAMA 287 (21): 2769–71. https://doi.org/10.1001/jama.287.21.2769.\n\n\nOsborne, Jason W., and Abigail Holland. 2009. “What Is Authorship, and What Should It Be? A Survey of Prominent Guidelines for Determining Authorship in Scientific Publications.” Practical Assessment, Research & Evaluation 14 (15). https://doi.org/10.7275/25PE-BA85.\n\n\nPaul-Hus, Adèle, Philippe Mongeon, Maxime Sainte-Marie, and Vincent Larivière. 2017. “The Sum of It All: Revealing Collaboration Patterns by Combining Authorship and Acknowledgements.” Journal of Informetrics 11 (1): 8087.\n\n\nPerneger, Thomas V, Antoine Poncet, Marc Carpentier, Thomas Agoritsas, Christophe Combescure, and Angèle Gayet-Ageron. 2017. “Thinker, Soldier, Scribe: Cross-Sectional Study of Researchers’ Roles and Author Order in the Annals of Internal Medicine.” BMJ Open 7 (6): e013898–98. https://doi.org/10.1136/bmjopen-2016-013898.\n\n\nPontille, David. 2004. La Signature Scientifique : Une Sociologie Pragmatique de l’attribution. Paris: CNRS.\n\n\n———. 2006. “Qu’est-Ce Qu’un Auteur Scientifique.” Sciences de La Société 67: 77–93.\n\n\n———. 2016. Signer Ensemble: Contribution Et Évaluation En Sciences. Paris: Economica.\n\n\nRennie, Drummond, and Annette Flanagin. 1994. “Authorship! Authorship! Guests, Ghosts, Grafters, and the Two-Sided Coin.” JAMA 271 (6): 469–69. https://doi.org/10.1001/jama.1994.03510300075043.\n\n\nRennie, Drummond, Veronica Yank, and Linda Emanuel. 1997. “When Authorship Fails: A Proposal to Make Contributors Accountable.” JAMA 278 (7): 579–79. https://doi.org/10.1001/jama.1997.03550070071041.\n\n\nSauermann, Henry, and Carolin Haeussler. 2017. “Authorship and Contribution Disclosures.” Science Advances 3 (11): e1700404. https://doi.org/10.1126/sciadv.1700404.\n\n\nShibayama, Sotaro, John P. Walsh, and Yasunori Baba. 2012. “Academic Entrepreneurship and Exchange of Scientific Resources: Material Transfer in Life and Materials Sciences in Japanese Universities.” American Sociological Review 77 (5): 804–30. https://doi.org/10.1177/0003122412452874.\n\n\nSismondo, Sergio. 2009. “Ghosts in the Machine: Publication Planning in the Medical Sciences.” Social Studies of Science 39 (2): 171–98. https://doi.org/10.1177/0306312708101047.\n\n\nSteen, R. G. 2010. “Retractions in the Scientific Literature: Is the Incidence of Research Fraud Increasing?” Journal of Medical Ethics 37 (4): 249–53. https://doi.org/10.1136/jme.2010.040923.\n\n\nSubramanyam, Krishnappa. 1983. “Bibliometric Studies of Research Collaboration: A Review.” Journal of Information Science 6 (1): 33–38.\n\n\nWager, Elizabeth. 2007. “Authors, Ghosts, Damned Lies, and Statisticians.” PLoS Medicine 4 (1): e34–34. https://doi.org/10.1371/journal.pmed.0040034.\n\n\n———. 2009. “Recognition, Reward and Responsibility: Why the Authorship of Scientific Papers Matters.” Maturitas 62 (2): 109–12. https://doi.org/10.1016/j.maturitas.2008.12.001.\n\n\nWalsh, John P., and You-Na Lee. 2015. “The Bureaucratization of Science.” Research Policy 44 (8): 1584–1600. https://doi.org/10.1016/j.respol.2015.04.010.\n\n\nWislar, Joseph S, Annette Flanagin, Phil B Fontanarosa, and Catherine D DeAngelis. 2011. “Honorary and Ghost Authorship in High Impact Biomedical Journals: A Cross Sectional Survey.” BMJ 343 (October): d6128–28. https://doi.org/10.1136/bmj.d6128.\n\n\nWuchty, Stefan, BF Benjamin F Jones, and Brian Uzzi. 2007. “The Increasing Dominance of Teams in Production of Knowledge.” Science 316 (5827): 1036–39. https://doi.org/10.1126/science.1136099.\n\n\nZuckerman, Harriet A. 1967. “The Sociology of the Nobel Prizes.” Scientific American. http://europepmc.org/abstract/MED/6065922.\n\n\n———. 1968. “Patterns of Name Ordering Among Authors of Scientific Papers: A Study of Social Symbolism and Its Ambiguity.” American Journal of Sociology 74 (3): 276–91. https://doi.org/10.2307/2775535."
  },
  {
    "objectID": "ch6.html#authorship-practices-an-norms",
    "href": "ch6.html#authorship-practices-an-norms",
    "title": "6  Measuring research output",
    "section": "6.3 Authorship practices an norms",
    "text": "6.3 Authorship practices an norms\nThe byline of an article ultimately depends on two elements:\n\nThe nature and extent of the contribution of those involved in the works (who did what?)\nThe decisions made about naming and ordering authors (who will be an author, and in what order will the names be listed?)\n\nThese questions can be difficult to answer in a context where science is increasingly complex and collaborative (Wuchty, Jones, and Uzzi 2007).\n\n6.3.1 Collaboration and division of labour\nLaudel (2002) identified six types of collaboration related to the types of contributions that individuals can make to a given research project:\n\nCollaboration involving a division of intellectual labour. Collaborators with a shared goal who make substantial intellectual contribution towards that goal. They are to some degree co-leading the research.\nService collaboration. Researchers who are called upon to produce routine work that require a specific expertise.\nProvision of access to research equipment. The collaborator does not perform any tasks related to the project but provides material, equipment, data, etc. used for the research.\nTransmission of know-how. The non-creative transmission of information stored in memory that is useful for the research.\nMutual stimulation: The stimulation and engagement through informal interactions that helps researchers develop their ideas.\nTrusted assessorship: Providing feedback on the work.\n\nNot all these forms of collaboration lead to authorship. Laudel (2002) found that authorship is usually attributed to collaborators of the first two types only.\nSubramanyam (1983) proposed four types of collaboration related to the hierarchical status of the involved individuals:\n\nSame-status collaborations.\nProfessor-student collaborations.\nSupervisor-assistant collaborations.\nResearcher-consultant collaborations.\n\nThe number of people required for a given project can have an influence on the types of collaboration that will take place. According to Walsh and Lee (2015), larger teams tend to lead to more bureaucratic organizational structures, characterized by increased division of labour, specialization and standardization of tasks, hierarchical relationships, and decentralized decision-making.\nAssigning authorship can be difficult when a work is the outcome of  many individuals with different statuses, roles, and contributions. Decisions have to be made about 1) who will be an author (not all contributions lead to authorship) and 2) the order of the names, which is usually meant to indicate what share of the credit each one deserves (Pontille 2006). It can be extremely difficult for an external observer to determine who did what or what share of credit everyone deserves (Rennie, Yank, and Emanuel 1997).\n\n\n6.3.2 Naming authors\nUnlike the concept of author in literature, which is linked with writing, the concept of author in science is not and other forms of contributions can lead to authorship. Moreover, as Pontille (2006) points out, the act of writing is not necessarily sufficient to obtain author status, and it is also possible to be an author without writing. So what kind of contributions do lead to authorship? Contributions involving the division of intellectual labour (the first type of collaboration identified by Laudel (2002)). This link between substantial intellectual contribution and authorship generates little to no debate in the literature. However, whether authorship should be awarded to individuals with technical, routine, or less substantial contributions is not so clear.\nHagstrom (1964) tackles the question of authorship for technical contributions by looking at the historical context and distinguishing traditional collaboration from modern collaboration. He portrays the traditional technician as having few qualifications and being involved in the search for solutions to scientific problems but performing simple tasks designed and assigned by the scientist in exchange for economic capital. In contrast, the modern technician is a qualified professional performing complex tasks that the researcher employing them may not know how to perform themselves. This professionalization of the technician is nicely illustrated by Knorr-Cetina (1999) who tells the story of an established researcher who mastered the theories of his field but was unable to execute the technical tasks their research required. This type of relationship between the scientist and the technician is not generalized and technical contributions do not always lead to authorship today (Haeussler and Sauermann 2015; Larivière et al. 2016; Laudel 2002).\nThe type of contribution is not always the only determinant. The same task performed by a paid technician or by another researcher may lead to authorship for the latter but not the former (Pontille 2016; Shibayama, Walsh, and Baba 2012).\nThe importance of a discovery can also affect authorship attribution. Jabbehdari and Walsh (2017) found that the individuals who performed technical or less substantial contributions are more often excluded from the byline of highly cited work.\nThe relationship between the nature and extent of a contribution and authorship is also determined by implicit disciplinary norms. Some type of contributions that often lead to authorship in a discipline may not (and may even be considered unethical) in another (Bozeman and Youtie 2016). For example, in sociology, writing remains one of the most important contribution and tends to be required to be an author (Pontille 2004). However, Pontille distinguishes French sociology from American sociology, where authorship norms are more inclusive. This highlights that the disciplinary context is not entirely independent from the local context. In physics, we find the special case of mega-collaborations leading to articles signed by hundreds (or thousands) of authors; a phenomena that Cronin (2001) calls “hyperauthorsip”. In this context, it is the affiliation to the collective and not a specific contribution to the publication, that justifies authorship (Biagioli et al. 2003; Birnholtz 2006; Knorr-Cetina 1999).\n\n6.3.2.1 Guest authors, ghost authors, and acknowledgees\nIn some cases, the relationship between an individual’s contribution to a work and the authorship status can be weak or nonexistent. It is frequent for individuals to be named as (guest) authors despite having made very little or no contribution to the work, and for individuals to not appear on the byline despite having offered a significant contribution to the work (ghost authors) (Bates et al. 2004; Flanagin et al. 1998; Sismondo 2009; Mowatt et al. 2002; Wislar et al. 2011). Contributors excluded from the byline are sometimes acknowledged in the acknowledgements section of the article to signal their contribution to the work (Kassirer and Angell 1991). Paul-Hus et al. (2017) found that the disciplinary differences in the number of authors are greatly reduced when we consider combined authors and acknowledgees, which suggests that these differences are not a reflection of the different number of contributors involved, but also of disciplinary differences in authorship practices. Overall, the existence of guest authors, ghost authors, and acknowledgees suggests that the byline can misrepresent the composition and size of research teams.\n\n\n\n6.3.3 Ordering authors\nAuthorship decisions are not only about who will be an author but also in what order the names will be listed, which is usually intended to reflect the importance of individual contributions to the work (Zuckerman 1968). In this section, we survey ordering approaches used in science.\n\n6.3.3.1 Alphabetical order\nAlphabetical order is sometimes used to distribute credit equally between members of the research team. It is also standard practice in some fields like Mathematics and Economics. This mode of ordering is also often used in hyperauthorship situations where specific individual contributions are difficult to capture and order (Knorr-Cetina 1999). Sometimes the authors will be grouped by institution or country first, and then ordered alphabetically within each group. The use of alphabetical in some cases but not always can generate some ambiguity since the first (or last) authors might be perceived as having made more important contributions than others even if that is not the case. Furthermore, it is possible for alphabetical ordering to occur by chance, sending a signal that authors may have contributed equally despite it not being the case (Zuckerman 1968).\n\n\n6.3.3.2 Decreasing order of contribution\nAuthors can also be listed based on the importance of their contribution, the first authors having typically contributed the most. This is a dominant mode of name ordering in Social Sciences and Humanities where writing is key (Pontille 2004). One limitation of this approach is that there is only one author at any position in the list. This issue is sometimes addressed by adding a note in the byline indicating that some authors contributed equally to the work (Hu 2009). According to Zuckerman (1967), prominent researchers sometimes let their less well-known collaborators take first authorship, even if they may have contributed less.\n\n\n6.3.3.3 From the outside in\nA variant of the decreasing order of contribution exists predominantly some fields of natural or biomedical sciences where research often occurs in laboratories. The difference is that the two ends of the byline (the first and last author positions) are the most important. The first author tends to be a junior researcher (a PhD student or postdoc) who lead the experiment. The last author is the lab director. Other contributors are listed in decreasing order of contribution between these two poles, those who contributed the least being in the middle of the byline (Pontille 2006).\n\n\n6.3.3.4 Hybrid (partial alphabetical order)\nZuckerman (1968) described another type of ordering that I call partial alphabetical order, in which a subset of authors are listed alphabetically to highlight the contribution of the other authors who are not part of the alphabetical list. This phenomenon was studied by Mongeon et al. (2017) who showed how alphabetical order is used in biomedical research to distinguish primary authors (at the beginning of the byline), the supervisory authors (at the end of the byline), and the others (in the middle of the byline, in alphabetical order).\n\n\n\n6.3.4 Categorizing authors\nSome scholars have proposed to divide authors in categories. Perneger et al. (2017) identified three types of authors in biomedical research:\n\nThe thinker who designs the work, acquires funding for it, and revises the article.\nThe soldier who provides material, administrative or technical support or participates in the data collection.\nThe scribe who analyzes and interprets the results and drafts the article.\n\nSimilarly, Baerlocher et al. (2007) proposed the following categories:\n\nPrimary authors participate in the planning and execution of the study, and in drafting the article. They are able to explain all the findings and share responsibility for the exactitude of the information reported in the article. They generally appear at the beginning of the byline.\nSenior/supervisory authors plan and supervise the study, and substantially revise the draft. Like primary authors, they can explain all the findings in the study and share responsibility for the exactitude of the information reported in the article. They are generally at the end of the byline.\nContributing authors make a substantial contribution to the work but do not meet the criteria or primary and senior/supervisory authorship. They are not necessarily able to explain all of the findings of the study and they are not responsible for the exactitude of the information reported in the article. They generally appear in the middle of the byline.\n\n\n\n6.3.5 Authorship guidelines\nScholarly societies and journal editors have developed authorship guidelines in response to the lack of standards in authorship practices (Osborne and Holland 2009), the increased number of authors on articles, and the increase in cases of scientific misconduct (Pontille 2016; Steen 2010) bringing forth the relationship between authorship and responsibility (Rennie and Flanagin 1994). Large research teams are most frequent in Natural Sciences, Engineering, and Health Sciences (Wuchty, Jones, and Uzzi 2007), and misconduct is most frequent in the Health Sciences (Fang, Steen, and Casadevall 2012). It is thus not surprising that these are research areas where authorship guidelines are frequently found (Bošnjak and Marušić 2012; Wager 2007).\nJournals rarely create their own authorship guidelines. Instead, they refer to the code of ethics of professional associations or publishers’ guidelines, which tend to refer to the recommendations of the International Committee of Medical Journal Editors (ICMJE). The ICMJE recommendations include four criteria that must be met by all authors of a paper:\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work;\nDrafting the work or revising it critically for important intellectual content;\nFinal approval of the version to be published;\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\nEven though their adoption is not generalized even in the Medical field (Bošnjak and Marušić 2012) these recommendation have a wide influence beyond this field. For instance, the Committee on Publication Ethics (COPE), the European Association of Science Editors (EASE), and major publishers like Elsevier, Springer-Nature, and Wiley-Blackwell refer to it.\nDespite the existence of these guidelines, there is no consensus on what constitutes a substantial contribution that should lead to authorship (Claxton 2005). Even when they do mention the type of tasks that matter for authorship, guidelines remain vague regarding the magnitude of the contribution required, simply stating that it should be substantial. Editors themselves are not convinced of the efficacy of the guidelines for reducing practices like guest and ghost authorship (Wager 2009). In sum, while guidelines might be able to help research teams with their authorship decisions, they seem to have a limited effect on these practices, and to be unable to address unethical authorship issues.\n\n\n6.3.6 Contribution statements\nRennie, Yank, and Emanuel (1997) proposed a model that would replace authors with a list of contributors and guarantors. In their model, contributors include everyone who contributed, no matter the nature and extent of their contribution, and guarantors are those contributors who are responsible for the integrity of the work as a whole. In addition, the article should include a description of the work done by each contributor. The model is recommended (but not required) by the ICMJE guidelines and the Council of Science Editors, and it is applied (although partially) by an increasingly large number of journals requiring that the respective contributions of the authors be indicated, typically in a contribution statement.\nHowever, the informative value of these statements remains limited. Firstly, only the type (and not the extent) of contributions is indicated. According to Ilakovac et al. (2006), contribution statements are also imprecise as they rely on the limited memory (and limited motivation) of researchers who are asked to produce them. A third of the respondants to a survey indicated that contribution statements provide are not more informative than the order in which the authors are listed (Sauermann and Haeussler 2017)."
  },
  {
    "objectID": "ch6.html#bibliometric-indicators",
    "href": "ch6.html#bibliometric-indicators",
    "title": "6  Measuring research output",
    "section": "6.4 Bibliometric indicators",
    "text": "6.4 Bibliometric indicators\n\n6.4.1 Research output\nThere are two basic ways of counting the research contributions of an individual researcher or another entity (institution, country, etc.): one that does not take into account collaboration (full counting), and one that does take into account collaboration (fractional counting, harmonic counting, geometric counting, and arithmetic counting.\n\nMethods of counting publications in bibliometrics\n\n\n\n\n\n\nIndicator\nShare of credit for the author in the ith position in a byline of N authors\n\n\n\n\nFull counting\n1\n\n\nFractional counting\n\\[                                                                                                                                                            \n                                                                                                                                                           \\dfrac{1}{N}                                         \n                                                                                                                                                                                                        \\]\n\n\nHarmonic counting\n\\[                                                                                                                                                            \n                                                                                                                                       \\dfrac{\\dfrac{1}{i}}{1+\\dfrac{1}{2}+...+\\dfrac{1}{N}}                    \n                                                                                                                                                                                                        \\]\n\n\nGeometric counting\n\\[                                                                                                                                                            \n                                                                                                                                                      \\dfrac{2^{N-i}}{2^N-1}                                    \n                                                                                                                                                                                                        \\]\n\n\nArithmetic counting\n\\[                                                                                                                                                            \n                                                                                                                                                     \\dfrac{N+1-i}{1+2+...+N}                                   \n                                                                                                                                                                                                        \\]\n\n\n\nA study by Hagen (2010) suggests that harmonic counting best fits empirical data on credit allocation between co-authors in chemistry, medicine and chemistry. However, full and fractional counting remain the most widely used indicators.\n\n\n6.4.2 Collaboration\nBibliometric indicators can also be used to measure and characterize collaboration at the article level. The table below summarizes commonly used indicators of collaboration.\n\nBibliometric indicators of research collaboration\n\n\n\n\n\n\n\nIndicator\nVariable type\nDescription\n\n\n\n\nCollaboration\nCategorical\nThere is more than one author on the byline.\n\n\nInterinstitutional collaboration\nCategorical\nThere is more than one institution on the byline.\n\n\nInternational collaboration\nCategorical\nThere is more than one country on the byline.\n\n\nIntersectoral collaboration\nCategorical\nThere are institutions from more than one sector (e.g., university, industry, government) on the byline.\n\n\nTeam size\nNumerical\nThe number of authors on the byline.\n\n\n\nNote that collaboration can also be represented as a co-authorship network, in which two entities are linked when they appear together on the same publication."
  },
  {
    "objectID": "ch5.html#classification-the-noun",
    "href": "ch5.html#classification-the-noun",
    "title": "5  Classifying research",
    "section": "5.2 Classification (the noun)",
    "text": "5.2 Classification (the noun)\nA classification is essentially a list of categories in which entities can be classified (what disciplines exist). Many classifications have been developed by organizations around the world, and the best classification to use is often determined by availability as well as the subject and goals of the analysis. Below are a few examples of general classifications (that cover all disciplines) and disciplinary classifications (that cover single disciplines with a higher level of granularity than the general classifications).\n\n5.2.1 General classifications\n\nThe Science Metrix classification of research outputs categorizes scientific journals and articles in 5 domains, 20 fields and 174 subfields. The classification can be downloaded here.\nThe Scopus All Science Journal Classification (ASJC) divides journals in 334 fields and 4 research areas. The list of ASJC fields can be found here.\nThe National Science Foundation (NSF) classification is a tried and tested mutually exclusive classification used in the Science & Engineering Indicators since the 1970s. It was originally designed by CHI Research (Archambault, Beauchesne, and Caruso 2011). It contains three layers: 2 domains, 14 fields, and 143 specialties.\nThe Web of Science Subject Categories is a journal-level non-exclusive classification of journals in 250 subject categories available in the Web of Science. More information can be found here.\nThe Field of Science and Technology (FOS) classification of the OECD has 40 FOS grouped in six broad fields. Details can be found here. The Web of Science provides a mapping of the FOS and the Web of Science subject categories here.\nThe fields of research (FOR) from the Australian and New Zealand Standard Research Classification (ANZSRC). The FOR have three levels: divisions, groups, and fields. You can find more details on the ANZSRC website. This classification is used by the Dimensions database to assign FOR to articles and to calculate the field citation ratio (FCR), which we will discuss again in chapter 7.\n\n\n\n5.2.2 Disciplinary classifications\nSome disciplines have developed their own classification. Here are some examples.\n\nThe Medical Subject Headings (MeSH) are used in PubMed and Medline databases to facilitate searching (details here).\nThe Mathematics Subject Classification (MSC) (details here).\nThe Journal of Economics Literature (JEL) classification in economics (details here)\n\n\n\n5.2.3 Computational (bottom up) approaches\nThere exists a variety of computational approaches to divide any set of entities into groups that can (both do not always) make sense. Topic models are an example. That said, topic models are not that popular in bibliometric studies, for which researchers tend to adopt citation-based networks approaches (discussed in more details below).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classifying research</span>"
    ]
  },
  {
    "objectID": "ch5.html#classification-the-verb",
    "href": "ch5.html#classification-the-verb",
    "title": "5  Classifying research",
    "section": "5.3 Classification (the verb)",
    "text": "5.3 Classification (the verb)\nAssigning one or multiple discipline to documents or other entities can be a challenging task. Depending on the objective of your analysis, it may be preferable to use the classification already available in the database you are using (or to choose a database that uses the classification that best suits your needs).\nBut how do we assign a discipline to another entity, like a researcher?\n\nDo we use the discipline of their PhD?\nDo we use the discipline of their current department or faculty?\nDo we use the discipline classification of their articles or the journals in which they are published?\n\nThere is no right answer to these questions. Most often our main guide will be data availability and quality. The discipline of the Ph.D., for instance, is an information rarely available other than on the CV of the researcher. Furthermore, not all bibliographic records include the department of the authors, and when they do, they do not usually use controlled vocabulary so the same departments can come up with different names or spelling, and sometimes it will require some web searching to figure out the discipline to which the department could be assigned to. Moreover, the department names might not match your disciplinary classification which can make the pairing of departments with disciplines challenging. If the publications in our database are assigned to one or many disciplines, we can infer the discipline of the researcher using the papers they published, or the journals in which they published. What do we do when a researcher has 5 publications in Physics, 3 in chemistry and 4 in economics? Is that person a Physicist, a Chemist, an Economist? A mix?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classifying research</span>"
    ]
  },
  {
    "objectID": "ch5.html#describing-disciplines",
    "href": "ch5.html#describing-disciplines",
    "title": "5  Classifying research",
    "section": "5.4 Describing disciplines",
    "text": "5.4 Describing disciplines\nHow can we represent disciplines (topics of interest, journals, researchers) using bibliometrics? The “simple” answer is that we can describe a discipline by looking at the entities that we put into that box. For example, to describe the field of management , we can look at the articles or journals that were assigned to the management discipline through some classification mechanism. Then we can look at different entities associated with these articles such as terms used in the articles (as a proxy for topics), journals, researchers, institutions, and countries to describe what the discipline is about, what are its main journals, and who are the agents involved in it. This can be done in two main ways: rankings and networks.\n\n5.4.1 Rankings\nBibliometric data is usually asymmetrically distributed so that:\n\nA minority of researchers publish the majority of papers (Lotka’s law)\nA minority of journals publish the majority of works on a subject (Bradford’s law)\nA minority of words used account for the majority of occurrences (Zipf’s law)\nA minority of articles (of a researcher, a journal, a discipline, etc.) account for the majority of the citations in the whole (Larivière et al. 2016).\n\nBecause of this, disciplines or other areas of research are sometimes described by identifying the most frequent keywords, journals, authors, and institutions. Because these types of analyses are easily done with the Web version of databases like Web of Science and Scopus, they are very popular among scholars outside of the bibliometrics field who often use the ranking methods to provide an empirical account of the main topics and actors involved in their area of expertise. These descriptions are however very limited and are ideally used with complementary approaches such as networks.\n\n\n\n\n\n\nNote\n\n\n\nListing most frequent words used in titles and abstracts of articles published in a field without any other processing will not produce a meaningful representation of the topics of interest in a field given the presence of stop words (a, the, it, when, if) and other generic terms (research, data, results, analysis, etc.). So, it is important to filter out those words so the top words included in a table can adequately reflect the core topics of the discipline one is trying to describe.\nWhat is a topic?\nTopic and terms are not equivalent, and a topic is usually represented by a set of terms. For instance, the COVID-19 pandemic topic could be represented by terms like coronavirus, COVID, COVID-19, Omicron, etc. This is something to keep in mind as you dive into the data and try to determine what topics are or interest within a discipline or any set of publications.\n\n\n\n\n\n\n\n\nDelineating a research topic\n\n\n\nNow might be a good time to note that so far we discussed mostly disciplines and sub-disciplines, which as we just saw, can be characterized by the terms (or topics) that are the most frequently found in the articles published in the discipline. But what if one is interested in analyzing all of the research on a given topic, irrespective of the discipline?\nThis is where information searching skills can be useful. Because there is no better way to gather publications on a topic than querying the database with all the necessary keywords and filters to achieve the best possible recall and precision. Of course, there are multiple approach to do this, such as writing a very broad query that maximizes recall and filtering out the irrelevant publications, or writing a very specific query that maximizes precision, perhaps at the expense of recall.\nThe point is that classifications are not so useful when they do not include a category that represent the body of literature that one want to study.\nOne fun thing about choosing a research topic as your object of study, you can use disciplinary classification to analyze the diversity of disciplines that are interested in the topic.\n\n\n\n\n5.4.2 Disciplines as networks\nAs we saw in chapter 3, disciplines can be understood as networks of agents and research objects that are connected with one another to some degree. Different kinds of networks are typically used in bibliometrics:\n\n5.4.2.1 Co-occurrence networks\nCo-occurrence networks are undirected networks in which the edges are determined by the appearance of two entities in a set. Typical types of co-occurence networks include:\n\nTerm co-occurrence networks are constructed by considering in how many articles two terms appear together (co-occur). For example, if an article’s title, abstract or keywords contains the terms “COVID-19” and “vaccine”, then the terms “COVID-19” and “Vaccines” are linked.\nBibliographic coupling networks. Bibliographic coupling occurs when two articles contain a reference to the same article. For example, if article B and C both cite article A, then B and C are linked.\nCo-citation networks. A co-citation between two articles occurs when they are both cited together in another article. For example, if article B and C are cited by articles A, then B and C are related. One of the issues with co-citations is that they take time to accumulate, so networks of recent articles are less accurate since the citations have yet form links between the articles.\n\n\n\n5.4.2.2 Direct citation network\nDirect citation networks are the most common for of directed networks that one will come across in bibliometrics. In a directed network, the relationship between two nodes has a direction: So when article A cites article B, a direct citation link from article A to article B is created. This is not the same as a direct citation link between article B and article A. In fact, it is extremely unlikely for A to cite B AND for B to cite A. Indeed, articles usually cite other articles that have already been published, which means that the cited articles cannot cite the citing articles back.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classifying research</span>"
    ]
  },
  {
    "objectID": "ch5.html#references",
    "href": "ch5.html#references",
    "title": "5  Classifying research",
    "section": "5.5 References",
    "text": "5.5 References\n\n\n\n\nArchambault, Eric, Olivier H Beauchesne, and Julie Caruso. 2011. “Towards a Multilingual, Comprehensive and Open Scientific Journal Ontology.” In, 66–77. Leiden, Netherlands: Noyons, B., Ngulube, P., & Leta, J. (Eds.).\n\n\nLarivière, Vincent, Véronique Kiermer, Catriona J. MacCallum, Marcia McNutt, Mark Patterson, Bernd Pulverer, Sowmya Swaminathan, Stuart Taylor, and Stephen Curry. 2016. “A Simple Proposal for the Publication of Journal Citation Distributions.” http://dx.doi.org/10.1101/062109.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classifying research</span>"
    ]
  },
  {
    "objectID": "ch7.html#what-is-research-impact",
    "href": "ch7.html#what-is-research-impact",
    "title": "7  Measuring research impact",
    "section": "7.2 What is research impact",
    "text": "7.2 What is research impact\nIn the previous chapter, we focused on the scholarly publication as a measure of the output or productivity of an individual or group. The research article can then be considered as a knowledge unit, to which the authors of that unit have attached their name. However, for a multitude of reasons, not all publications are created equal. Some may be highly original and significant advances to knowledge in a field or across fields (a single publication may even revolutionize fields) while other may make a smaller contribution. So, while research output may be thought of as the number of units produced, research impact can be thought of as the difference that these units have made, their influence. So how can we measure that?\nWe have seen in Chapter 2 that being cited by peers is the most basic form of recognition that researchers receive within the reward system of science (Cole and Cole 1973). Citations have been shown to strongly correlate with prestigious awards and other institutionalized forms of recognition and can thus reflect the amount of scientific capital (Bourdieu 1975) that one possesses. As Merton (1968) put it:\n\n“The reference serves both instrumental and symbolic functions in the transmission and enlargement of knowledge. Instrumentally, it tells us of work we may not have known before, some of which may hold further interest for us; symbolically, it registers in the enduring archives the intellectual property of the acknowledged source by providing a pellet of peer recognition of the knowledge claim, accepted or expressly rejected, that was made in that source” (p. 622)\n\nCitations thus have gained acceptance as a measure of scientific excellence and as a tool for research evaluation (Narin 1976)."
  },
  {
    "objectID": "ch7.html#citation-based-impact-measures",
    "href": "ch7.html#citation-based-impact-measures",
    "title": "7  Measuring research impact",
    "section": "7.3 Citation-based impact measures",
    "text": "7.3 Citation-based impact measures\n\n7.3.1 Citation count\nThe citation count is the most basic (and perhaps most used) indicator of academic impact. It is obtained by counting the number of times that the paper appears in the reference lists of other papers. Its premise is simple the more a publication is cited, the more it has been influential. While the citation count cannot reveal what the nature of this influence might have been, it is considered to indicate, to some degree, the amount of that influence.\nOnce we obtain the citation count of every article in a set representing a research unit (e.g., a researcher, a journal, an institution), we can then compute the total citations for the set. For example, my Google Scholar profile shows the total number of citations of all my publications combined.\n\n\n7.3.2 Normalized citation count\nIs 37 a high number of citations for a publication? It depends on several factors, mainly the research field, the date of publication, and the type of document.\nDifferent fields or area of research have different epistemic cultures (Knorr Cetina 1991) and scholarly communication practices that can determine the potential number of citations that a publication can receive. This can limit the validity of an assessment based on citation counts alone, especially when this assessment is performed by someone with little knowledge of the field and what might be an excellent, or above-average citation count. Field-normalized indicators take into account field differences in citation rates by comparing the number of citations a paper has received to the citation counts of other papers in the same field.\n\n\n\n\n\n\nPause and reflect\n\n\n\nIn Chapter 5, we discussed the classification of research and research outputs and showed that there exists many different classification schemes, and that it can be challenging to determine which field (or fields) a paper belongs to. Consider the implications of that for research evaluation using field-normalized indicators.\n\n\nTime is an important factor as well since the potential citation count of an article published in 2022 is obviously much lower than the potential citation count of an article published in the same field in 2005.\nEditorials, or letters to the editors tend to receive fewer citations than research articles, which tend to receive fewer citations than review articles. Conference proceedings are also often less cited than journal publications. Therefore, it is generally good practice to take into account the document type when normalizing indicators.\nNormalized citation counts are usually calculated by simply dividing the citation count of a publication by the average citation count of all publication of the same type, in the same field, and published in the same year.\n\n\n\n\n\n\n\n\n\n\n\n\nArticle\nYear\nField\nType\nCitations\nMean for the same field, year and type\nNormalized citations\n\n\n\n\nA\n2017\nInformation science\nArticle\n84\n72\n1.167\n\n\nB\n2017\nInformation science\nReview\n75\n79\n0.950\n\n\nC\n2017\nComputer Science\nArticle\n95\n120\n0.792\n\n\nD\n2018\nInformation Science\nArticle\n50\n50\n1.000\n\n\nE\n2019\nInformation Science\nArticle\n40\n25\n1.600\n\n\n\n\n7.3.2.1 Challenges with normalization\nNormalizing indicators is not so easy. It implies that we have an adequate classification of research outputs that does not systematically disadvantage subgroups of publications. For example, classifying information science publications with computer science publications may be problematic because these are different fields with different scholarly communication practices. It also implies that we have consistent metadata on the document type, which is not always the case. Finally, and perhaps most importantly, the calculation of the denominator in the normalization formula requires that we have access to the citation counts of all articles published in the same year, field, and document type (essentially, the entire database is required to calculate the denominators). This is problematic in two ways: 1) access to the entire databases is costly (and thus rare), and 2) the normalized citation scores will depend on the database used (Web of Science, Scopus, Dimensions, OpenAlex, PubMed, etc.).\n\n\n\n7.3.3 Highly cited publications (HCP)\nAnother popular indicator is highly cited publications (HCP). This is a rank-based indicator that is obtained by ranking a set of publications from the same field and year (again, normalization is important), and then setting a threshold to distinguish HCPs from the rest. The 1st, 5th and 10th percentiles are often used as thresholds, but these choices are always arbitrary. Using this approach, we get a dichotomous variable (0 or 1) indicating whether each paper in a set belongs to the HCP group or not, which allows us to calculate the share of publications within the set that are highly cited as a size-independent indicator of impact.\n\n\n\n\n\n\nImportant note\n\n\n\nJust like the normalized citations, the identification of HCPs should also take into account the field, publication year and document type. The process thus suffers from the same challenges as citation normalization.\n\n\nThe table below provides an example of what the data could look like. We can see that article A and B are published in the same year and field, but only one is above the HCP threshold and thus considered an HCP. We also see that because the HCP threshold is higher in Computer Science, the 95 citations received by article C are not sufficient to make that paper an HCP. We also see that the threshold varies by year, so paper D and E both have 50 citations but only paper E is an HCP.\n\n\n\n\n\n\n\n\n\n\n\nArticle\nYear\nField\nCitation count\nHCP threshold\nHCP\n\n\n\n\nA\n2017\nInformation science\n84\n80\n1\n\n\nB\n2017\nInformation science\n75\n80\n0\n\n\nC\n2017\nComputer Science\n95\n97\n0\n\n\nD\n2018\nInformation Science\n50\n67\n0\n\n\nE\n2019\nInformation Science\n50\n45\n1\n\n\n\nOnce we have determined whether or not each paper meets the HCP threshold, we can calculate the share of HCPs for the set, which in this case would be 40%.\n\n\n7.3.4 H-index\nIn 2005, a physicist named Jorge Hirsch introduced a composite indicator that combines the output and impact dimensions of research performance into a single number: the h-index.\nThe h-index is equal to the number of publications with a citation number greater than or equal to h For instance, a researcher (or another unit) has an h-index of 10 if they published at least 10 articles cited at least 10 times.\n\n\n\nVisual representation of the calculation of the h-index\n\n\nAs we can see in the conclusion of Hirsch’s paper, he was arguing for the use of his indicator as an unbiased measure of scientific achievement that can be used to compare researchers competing for resources.\n\nIn summary, I have proposed an easily computable index, h, which gives an estimate of the importance, significance, and broad impact of a scientist’s cumulative research contributions. I suggest that this index may provide a useful yardstick with which to compare, in an unbiased way, different individuals competing for the same resource when an important evaluation criterion is scientific achievement. (Hirsch 2005)\n\nThe h-Index (along with its many variations) has been widely criticized (this blog post covers some of those criticisms) for not accounting for author position in the byline and for field differences. It also undervalues highly influential work since it is bounded by the number of publications, and it is correlated with academic age and with the much simpler total number of citations. Even Hirsch himself recognized the limits of his indicator and the potential adverse effects of its popularity.\nLudo Waltman and Nees Jan van Eck (Waltman and Eck 2011) argued that the h-index can create inconsistencies between single authors considered individually or as a group. For instance, five authors who co-authored the same five papers each cited five teams will have an h-index of 5 when considered individually or as a research unit. However, 5 authors who separately published two papers with 10 citations each will have individual h-indices of 2 and a collective h-index of 10. Waltman and van Eck Waltman and Eck (2011) argue that the share of HCPs is a better indicator since it doesn’t suffer from this limitation: researchers who perform better individually than others will maintain this advantage once aggregated into a unit.\n\n\n7.3.5 The journal impact factor\nThe Journal Impact Factor is a citation-based indicator designed to evaluate the relative influence of a journal. It is quite simply the average number of citations received during a given year by the articles published in the journal over the two previous years:\n\\[                                                                                                       \n                                                                                                     JIF_y = \\dfrac{Citations_y}{Publications_{y-1} + Publications_{y-2}}                                 \n                                                                                                                                               \\]\nSo the 2017 JIF for the journal X would be calculated by counting all citations received in 2017 by the articles published in the journal in 2015 and 2016 and dividing this citation count by the number of articles published in the journal for 2015 and 2016. Because citations take llonger to accumulate in the social sciences and humanities, a variant of the JIF that uses a five-year citation window rather than a two-year one was eventually introduced.\n\n\n\n\n\n\nOne of many\n\n\n\nThe term Journal Impact Factor is a trademark of Clarivate Analytics (the company that owns the Web of Science). Other similar indicators have been proposed such as the CiteScore, and the Source Normalized Impact per Paper (SNIP) and the Scimago Journal Ranking (SJR) used by Elsevier.\n\n\nAs we know, accumulating citations take time, which means that it is difficult to determine early on whether a piece of research is a “significant contribution” to science or not. Partly because they are immediately available, the JIF or other journal indicators or rankings are commonly used in the evaluation process. Such practices have, however, been widely criticized and using the journal as a proxy for the importance of a single publication is often considered a misuse of these journal indicators (some of these criticisms can be found in Larivière and Sugimoto (2019))."
  },
  {
    "objectID": "ch7.html#limits-of-citation-based-indicators",
    "href": "ch7.html#limits-of-citation-based-indicators",
    "title": "7  Measuring research impact",
    "section": "7.4 Limits of citation-based indicators",
    "text": "7.4 Limits of citation-based indicators\n\n7.4.1 Diversity in citation practices\nAn important factor that is not always at the forefront of discussions around citation-based indicators and research evaluation is the fact that a paper doesn’t just get cited by virtue of existing or based on its intrinsic characteristics; it is cited because another researcher referred to it in their own work. We might then ask: why do researchers cite other works?\nThere are two dominant theories of citation behaviour: the normative and the socio-constructivist theories. The normative view suggests that researchers mainly cite in order to acknowledge the work of their peers and predecessors and give credit where credit is due. It is the Mertonian view that was mentioned at the beginning of this chapter which supports the use of citations for evaluative purposes. The socio-constructivist view focuses on citations as a rhetorical device that is not meant to acknowledge, but rather to convince. It emphasizes the strategic and biased nature of citation choices. For instance, one might omit to cite sources that don’t align with their findings or arguments, or cite work of little relevance by prestigious scholars at the expense of more relevant work by less known scholars.\nBornmann and Daniel (2008) performed a review of studies that sought to empirically test those two theories. Their review suggests that reasons for citing are mixed and partly support both views, although support for the normative theory is stronger. They do however suggest that this is mostly true at higher level of aggregation (e.g., researchers with substantial publication records, research units, institutions).\nThis leads us to a second main limit of citation-based impact indicators: their limited reliability when working with small datasets (e.g., individual papers or researchers). This is partly due to the ambiguities surrounding the concept of impact and what citations are actually supposed to measure, and by the general concept of statistical power according to which small sample sizes increase the likelihood of failing to reject a null hypothesis.\nFinally, one of the most important limits of citation-based indicators is that they fail to account for other forms of impact. Indeed, citations are often seen to measure relatively accurately the academic impact of a publication or research unit, that is the contribution of theoretical or methodological advances to the field. However, the impact of academic research is not limited to the scientific realm. Research can also lead to economic development (economic impact), fuel technological innovation (technological impact), and have impact on policy, on public health, on the environment and on culture and society at large. In the next chapter, we will discuss “Altmetrics” (alternative metrics), which are a range of bibliometric indicators that seek to measure the impact of a publication outside of scholarly communication system. Could those new metrics succeed in measuring the social impact of research? We shall see."
  },
  {
    "objectID": "ch7.html#conclusion",
    "href": "ch7.html#conclusion",
    "title": "7  Measuring research impact",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIn this chapter, we introduced some of the most commonly used citation-based indicators of research impact and their calculation. While a large number of empirical studies have demonstrated a correlation between citations and the not always clearly defined concept that they claim to measure, caution remains necessary when using citations for evaluation purposes. Over the years, the research community published several declarations and manifestos that emphasize the challenges of quantitative research evaluation processes and propose best practices; the Leiden Manifesto and the San Francisco Declaration on Research Assessment (DORA) are good examples."
  },
  {
    "objectID": "ch7.html#references",
    "href": "ch7.html#references",
    "title": "7  Measuring research output",
    "section": "7.5 References",
    "text": "7.5 References\n\n\n\n\nAzoulay, Pierre, Jeffrey L Furman, Krieger, and Fiona Murray. 2015. “Retractions.” Review of Economics and Statistics 97 (5): 1118–36. https://doi.org/10.1162/REST_a_00469.\n\n\nBaerlocher, Mark Otto, Marshall Newton, Tina Gautam, George Tomlinson, and Allan S Detsky. 2007. “The Meaning of Author Order in Medical Research.” Journal of Investigative Medicine 55 (4): 174–80. https://doi.org/10.2310/6650.2007.06044.\n\n\nBates, Tamara, Ante Anić, Matko Marušić, and Ana Marušić. 2004. “Authorship Criteria and Disclosure of Contributions.” JAMA 292 (1): 86–88. https://doi.org/10.1001/jama.292.1.86.\n\n\nBiagioli, Mario, Judith Crane, Pamela Derish, Mark Gruber, Drummond Rennie, and Richard Horton. 2003. “CSE Task Force on Authorship.” Draft White Paper [Sitio En Internet]. Encontrado En: Http://Www. Councilscienceeditors. Org/Services/Atf_whitepaper. Cfm.\n\n\nBirnholtz, Jeremy P. 2006. “What Does It Mean to Be an Author? The Intersection of Credit, Contribution, and Collaboration in Science.” Journal of the American Society for Information Science and Technology 57 (13): 1758–70. https://doi.org/10.1002/asi.20380.\n\n\nBošnjak, Lana, and Ana Marušić. 2012. “Prescribed Practices of Authorship: Review of Codes of Ethics from Professional Bodies and Journal Guidelines Across Disciplines.” Scientometrics 93 (3): 751–63. https://doi.org/10.1007/s11192-012-0773-y.\n\n\nBourdieu, Pierre. 1987. Choses Dites. Paris: Editions de minuit.\n\n\nBozeman, Barry, and Jan Youtie. 2016. “Trouble in Paradise: Problems in Academic Research Co-Authoring.” Science and Engineering Ethics 22 (6): 1717–43. https://doi.org/10.1007/s11948-015-9722-5.\n\n\nClaxton, Larry D. 2005. “Scientific Authorship. Part 1. A Window into Scientific Fraud?” Mutation Research 589 (1): 17–30. https://doi.org/10.1016/j.mrrev.2004.07.003.\n\n\nCole, Jonathan R, and Stephen Cole. 1973. Social Stratification in Science. Chicago, IL: University of Chicago Press.\n\n\nCronin, Blaise. 2001. “Hyperauthorship: A Postmodern Perversion or Evidence of a Structural Shift in Scholarly Communication Practices?” Journal of the American Society for Information Science and Technology 52 (7): 558–69. https://doi.org/10.1002/asi.1097.\n\n\nFang, FC Ferric C, R Grant Steen, and Arturo Casadevall. 2012. “Misconduct Accounts for the Majority of Retracted Scientific Publications.” Proceedings of the National Academy of Sciences 109 (42): 17028–33. https://doi.org/10.1073/pnas.1212247109.\n\n\nFlanagin, Annette, Lisa A Carey, Phil B Fontanarosa, Stephanie G Phillips, Brian P Pace, George D Lundberg, and Drummond Rennie. 1998. “Prevalence of Articles with Honorary Authors and Ghost Authors in Peer-Reviewed Medical Journals.” JAMA 280 (3): 222–24. https://doi.org/10.1001/jama.280.3.222.\n\n\nHaeussler, C., and H. Sauermann. 2015. “The Anatomy of Teams: Division of Labor in Collaborative Knowledge Production.” Academy of Management Proceedings 2015 (1): 11383–83. https://doi.org/10.5465/AMBPP.2015.11383abstract.\n\n\nHagen, Nils T. 2010. “Harmonic Publication and Citation Counting: Sharing Authorship Credit Equitably - Not Equally, Geometrically or Arithmetically.” Scientometrics 84 (3): 785–93. https://doi.org/10.1007/s11192-009-0129-4.\n\n\nHagstrom, Warren O. 1964. “Traditional and Modern Forms of Scientific Teamwork.” Administrative Science Quarterly 9 (3): 241. https://doi.org/10.2307/2391440.\n\n\nHu, Xiaojun. 2009. “Loads of Special Authorship Functions: Linear Growth in the Percentage of “Equal First Authors” and Corresponding Authors.” Journal of the American Society for Information Science and Technology 60 (11): 2378–81. https://doi.org/10.1002/asi.21164.\n\n\nIlakovac, V., K. Fister, M. Marusic, and A. Marusic. 2006. “Reliability of Disclosure Forms of Authors’ Contributions.” Canadian Medical Association Journal 176 (1): 41–46. https://doi.org/10.1503/cmaj.060687.\n\n\nJabbehdari, Sahra, and John P. Walsh. 2017. “Authorship Norms and Project Structures in Science.” Science, Technology, & Human Values, March, 016224391769719–19. https://doi.org/10.1177/0162243917697192.\n\n\nKassirer, Jerome P., and Marcia Angell. 1991. “On Authorship and Acknowledgments.” New England Journal of Medicine 325 (21): 1510–12. https://doi.org/10.1056/NEJM199111213252112.\n\n\nKnorr-Cetina, K. 1999. Epistemic Cultures: How the Sciences Make Knowledge. Cambridge, Mass: Harvard University Press.\n\n\nLarivière, Vincent, Nadine Desrochers, Benoît Macaluso, Philippe Mongeon, Adèle Paul-Hus, and Cassidy R. Sugimoto. 2016. “Contributorship and Division of Labor in Knowledge Production.” Social Studies of Science 46 (3): 417435. https://doi.org/10.1177/0306312716650046.\n\n\nLaudel, Grit. 2002. “What Do We Measure by Co-Authorships?” Research Evaluation 11 (1): 3–15. https://doi.org/10.3152/147154402781776961.\n\n\nMongeon, Philippe. 2015. “Costly Collaborations.” JASIST 15 (2): 125–45.\n\n\nMongeon, Philippe, Elise Smith, Bruno Joyal, and Vincent Larivière. 2017. “The Rise of the Middle Author: Investigating Collaboration and Division of Labor in Biomedical Research Using Partial Alphabetical Authorship.” Edited by Miguel A. Andrade-Navarro. PLOS ONE 12 (9): e0184601. https://doi.org/10.1371/journal.pone.0184601.\n\n\nMowatt, Graham, Liz Shirran, Jeremy M Grimshaw, Drummond Rennie, Annette Flanagin, Veronica Yank, Graeme MacLennan, Peter C Gøtzsche, and Lisa A Bero. 2002. “Prevalence of Honorary and Ghost Authorship in Cochrane Reviews.” JAMA 287 (21): 2769–71. https://doi.org/10.1001/jama.287.21.2769.\n\n\nOsborne, Jason W., and Abigail Holland. 2009. “What Is Authorship, and What Should It Be? A Survey of Prominent Guidelines for Determining Authorship in Scientific Publications.” Practical Assessment, Research & Evaluation 14 (15). https://doi.org/10.7275/25PE-BA85.\n\n\nPaul-Hus, Adèle, Philippe Mongeon, Maxime Sainte-Marie, and Vincent Larivière. 2017. “The Sum of It All: Revealing Collaboration Patterns by Combining Authorship and Acknowledgements.” Journal of Informetrics 11 (1): 8087.\n\n\nPerneger, Thomas V, Antoine Poncet, Marc Carpentier, Thomas Agoritsas, Christophe Combescure, and Angèle Gayet-Ageron. 2017. “Thinker, Soldier, Scribe: Cross-Sectional Study of Researchers’ Roles and Author Order in the Annals of Internal Medicine.” BMJ Open 7 (6): e013898–98. https://doi.org/10.1136/bmjopen-2016-013898.\n\n\nPontille, David. 2004. La Signature Scientifique : Une Sociologie Pragmatique de l’attribution. Paris: CNRS.\n\n\n———. 2006. “Qu’est-Ce Qu’un Auteur Scientifique.” Sciences de La Société 67: 77–93.\n\n\n———. 2016. Signer Ensemble: Contribution Et Évaluation En Sciences. Paris: Economica.\n\n\nRennie, Drummond, and Annette Flanagin. 1994. “Authorship! Authorship! Guests, Ghosts, Grafters, and the Two-Sided Coin.” JAMA 271 (6): 469–69. https://doi.org/10.1001/jama.1994.03510300075043.\n\n\nRennie, Drummond, Veronica Yank, and Linda Emanuel. 1997. “When Authorship Fails: A Proposal to Make Contributors Accountable.” JAMA 278 (7): 579–79. https://doi.org/10.1001/jama.1997.03550070071041.\n\n\nSauermann, Henry, and Carolin Haeussler. 2017. “Authorship and Contribution Disclosures.” Science Advances 3 (11): e1700404. https://doi.org/10.1126/sciadv.1700404.\n\n\nShibayama, Sotaro, John P. Walsh, and Yasunori Baba. 2012. “Academic Entrepreneurship and Exchange of Scientific Resources: Material Transfer in Life and Materials Sciences in Japanese Universities.” American Sociological Review 77 (5): 804–30. https://doi.org/10.1177/0003122412452874.\n\n\nSismondo, Sergio. 2009. “Ghosts in the Machine: Publication Planning in the Medical Sciences.” Social Studies of Science 39 (2): 171–98. https://doi.org/10.1177/0306312708101047.\n\n\nSteen, R. G. 2010. “Retractions in the Scientific Literature: Is the Incidence of Research Fraud Increasing?” Journal of Medical Ethics 37 (4): 249–53. https://doi.org/10.1136/jme.2010.040923.\n\n\nSubramanyam, Krishnappa. 1983. “Bibliometric Studies of Research Collaboration: A Review.” Journal of Information Science 6 (1): 33–38.\n\n\nWager, Elizabeth. 2007. “Authors, Ghosts, Damned Lies, and Statisticians.” PLoS Medicine 4 (1): e34–34. https://doi.org/10.1371/journal.pmed.0040034.\n\n\n———. 2009. “Recognition, Reward and Responsibility: Why the Authorship of Scientific Papers Matters.” Maturitas 62 (2): 109–12. https://doi.org/10.1016/j.maturitas.2008.12.001.\n\n\nWalsh, John P., and You-Na Lee. 2015. “The Bureaucratization of Science.” Research Policy 44 (8): 1584–1600. https://doi.org/10.1016/j.respol.2015.04.010.\n\n\nWislar, Joseph S, Annette Flanagin, Phil B Fontanarosa, and Catherine D DeAngelis. 2011. “Honorary and Ghost Authorship in High Impact Biomedical Journals: A Cross Sectional Survey.” BMJ 343 (October): d6128–28. https://doi.org/10.1136/bmj.d6128.\n\n\nWuchty, Stefan, BF Benjamin F Jones, and Brian Uzzi. 2007. “The Increasing Dominance of Teams in Production of Knowledge.” Science 316 (5827): 1036–39. https://doi.org/10.1126/science.1136099.\n\n\nZuckerman, Harriet A. 1967. “The Sociology of the Nobel Prizes.” Scientific American. http://europepmc.org/abstract/MED/6065922.\n\n\n———. 1968. “Patterns of Name Ordering Among Authors of Scientific Papers: A Study of Social Symbolism and Its Ambiguity.” American Journal of Sociology 74 (3): 276–91. https://doi.org/10.2307/2775535.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measuring research output</span>"
    ]
  },
  {
    "objectID": "ch8.html#commercial-altmetric-data-sources",
    "href": "ch8.html#commercial-altmetric-data-sources",
    "title": "8  Altmetrics",
    "section": "8.2 Commercial Altmetric data sources",
    "text": "8.2 Commercial Altmetric data sources\n\n8.2.1 Altmetric\nAltmetric (https://www.altmetric.com/) is an appropriately-named data science company that supplies altmetric data. Using a variety of sources ranging from social media to policy documents, Altmetric tracks conversations around scholarly content. Their goal is to illuminate the attention that scientific articles are receiving in real-time, which allows authors and publishers to know what people are saying about their work.\n\n8.2.1.1 The Altmetric donut\nThe Altmetric donut illustrated here is designed to convey the amount and type of attention a scholarly article has received at a quick glance. The colors of the donut each represent a different attention source. This means that the amount of color in the donut varies based on how much attention an article has received from different sources.\nThe number in the middle of the donut is the Altmetric Attention Score, which is a weighted count of all the attention a scholarly article has received. It is calculated based on three factors:\n\nVolume: The more people mention an article, the higher its score will be.\nSources: Different sources of mentions contribute a different weighted amount to the Altmetric Attention Score. For example, being mentioned in a news article carries more weight than being mentioned in a tweet and thus contributes to a higher score.\nThe sharing frequency of the source: The amount that an author of a mention generally talks about scholarly articles is considered in the score. For example, a doctor who shares a link with other doctors on Twitter counts more than a journal’s Twitter account that automatically pushes out links to all their articles.\n\nAltogether, the Altmetric Attention Score is a single metric that is meant to communicate the level of online activity surrounding a research output.\n\n\n\n8.2.2 Plum Analytics\n\n\n8.2.3 Overton"
  },
  {
    "objectID": "ch8.html#open-altmetric-data-sources",
    "href": "ch8.html#open-altmetric-data-sources",
    "title": "8  Altmetrics",
    "section": "8.3 Open Altmetric data sources",
    "text": "8.3 Open Altmetric data sources\n\n8.3.1 Crossref Event Data\nCrossref Event data is a free source of altmetric data. Crossref to DataCitecrossref_dataciteCrossrefDataset citations from Crossref Items to DataCite ItemsDataCite to Crossrefdatacite_crossrefDataCiteDataset citations from DataCite Items to Crossref ItemsNewsfeednewsfeedCrossrefLinks to Items on blogs and websites with syndication feedsRedditredditCrossrefMentions and discussions of Items on RedditTwittertwitterCrossrefLinks to Items on TwitterWebwebCrossrefLinks of Items on web pagesWikipediawikipediaCrossrefReferences of Items on WikipediaWordpress.comwordpressdotcomCrossrefReferences of Items on Wordpress.com blogs\n\nHere’s a code that you can run in R to collect the events for a single DOI\n\nlibrary(httr)\nlibrary(stringr)\nlibrary(dplyr)\n\nemail <- \"YOUR EMAIL HERE\"\ndoi <- \"DOI HERE\"\nfilename <- \"event_data_output.csv\" # change the name of the output file as needed (as to be a .csv). \n\nevents_raw <- content(GET(str_c(\"https://api.eventdata.crossref.org/v1/events?mailto=\",\n                    email,\"&obj-id=\",doi,\"&rows=1000\", sep = \"\")))$message$event\n\nevents <- tibble()\nfor (i in 1:length(events_raw)) {\n  data<-tibble()\n  subj_id <- events_raw[[i]]$subj_id\n  relation <- events_raw[[i]]$relation_type_id\n  obj_id <- events_raw[[i]]$obj_id\n  date <- events_raw[[i]]$occurred_at\n  events <- bind_rows(events, tibble(subj_id, relation, obj_id, date))\n}\n\nwrite.csv(events, filename)\n\n\n\n# A tibble: 6 x 4\n  subj_id                                                   relat~1 obj_id date \n  <chr>                                                     <chr>   <chr>  <chr>\n1 http://twitter.com/YvanDutil/statuses/1011761900953178112 discus~ https~ 2018~\n2 http://twitter.com/KBBS/statuses/1047410808093904896      discus~ https~ 2018~\n3 twitter://status?id=1097823973432328193                   discus~ https~ 2019~\n4 twitter://status?id=1118528823333670912                   discus~ https~ 2019~\n5 twitter://status?id=1118822009998278657                   discus~ https~ 2019~\n6 twitter://status?id=1153422162813808640                   discus~ https~ 2019~\n# ... with abbreviated variable name 1: relation\n\n\n\n\n8.3.2 Impactstory Profiles\nImpactstory profiles is a tool launched in 2011 by what is now the non-profit organization OurResearch. Researchers can join through their Twitter account and then create linked their Impactstory profile to their ORCID account. The profiles are public, so researchers can easily share their Impactstory profiles with others. Impactstory doesn’t have a search engine to find or browse profiles. But you can see if someone has a profile by finding them on ORCID and sticking their ORCID idea at the end of this URL: https://profiles.impactstory.org/u/\nHere’s an example ImpactStory profile: https://profiles.impactstory.org/u/0000-0003-1021-059X"
  },
  {
    "objectID": "ch8.html#conclusion",
    "href": "ch8.html#conclusion",
    "title": "8  Measuring research impact",
    "section": "8.5 Conclusion",
    "text": "8.5 Conclusion\nIn this chapter, we introduced some of the most commonly used citation-based indicators of research impact and their calculation. While a large number of empirical studies have demonstrated a correlation between citations and the not always clearly defined concept that they claim to measure, caution remains necessary when using citations for evaluation purposes. Over the years, the research community published several declarations and manifestos that emphasize the challenges of quantitative research evaluation processes and propose best practices; the Leiden Manifesto and the San Francisco Declaration on Research Assessment (DORA) are good examples.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measuring research impact</span>"
    ]
  },
  {
    "objectID": "ch8.html#data-sources",
    "href": "ch8.html#data-sources",
    "title": "8  Altmetrics",
    "section": "8.2 Data sources",
    "text": "8.2 Data sources\n\n8.2.1 Commercial data sources\n\n8.2.1.1 Altmetric\nAltmetric (https://www.altmetric.com/) is an appropriately named company that supplies altmetric data. Using a variety of sources ranging from social media to policy documents, Altmetric tracks conversations around scholarly content. Their goal is to illuminate the attention that scientific articles are receiving in real-time, which allows authors and publishers to know what people are saying about their work.\nThe Altmetric donut illustrated below is designed to convey the amount and type of attention a scholarly article has received at a quick glance.\n\n\n\n\n\nThe colors of the donut each represent a different attention source. This means that the amount of color in the donut varies based on how much attention an article has received from different sources.\n\n\n\n\n\nThe number in the middle of the donut is the Altmetric Attention Score, which is a weighted count of all the attention a scholarly article has received. It is calculated based on three factors:\n\nVolume: The more people mention an article, the higher its score will be.\nSources: Different sources of mentions contribute a different weighted amount to the Altmetric Attention Score. For example, being mentioned in a news article carries more weight than being mentioned in a tweet and thus contributes to a higher score.\nThe sharing frequency of the source: The amount that an author of a mention generally talks about scholarly articles is considered in the score. For example, a doctor who shares a link with other doctors on Twitter counts more than a journal’s Twitter account that automatically pushes out links to all their articles.\n\nA good way to collect altmetric data for a publication is to use the Altmetric Bookmarklet.\nOnce added to your bookmarks, you can click on the Altmetric it! bookmark when you are looking at an article online, and the altmetric badge will pop up.\n\nThen if you click on “click for more details”, you will be able to see a complete altmetric profile for the article.\n\n\n\n8.2.1.2 Plum Analytics\nPlum Analytics (https://plumanalytics.com/) is a competitor of Altmetric, owned by Elsevier. Because it is owned by Elsevier, the altmetrics indicators obtained from Plum Analytics (which they call PlumX Metrics) are usually available on Elsevier’s journals and databases like Scopus and Scival.\nHere is an example of the PlumX metrics displayed on an article’s record in Scopus.\n\n\n\n\n\nAgain, you can click on “View PlumX details” to obtain the full PlumX Metrics profile for the article.\n\n\n\n\n\n\n\n8.2.1.3 Overton\nWhile we will not go into details here, it is worth mentioning the relatively recent launch of a new altmetric startup, Overton (https://www.overton.io/), that focuses exclusively on tracking the policy impact of research.\n\n\n\n8.2.2 Open data sources\n\n8.2.2.1 Crossref Event Data\nCrossref Event Data is a free source of altmetric data that collect events involving digital objects (things with DOIs). The data sources and the types of events collected are described here.\nOne challenge of using Crossref Event Data is that is that there is no easy-to-use browser plugin, website, or search engine. Data can be retrieved using the API, which has a bit of a learning curve.\nTo make things simple, here is a code that you can run in R to collect the events for a single DOI.\n\nlibrary(httr)\nlibrary(stringr)\nlibrary(dplyr)\n\nemail <- \"YOUR EMAIL HERE\"\ndoi <- \"DOI HERE\"\nfilename <- \"event_data_output.csv\" # change the name of the output file as needed (as to be a .csv). \n\nevents_raw <- content(GET(str_c(\"https://api.eventdata.crossref.org/v1/events?mailto=\",\n                    email,\"&obj-id=\",doi,\"&rows=1000\", sep = \"\")))$message$event\n\nevents <- tibble()\nfor (i in 1:length(events_raw)) {\n  data<-tibble()\n  subj_id <- events_raw[[i]]$subj_id\n  relation <- events_raw[[i]]$relation_type_id\n  obj_id <- events_raw[[i]]$obj_id\n  date <- events_raw[[i]]$occurred_at\n  events <- bind_rows(events, tibble(subj_id, relation, obj_id, date))\n}\n\nwrite.csv(events, filename)\n\nThe Crossref Even Data API provides much more information, but the code above will extract the essential:\n\nsubj_id: the source of the event.\nrelation: the type of event (references, mentions, discusses).\nobj_id: the object of the event.\ndate: the date of the event.\n\nThis file is an example of the data you would obtain if you ran this code for the DOI 10.1371/journal.pone.0127502.\n\n\n\n\n\n\nNote\n\n\n\nThe metadata retrieval app that we are developing will soon allow you to automatically collect event data from a list of DOI or OpenAlex work IDs. Stay tuned!\n\n\n\n\n8.2.2.2 Impactstory Profiles\nImpactstory profiles is a tool launched in 2011 by what is now the non-profit organization OurResearch. Researchers can join through their Twitter account and then link their Impactstory profile to their ORCID account. The profiles are public, so researchers can easily share their Impactstory profiles with others. Impactstory doesn’t have a search engine to find or browse profiles. But you can see if someone has a profile by finding them on ORCID and sticking their ORCID idea at the end of this URL: https://profiles.impactstory.org/u/\nHere’s an example ImpactStory profile: https://profiles.impactstory.org/u/0000-0003-1021-059X"
  },
  {
    "objectID": "ch11.html#what-is-research-misconduct",
    "href": "ch11.html#what-is-research-misconduct",
    "title": "11  Errors, fraud, and questionable research",
    "section": "11.1 What is research misconduct?",
    "text": "11.1 What is research misconduct?\nResearch misconduct is not a new topic. Almost two centuries ago, Charles Babbage (1830) described four types of misconduct: trimming, cooking, hoaxing and forging. It was not until more than a century later, however, that the subject enjoyed a resurgence in popularity, with the writings of Merton. According to Merton (1942), fraud is particularly infrequent in science, not because researchers are more honest people than others, but because of the structure and norms of science which were discussed in Chapter 2. Then, major cases of fraud (notably that of John Darsee at Harvard, who was caught fabricating data in 52 papers that were subsequently retracted) sparked much discussion in scholarly journals and the media.\nAfter defining scientific fraud, we present in more detail the types of fraud, its causes, its consequences, the mechanisms for detecting scientific fraud and, finally, the prevalence of fraud in science."
  },
  {
    "objectID": "ch11.html#what-is-research-misconduct-1",
    "href": "ch11.html#what-is-research-misconduct-1",
    "title": "11  Errors, fraud, and questionable research",
    "section": "11.2 What is research misconduct",
    "text": "11.2 What is research misconduct\nThe definition of research misconduct generated debates within the scientific community since the beginning of the 1980s. The consensus that emerged is a frame of reference that separates research practices into three categories positioned on a continuum: responsible practices, questionable practices, and misconduct (Steneck 2006).\nOne of the main challenges that arises from this continuum is the that it may be difficult to draw a clear line between questionable practices and intentional misconduct (Judson 2004). Chop and Silva (1991) note that some argued that misconduct doesn’t have to be intentional, while for others emphasized the intentionality of misconduct, and the need to distinguish between misconduct and honest error. Of course, intentions are difficult to determine, since only the researcher knows his real intentions (Weinstein 1981a).\nThere are various means by which an ill-intentioned researcher can mislead the scientific community. Charles Babbage (1830) described four types of misconduct: forging, cooking, trimming and hoaxing. Today, these phenomena described by Babbage are grouped under two concepts: data falsification and data fabrication, and plagiarism has been added to the list of acts recognized as misconduct (Merton 1973; Judson 2004; Steneck 2006). The following definition of scientific misconduct:\n[The] fabrication, falsification, plagiarism or other practices that seriously deviate from those that are commonly accepted within the scientific community for proposing, conducting, or reporting research. It does not include honest error or honest differences in interpretation or judgments of data. (Public Health Service 1989)\nThe clause “or other practices that seriously deviate from those that are commonly accepted within the scientific community for proposing, conducting or reporting research” was subsequently heavily criticized for its vagueness and its inability to reduce discrepancies. opinions on what does or does not constitute fraud, so this clause was later dropped (Judson 2004). So today scientific fraud is generally recognized as any intentional act of data fabrication, data falsification or plagiarism (FFP).\n\n11.2.1 Fabrication\nData fabrication is probably the most serious type of scientific misconduct, and occurs when researchers makes up all or some of their data from scratch without having carried out the alleged experiments. Data fabrication can take many forms.\nA widely publicized example of fabrication is that of Diederik Stapel, a former Dutch social psychology researcher. Before being found guilty of scientific misconduct, Stapel was a star researcher in his field, having, among other things, founded the Tilburg Institute for Behavioral Economics Research in 2007, obtained the Career Trajectory Award from the Society of Experimental Social Psychology in 2009, and being appointed rector of his faculty in 2010. However, in 2011, after some of the students under his supervision expressed concern about the validity of some of his data, an investigation revealed that for more than a decade Stapel had invented the data for most of his research. More than 50 articles signed by Stapel and published in the most important journals in the field have been identified as fraudulent and subsequently retracted from the scientific literature.\nAnother well-known case, this time in physics, is that of Hendrik Schön, who fabricated data for more than four years (1997-2002) while he was a postdoctoral fellow at Bell Laboratories. He was able to fool the scientific community with his discovery of a method to manufacture plastic superconductors that would revolutionize the world of nanotechnology. Schön’s articles were published in major journals like Science and Nature. Some researchers began to question Schön’s results when several attempts to replicate Schön’s findings were unsuccessful. However, it was Schön’s use of the same fabricated figure in two different papers that ultimately uncovered the fraud (Reich 2009).\n\n\n11.2.2 Falsification\nBabbage (1830) described two types de falsification. The first one, cooking, consists in the cherry-picking of data in order to achieve a desired finding. The second, which Babbage called trimming, consists in eliminating data points that deviate from the average and artificially increase the precision and homogeneity of the data. Larivée argue that falsification is not only about cooking and trimming data, but can also include the misuse of statistical tests or of the altering of equipment or data collection processes. The main difference between falsification and fabrication is that falsification involves real data, whereas fabrication involves data artificially created.\nIsaac Newton, one of the most important figures in the history of science, apparently falsified some of the data described in his masterpiece Philosophiæ Naturalis Principia Mathematica. He improved the precision of some measurements, namely his calculations on the acceleration of gravity, the velocity of sound, and axial precession (Westfall 1973), to make his theory more convincing and and accepted by the scientific community at the time (Broad 1983).\n\n\n11.2.3 Plagiarism\nPlagiarism, which is said to be the most frequent type of scientific fraud (Merton 1957). it can be defined as\n\n“the appropriation of another person’s ideas, processes, results, or words without giving appropriate credit, including those obtained through confidential review of others’ research proposals and manuscripts” (Public Health Service 1999).\n\nThis can take the form of excessive paraphrasing or intentional, unconscious, or self-plagiarism. Intentional plagiarism is the use of someone else’s text in part or in whole without citing the source. Paraphrasing is a perfectly acceptable practice, but it becomes abusive and akin to intentional plagiarism when the words have been changed in order to give the illusion that it is a new text, and when the sources are not cited (Kochen 1987).\nThe standards differ from one discipline to another with regard to plagiarism; what is considered plagiarism in one discipline might be perfectly acceptable in another (Loui 2002). Moreover, omitting to cite the source of a fact or theory that is common knowledge is not an act of plagiarism (Loui 2002).\nAs for self-plagiarism, which consists of the reuse of text that one has written oneself, it is more difficult to determine whether or not it is a question of fraud, because the reuse of certain parts of texts may sometimes be acceptable or even preferable (eg, the description of a very technical and complex methodology) provided that the source is duly cited (Bird and Sivilotti 2008). The concept of self-plagiarism itself is debated, some arguing that it is impossible to steal one’s own ideas or text, and that the publishing the same results in different articles aimed at different communities is not only acceptable, but desirable from an knowledge dissemination standoint (Bird 2002).\nA famous case of plagiarism in medicine is that of Elias Alsabti. In addition to falsely pretending that he came from a Jordanian royal family and that he held a doctorate, he managed to plagiarize several hundred articles. He found these articles in obscure journals, replaced the author’s name with his own (and sometimes that of fictitious co-authors), then submitted this copied article to another, equally obscure, journal. Building up an impressive publication record, he obtained research positions in several universities and hospitals in the United States. Although Alsabti’s fraud was known to many in the institutions where he worked during his career, it took three years before it was made public (Broad 1983; Judson 2004)."
  },
  {
    "objectID": "ch11.html#references",
    "href": "ch11.html#references",
    "title": "11  Scientific fraud and questionable research practices",
    "section": "11.6 References",
    "text": "11.6 References\n\n\n\n\nAzoulay, Pierre, Jeffrey L Furman, Krieger, and Fiona Murray. 2015. “Retractions.” Review of Economics and Statistics 97 (5): 1118–36. https://doi.org/10.1162/REST_a_00469.\n\n\nBabbage, Charles. 1830. Reflections on the Decline of Science in England, and Some of Its Causes. London.\n\n\nBird, Stephanie J. 2002. “Self-Plagiarism and Dual and Redundant Publications: What Is the Problem? Commentary on ’Seven Ways to Plagiarize: Handling Real Allegations of Research Misconduct’.” Science and Engineering Ethics 8 (4): 543–44.\n\n\nBird, Steven B., and Marco L. A. Sivilotti. 2008. “Self-Plagiarism, Recycling Fraud, and the Intent to Mislead.” Journal of Medical Toxicology 4 (2): 69–70. https://doi.org/10.1007/BF03160957.\n\n\nBroad, William J. 1983. “Notorious Darsee Case Shakes Assumptions about Science.” New York Times.\n\n\nChubin, Daryl E. 1985. “Miconduct in Research : An Issue of Science Policy and Practice.” Minerva 23 (2): 175–202.\n\n\nCouzin, Jennifer. 2006. “Scientific Fraud.” Science 314 (5807): 1853–53. https://doi.org/10.1126/science.314.5807.1853.\n\n\nCulliton, Barbara J. 1974. “The Sloan-Kettering Affair: A Story Without a Hero.” Science 184 (4137): 644–50. https://doi.org/10.1126/science.184.4137.644.\n\n\nFanelli, Daniele. 2009. “How Many Scientists Fabricate and Falsify Research? A Systematic Review and Meta-Analysis of Survey Data.” PLOS ONE 4 (5): e5738. https://doi.org/10.1371/journal.pone.0005738.\n\n\nGrieneisen, Michael L, and Minghua Zhang. 2012. “A Comprehensive Survey of Retracted Articles from the Scholarly Literature.” PLoS ONE 7 (10): e44118–18.\n\n\nJudson, Horace Freeland. 2004. The Great Betrayal : Fraud in Science. Orlando: Harcourt.\n\n\nKarcz, Marcin, and Peter J Papadakos. 2011. “The Consequences of Fraud and Deceit in Medical Research.” Canadian Journal of Respiratory Therapy 47 (1): 18–27.\n\n\nKochen, Manfred. 1987. “How Well Do We Acknowledge Intellectual Debts?” Journal of Documentation 43 (1): 54–64.\n\n\nLarivée, Serge, and Maria G. Baruffaldi. 1993. La science au-dessus de tout soupçon : enquête sur les fraudes scientifiques. Laval, Québec: Méridien.\n\n\nMerton, Robert K. 1968. “The Matthew Effect in Science.” Science, New series, 159 (3810): 56–63. https://doi.org/10.2307/1723414.\n\n\nMongeon, Philippe. 2015. “Costly Collaborations.” JASIST 15 (2): 125–45.\n\n\nReich, Eugenie Samuel. 2009. Plastic Fantastic : How the Biggest Fraud in Physics Shook the Scientific World. New York: Palgrave Macmillan.\n\n\nSteneck, Nicholas. 2006. “Fostering Integrity in Research: Definitions, Current Knowledge, and Future Directions.” Science and Engineering Ethics 12 (1): 53–74.\n\n\nTilden, Samuel J. 2010. “Incarceration, Restitution, and Lifetime Debarment: Legal Consequences of Scientific Misconduct in the Eric Poehlman Case: Commentary on Scientific Forensics: How the Office of Research Integrity Can Assist Institutional Investigations of Research Miscond.” Science and Engineering Ethics 16 (4): 737–41. https://doi.org/10.1007/s11948-010-9228-0.\n\n\nUS Public Health Service. 1989. “Responsibilities of awardee and applicant institutions for dealing with and reporting possible misconduct in science; final rule.” Federal register 54 (151): 32446–51.\n\n\nWeinstein, Deena. 1981. “Scientific Fraud and Scientific Ethics.” Connecticut Medicine 45 (10): 655–58.\n\n\nWestfall, Richard S. 1973. “Newton and the Fudge Factor.” Science, New series, 179 (4075). https://doi.org/10.2307/1735787."
  },
  {
    "objectID": "ch11.html#what-is-research-fraud",
    "href": "ch11.html#what-is-research-fraud",
    "title": "11  Scientific fraud and questionable research practices",
    "section": "11.1 What is research fraud?",
    "text": "11.1 What is research fraud?\nThere are various means by which an ill-intentioned researcher can mislead the scientific community. Babbage (1830) described four types of fraud: forging, cooking, trimming and hoaxing. Today, these phenomena tend to be grouped under two concepts, data falsification and data fabrication, to which plagiarism has been added as a third type of fraud. Following the rise of cases of research fraud in the 1980s, the US Public Health Service proposed the following definition of scientific fraud:\n\n[The] fabrication, falsification, plagiarism or other practices that seriously deviate from those that are commonly accepted within the scientific community for proposing, conducting, or reporting research. It does not include honest error or honest differences in interpretation or judgments of data (US Public Health Service 1989).\n\nThe clause “or other practices that seriously deviate from those that are commonly accepted within the scientific community for proposing, conducting or reporting research” was heavily criticized for its vagueness and its inability to reduce discrepancies between opinions on what does or does not constitute fraud, so this clause was later dropped (Judson 2004). Today scientific fraud is generally recognized as any intentional act of data fabrication, data falsification or plagiarism (FFP), and other practices that seriously deviate from those that are commonly accepted within the scientific community for proposing, conducting, or reporting research tend to now be labelled as questionable research practices (QRP).\n\n11.1.1 Fabrication\nData fabrication is probably the most serious type of scientific fraud and occurs when researchers makes up all or some of their data from scratch without having carried out the alleged experiments. Data fabrication can take many forms.\nA widely publicized example of fabrication is that of Diederik Stapel, a former Dutch social psychology researcher. Before being found guilty of scientific fraud, Stapel was a star researcher in his field, having, among other things, founded the Tilburg Institute for Behavioral Economics Research in 2007, obtained the Career Trajectory Award from the Society of Experimental Social Psychology in 2009, and being appointed rector of his faculty in 2010. However, in 2011, after some of the students under his supervision expressed concern about the validity of some of his data, an investigation revealed that for more than a decade Stapel had invented the data for most of his research. More than 50 articles signed by Stapel and published in the most important journals in the field have been identified as fraudulent and subsequently retracted from the scientific literature.\nAnother well-known case, this time in physics, is that of Hendrik Schön, who fabricated data for more than four years (1997-2002) while he was a postdoctoral fellow at Bell Laboratories. He was able to fool the scientific community with his discovery of a method to manufacture plastic superconductors that would revolutionize the world of nanotechnology. Schön’s articles were published in major journals like Science and Nature. Some researchers began to question Schön’s results when several attempts to replicate Schön’s findings were unsuccessful. However, it was Schön’s use of the same fabricated figure in two different papers that ultimately uncovered the fraud (Reich 2009).\n\n\n11.1.2 Falsification\nBabbage (1830) described two types of falsification. The first one, cooking, consists in the cherry-picking of data in order to achieve a desired finding. The second, which Babbage called trimming, consists in eliminating data points that deviate from the average and artificially increase the precision and homogeneity of the data. Larivée and Baruffaldi (1993) argue that falsification is not only about cooking and trimming data but can also include the misuse of statistical tests or the altering of equipment or data collection processes. The main difference between falsification and fabrication is that falsification involves real data, whereas fabrication involves data artificially created.\nIsaac Newton, one of the most important figures in the history of science, apparently falsified some of the data described in his masterpiece Philosophiæ Naturalis Principia Mathematica. He improved the precision of some measurements, namely his calculations on the acceleration of gravity, the velocity of sound, and axial precession (Westfall 1973), to make his theory more convincing and accepted by the scientific community at the time Broad (1983).\n\n\n11.1.3 Plagiarism\nPlagiarism, which is said to be the most frequent type of scientific fraud (Merton 1957), can be defined as:\n\n[T]he appropriation of another person’s ideas, processes, results, or words without giving appropriate credit, including those obtained through confidential review of others’ research proposals and manuscripts (US Public Health Service 1989).\n\nThis can take the form of excessive paraphrasing or intentional, unconscious, or self-plagiarism. Intentional plagiarism is the use of someone else’s text in part or in whole without citing the source. Paraphrasing is a perfectly acceptable practice, but it becomes abusive and akin to intentional plagiarism when the words have been changed in order to give the illusion that it is a new text, and when the sources are not cited (Kochen 1987). Self-plagiarism consists of the reuse of one’s own published text. It can be challenging to determine whether or not an instance of self-plagiarism is fraudulent, because the reuse of certain parts of texts may sometimes be acceptable or even preferable (e.g., the description of a very technical and complex methodology) provided that the source is duly cited (S. B. Bird and Sivilotti 2008). The concept of self-plagiarism itself is debated, some arguing that it is impossible to steal one’s own ideas or text, and that the publishing the same results in different articles aimed at different communities is not only acceptable, but desirable from an knowledge dissemination standpoint (S. J. Bird 2002).\nA famous case of plagiarism in medicine is that of Elias Alsabti. In addition to falsely pretending that he came from a Jordanian royal family and that he held a doctorate, he managed to plagiarize several hundred articles. He found these articles in obscure journals, replaced the author’s name with his own (and sometimes that of fictitious co-authors), then submitted this copied article to another, equally obscure, journal. Building up an impressive publication record, he obtained research positions in several universities and hospitals in the United States. Although Alsabti’s fraud was known to many in the institutions where he worked during his career, it took three years before it was made public (Broad 1983; Judson 2004)."
  },
  {
    "objectID": "ch11.html#questionable-research-practices",
    "href": "ch11.html#questionable-research-practices",
    "title": "11  Scientific fraud and questionable research practices",
    "section": "11.5 Questionable research practices",
    "text": "11.5 Questionable research practices\nQuestionable research practices (QRPs) exists in the grey zone between the responsible conduct of research (RCR) and fabrication, falsification, and plagiarism (FFP).\n\n\n\nResearch behaviours continuum (Steneck 2006).\n\n\nQRPs encompass a whole range of behaviours and practices that drift away from the ideal of the responsible conduct of research. They can occur at different stages of the research process and they are sometimes considered to not only apply to the individuals performing the research but also to other actors such as editors and reviewers. They also vary in their perceived prevalence and severity and these perceptions also vary between fields. Here are examples of QRPs:\n\nImproper referencing: Attributing an idea or concept to the wrong source, failing to cite sources, or abusive self-citations. This include citing only studies that are in agreement with one’s findings.\nSelective reporting: Reporting only some of the data and results of a study (typically only the positive results). Failing to provide adequate level of details in the different parts of the manuscript. Publishing only positive results (publication bias).\nSalami slicing: Splitting what could and should have been a single work into multiple smaller publications, sometimes called “smallest publishable units”.\nNot sharing data: While sharing data is not always possible for ethical or legal reason. Sharing data is increasingly expect by journals and other researchers and not doing so is often considered a QRP.\nP-hacking: Performing all kinds of test and retaining those that produce significant results.\nHarking: Coming up with the hypothesis after looking at the results and presenting it as an a priori hypothesis.\nAuthorship misattribution: failing to give credit where credit is due by excluding significant contributors from the byline, or giving credit where it is not due by including co-authors that did not significantly contribute to the work. Not ordering authors in a way that reflects their respective contribution to the work."
  },
  {
    "objectID": "ch11.html#consequences-of-research-fraud",
    "href": "ch11.html#consequences-of-research-fraud",
    "title": "11  Scientific fraud and questionable research practices",
    "section": "11.2 Consequences of research fraud",
    "text": "11.2 Consequences of research fraud\nFor the guilty researchers, fraud has informal consequences in the form of a loss of reputation (Larivée and Baruffaldi 1993) and the questioning of all their work by the scientific community (Culliton 1974). There are also formal consequences that vary from case to case. These can include a ban on obtaining funding for a certain number of years, suspension, dismissal, house arrest, community service, fine and imprisonment (Couzin 2006; Karcz and Papadakos 2011; Tilden 2010). Mongeon (2015) found that most researchers found guilty of fraud in the biomedical field did not publish in the 5 years following the retraction of their fraudulent papers. That same study found that fraud can also significantly affect the careers of innocent co-authors, especially junior researchers who, in most cases, stopped publishing and appear to have abandoned their research careers (see the figure below).\n\n\n\nShare of research output by innocent collaborators published in the five years before and after a retracted publication in the biomedical field. (Mongeon 2015)\n\n\nIt is perhaps worth noting that, according to these findings, retracting a publication because of an honest error also appears to significantly affect the careers of junior researchers (who are typically listed as first authors in biomedical research).\nBecause researchers build on existing research to advance knowledge, the publication of false results can lead to considerable waste of time, money and effort for a large number of researchers, not to mention the waste resulting from the fraudulent research itself. Fraud can reduce mutual trust between researchers, potentially hindering the advancement of knowledge by reducing the sharing and use of information, or pushing researchers to systematically reproduce the experiments (Chubin 1985; Weinstein 1981).\nCases of fraud can also negatively affect the discipline in which they occur. According to Azoulay et al. (2015), there would be a decrease in both the research funds granted to researchers in a discipline where a fraud has been discovered, new researchers entering it, and citations received by articles dealing with subjects similar to those of fraudulent articles.\nFunding for research comes largely from public funds, and it important that science be seen as trustworthy by governments, funding agencies and the public. Research fraud, and especially the cases that generate a lot of media coverage, can harm the credibility and legitimacy of science."
  },
  {
    "objectID": "ch11.html#detecting-research-fraud",
    "href": "ch11.html#detecting-research-fraud",
    "title": "11  Scientific fraud and questionable research practices",
    "section": "11.3 Detecting research fraud?",
    "text": "11.3 Detecting research fraud?\nGiven the significant consequences that research fraud can have, it is important for the scientific community be able to detect it. It is sometimes taken for granted that science is self-correcting and has mechanisms that will ensure that sooner or later errors (and frauds) will be discovered. However, the surge of research fraud observed over the last decades raises questions as to the effectiveness of these mechanisms.\nOne of these mechanisms is the evaluation of research manuscripts by several experts before they get published, also known as peer-review. In theory, peer-review ensures that only relevant, reliable, and rigorous research will make it into the scientific literature. However, peer review is often not effective in detecting errors and fraud. Researchers who are part of the elite will not be judged as harshly as other less prominent researchers (Merton 1968). A researcher who is prominent or affiliated to a prestigious organization may manage to publish fabricated or falsified data since the reviewers may not dare question their work. The case of John Long is a good example. From 1970 until the early 1980s, he published the results of his work on tumor cells from Hodgkin’s disease that he supposedly managed to grow in a test tube. Since he worked at the prestigious Massachusetts General Hospital under the supervision of Paul Zamecnik, a well-known researcher and member of the National Academy of Sciences, the peer review system took certain aspects of his work for granted (Broad 1983).\nThe peer review system is also quite ineffective in detecting plagiarism, and even more so in the pre-digital era. This allowed fraudsters like Alsabti to take advantage of this flaw. While it is now much easier to detect cases of plagiarism by using, for instance, plagiarism detection software, plagiarism remains relatively frequent (Grieneisen and Zhang 2012).\nReplication of experiments by other researchers is the most effective mechanism for validating findings and detecting errors and fraud. The replication of (or the possibility to replicate) findings is made possible by the fact that research publications include a detailed description of the research data and process (a.k.a., the methods). In practice, the reproduction of experiments is relatively rare. One of the reasons for this is inaccurate or incomplete description of the methods, or inaccessibility of the required data or equipment. Even when replication would in theory be possible, there is a lack of resources and incentives for replication studies which are difficult to get funded and published due to the emphasis on originality in the reward system of science. Rather, the research will be validated indirectly by other researchers who replicate or build on results to improve them (not to verify them).\nWhistle-blowing is often involved in the process of uncovering research fraud. For instance, Diederik Stapel’s case was uncovered by some of his students who grew suspicious when they asked to see the raw data for an experiment they were involved in and Stapel said he didn’t have it anymore. One of the whistleblowers thought Stapel’s results were too good to be true, and their suspicions were validated when they found two rows of data that appeared to have been copy-pasted."
  },
  {
    "objectID": "ch11.html#prevalence-of-research-fraud",
    "href": "ch11.html#prevalence-of-research-fraud",
    "title": "11  Scientific fraud and questionable research practices",
    "section": "11.4 Prevalence of research fraud",
    "text": "11.4 Prevalence of research fraud\nJust how much fabricated, falsified, or plagiarized research is there? Given that fraud is by nature something that its author seeks to hide and hope that it will never be discovered, and given that research fraud is not easy to detect, it is plausible that the cases that are discovered are really just the tip of the iceberg. Some attempted to estimate the prevalence of fraud using surveys. This approach is expected to underestimate the prevalence of research fraud since researchers are probably not very inclined to admit their fraud. Nevertheless, in a meta-analysis of these surveys Fanelli (2009), reported that 2% of researchers admit to having distorted data themselves, while 14% claim to know at least one colleague who has done so.\nRetractions are one of the ways by which the scholarly community treats fraud when it is discovered. Retractions, in principle, clean up the scientific record by flagging invalid or untrustworthy research. The census of retracted articles thus makes it possible to estimate the prevalence of cases of fraud, but only on condition that the cases in question meet four conditions:\n\nThe fraudulent research has been published.\nThe fraud was detected after publication.\nThe fraudulent article was retracted by the journal.\nA notice of retraction has been circulated and is easily identifiable, and ideally the article itself has been identified as retracted in the databases.\n\nThus, retractions also only provide an underestimate of the prevalence of scientific fraud since obviously not all cases of scientific fraud or error will meet these four conditions.\nAccording to the Retraction Watch blog and its searchable database, more than 40,000 articles have been retracted as of November 13th, 2022. However, not all of these retractions are due to fraud. There are many reasons why articles get retracted, including honest errors and a range of issues that undermine the trustworthiness of the studies. Also, it is important to keep in mind that there are millions of studies published every year, so despite the increased number of retractions in recent decades, retractions remain extremely rare. Less rare are questionable research practices, which we discuss in the next section."
  },
  {
    "objectID": "ch10.html#what-is-open-access",
    "href": "ch10.html#what-is-open-access",
    "title": "10  Open research",
    "section": "10.2 What is Open Access?",
    "text": "10.2 What is Open Access?\nAccording to the Budapest Open Access Initiative (BOAI) declaration, Open Access (OA) can be defined as follows:\n\nThe “free availability [of research outputs] on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself”.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open research</span>"
    ]
  },
  {
    "objectID": "ch10.html#types-of-oa",
    "href": "ch10.html#types-of-oa",
    "title": "10  Open research",
    "section": "10.3 Types of OA",
    "text": "10.3 Types of OA\nOpen Access comes in many shapes and forms. In general, we differentiate between five types of OA:\n\nGold: Articles are free to read on the journal website, under a creative commons or similar license. Article Processing Charges (APCs) are paid by the author or another organization (library, university, government) through some agreement. This is a free-to-read, pay-to-publish type of OA.\nHybrid: Articles published in a non-OA journal, but for which APCs have been paid to make the article OA. This is a widely criticized form of OA since access to the article has already been paid for by the journal subscribers. This means that the publisher gets paid twice for the same article (double dipping). The issue is exacerbated by the high APCs (typically multiple thousands of US dollars) that publishers charge for Hybrid OA.\nDiamond/Platinum: Articles are free to read on the journal website, under a creative commons or similar license, but no APCs are paid by authors. This is a free-to-read, free-to-publish type of OA.\nBronze: Articles that are free to read online but without a creative commons or similar licence.\n\nAll that the above types of OA have in common is that they make the published version of the articles free to read directly on the journal’s website. This is the main distinctive characteristic of the last type of OA.\n\nGreen – refers to self-archiving generally of the pre- or post-print of articles in repositories. A pre-print is the version of the article before it has been reviewed, and a post-print is the version that was accepted by the journal for publication, but without the journal’s formatting. The version that is formatted and appears on the publisher’s website is called the publisher’s version.\n\n\n\n\n\n\n\nSuggested readings\n\n\n\nThe following preprint (green OA!) provides insights on global costs of OA by estimating the amount of APCs paid to large commercial publishers.\nButler, Leigh-Ann, Matthias, Lisa, Simard, Marc-André, Mongeon, Philippe, & Haustein, Stefanie. (2022). The Oligopoly’s Shift to Open Access. How For-Profit Publishers Benefit from Article Processing Charges (Version v1). Zenodo. https://doi.org/10.5281/zenodo.7057144",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open research</span>"
    ]
  },
  {
    "objectID": "ch10.html#predatory-publishing",
    "href": "ch10.html#predatory-publishing",
    "title": "10  Open research",
    "section": "10.4 Predatory publishing",
    "text": "10.4 Predatory publishing\nThe pay-to-publish (gold) OA as a business model has led to the proliferation of predatory publishers and journals, that have been defined as “entities that prioritize self-interest at the expense of scholarship and are characterized by false or misleading information, deviation from best editorial and publication practices, a lack of transparency, and/or the use of aggressive and indiscriminate solicitation practices.” (Grudniewicz et al. 2019). While the definition may seem clear enough, dividing publishers in two categories (predatory or not predatory) can be challenging and it may be better to think of publishers and journals as exhibiting degrees of predatoriness (Siler 2020).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open research</span>"
    ]
  },
  {
    "objectID": "ch10.html#data-sources-for-oa",
    "href": "ch10.html#data-sources-for-oa",
    "title": "10  Open research",
    "section": "10.5 Data sources for OA",
    "text": "10.5 Data sources for OA\n\n10.5.1 Unpaywall\nUnpaywall is a database of OA publications which you can access in multiple ways to obtain the OA status as well as every OA location of documents that have a DOI:\n\nThe Unpaywall REST API\nThe roadoi R package that makes it easier to query the REST API.\nThe Simple Query Tool that allows you to simply past a list of DOI and get the data by email.\nIf you find the 100,000 DOIs limit for API calls, and the 1,000 DOIs limit for the Simple Query Tool, you can also download the entire Unpaywall database snapshot.\n\nUnpaywall also offers an extension for your web browser that automatically shows you when an article you are viewing on the Web has an available OA version, which you can access in a click.\nUnpaywall data is also integrated in OpenAlex. used to provide the OA status of articles in OpenAlex, which provides the following metadata:\n\nis_oa: TRUE or FALSE\noa_status: One of the following OA statuses:\n\nGold: Published in an OA journal that is indexed by the DOAJ.\nGreen: Toll-access on the publisher landing page, but there is a free copy in an OA repository.\nHybrid: Free under an open license in a toll-access journal.\nBronze: Free to read on the publisher landing page, but without any identifiable license\nClosed All other articles.\n\noa_url: The best Open Access (OA) URL for the work.\n\n\n\n10.5.2 Directory of Open Access Journals (DOAJ)\nThe Directory of Open Access journals (DOAJ) is a database of OA journals that meet certain inclusion criteria as well as the papers published in those journals. Data from the DOAJ can be accessed through the website’s search engine, by using the API, or by downloading the public data dump.\n\n\n10.5.3 Sherpa Romeo\nSherpa Romeo is another useful resource developed by the Jisc, a non-profit organization in the UK formally called the Joint Information Systems Committee (JISC). It is a directory of journals’ and publishers’ OA policies. It provides useful information on the OA pathways that are permitted by the journals, and the associated fees, embargo period, the version of the manuscript to which it applies, and so on. Again, the data can be accessed through the website’s search engine, or with the Sherpa Romeo API.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open research</span>"
    ]
  },
  {
    "objectID": "ch10.html#references",
    "href": "ch10.html#references",
    "title": "10  Open research",
    "section": "10.6 References",
    "text": "10.6 References\n\n\n\n\nGrudniewicz, Agnes, David Moher, Kelly D. Cobey, Gregory L. Bryson, Samantha Cukier, Kristiann Allen, Clare Ardern, et al. 2019. “Predatory Journals: No Definition, No Defence.” Nature 576 (7786): 210–12. https://doi.org/10.1038/d41586-019-03759-y.\n\n\nSiler, Kyle. 2020. “There Is No Black and White Definition of Predatory Publishing.” https://blogs.lse.ac.uk/impactofsocialsciences/2020/05/13/there-is-no-black-and-white-definition-of-predatory-publishing/.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open research</span>"
    ]
  },
  {
    "objectID": "ch12.html#the-social-stratification-of-science-revisited",
    "href": "ch12.html#the-social-stratification-of-science-revisited",
    "title": "12  Equity, diversity, and inclusion in research",
    "section": "12.2 The social stratification of science (revisited)",
    "text": "12.2 The social stratification of science (revisited)\nMerton wrote of citations as the scientific community’s recognition and signification of knowledge and its source that has been accepted or rejected by the community (as a citation is agnostic whether you cited it to confirm or reject their results). In his psychosociological analysis of science as a social institution (this is a sociological perspective), Merton (1968) finds correlations in the self-assurance in highly acknowledged researchers that is concurrently inherent, yet also socially constructed and supported, so that pursuit of high-risk problems yields disproportionate recognition when communicated. This process of social selection leads to a “concentration of science resources and talent” (Merton 1968). Merton’s original 1968 article pointed out two aspects of the Matthew effect, one in which a person is given over-recognition for contributions and the other as misattribution of work for which they did not do (Merton 1968). However, the term is used most commonly for the first condition to describe the accumulation effect of ‘the rich get richer’ in which those at the center, who have already been recognized, are attributed more than those at the margins. To put this another way, the prestige of an author can affect citation behaviours and subsequent patterns.\nBourdieu recognized this accumulation in the form of citation amounts as social capital (Bourdieu 1975), which draws those without towards those that have it. Similarly, those that have it are often awarded funding and resources which may support those around them, not only gaining more social capital for themselves in the process but also benefiting those contributing to it. This dynamic process results in communities with centralized figures that are highly recognized and rewarded with other agents surrounding them, not only trying their best to be a central figure but also preventing their movement to the margins. The accumulation, then, is a dynamic driving force in a society, regardless of whether the accumulation is in citations, resources, attention, or otherwise that contributes to their social capital.\nCitations differ from other forms of capital, however, in that while aggregate citation rates may follow a normally distributed curve over the course of an author’s career, the number of citations only continues to grow as a symbolic reward (Cole and Cole 1973). Citations can be thought of as a currency that can only accumulate, and the gaps between those who have a lot of it and those who do not keep getting larger (Kwon 2022).\nPerhaps less central in the scholarship of Merton and Bourdieu is the recognition of the behaviours which result in the oppression, marginalization, or erasure to maintain control of the centre. Margaret Rossiter names the “Matilda effect” (Rossiter 1993) after the suffragist and feminist Matilda Joslyn Gage, who criticized the phenomena in which “women scientists …have been ignored, denied credit, or otherwise dropped from sight” (Rossiter 1993). Rossiter seizes upon Merton’s original definition of the Matthew effect to explore the phenomena of misattribution through purposeful or innocent ignorance of the contributions of women. She also points out the beautiful irony that the book of Matthew was not written until “two or three generations after his death”, further reinforcing her point of misattribution. Rossiter’s key contribution is the criticism of Merton’s functionalist approach to the effect that there was no emancipatory call as a result of discovering this but rather a map for how new scientists could capitalize on this effect. The Matilda effect represents not just disproportionate attributions benefiting those who already accumulated recognition but the purposeful obfuscation or erasure of women in science whose accomplishments were attributed to men.\nThis obfuscation and erasure of information are explained as epistemic injustice by Fricker (2007) as both testimonial and hermeneutical injustices in the information world. Testimonial injustice is a form of social power in which identity is used as an oppressive instrument of power. It occurs when one receives or is denied credibility by another, such as when a listener dismisses what a speaker is saying. Hermeneutical injustice is another form of social power and occurs when the speaker is denied “interpretive resources” to understand the unfair disadvantages that they are experiencing. For example, when someone is denied education on domestic violence, they may lack the ability to interpret their situation as being abused.\nBoth testimonial and hermeneutical injustices exist within LIS (Patin et al. 2020), and without a critical examination of our own contributions, we will continue to unknowingly support systems that contribute to the eradication or devaluation of information. As librarians, we have been part of a history of “privileging certain knowledge systems and ways of knowing over others” (Patin et al. 2020). While academic institutions work with a very small amount of the knowledge that humanity produces, it is a critical part of the scientific contribution to society. As such, moving beyond individual awareness to collective action against epistemic injustices in our information systems is important for LIS to address all injustices, even the accidental omissions that have occurred historically. Within the context of scholarly communication, citations are one of the main ways we can see evidence of biased behaviours and injustices from the scientific community.\nIn contrast to the observed phenomena from Merton and Bourdieu, Sara Ahmed reframes, from a feminist lens, citations as reproductive technology or “a way of reproducing the world around certain bodies” (Ahmed 2013). “Who appears, who does not appear” (Ahmed 2013) is both a conscious and unconscious choice supported by assumptions and beliefs. Are we inserting ourselves into a viewpoint/perspective, or are we establishing our own perspective or that of our community? With Ahmed’s perspective in mind, perhaps there is an emancipatory approach to citations."
  },
  {
    "objectID": "ch12.html#citational-justice",
    "href": "ch12.html#citational-justice",
    "title": "12  Equity, diversity, and inclusion in research",
    "section": "12.3 Citational justice",
    "text": "12.3 Citational justice\nWhat is wrong with how we are citing and how are inequalities manifest? In the investigation into their own citation behaviours, Kumar & Karusala (2021) introduce Iris Marion Young’s faces of oppression as a framework for understanding citational justice. They define justice as “a relational value of the actions, structures, and institutions in which persons stand to each other as social and political subjects, be they structures of the production and distribution of material goods or of the exercise of political power” (Kumar & Karusala, 2021 citing Rainer Forst 2007). Like Ahmed, they view the citation as “anti-racist, feminist technologies” (Kumar & Karusala, 2021) with the potential to correct the imbalances have occurred. The authors present some examples of ways in which injustices have shown up in their own work and reviews, which may provide an opportunity for self-reflection upon your own citation practices.\n\nExploitation – occurs when the balance of work and compensation is leveraged creating inequality and power dynamics. This supports the rich-get-richer aspect of Merton’s Matthew effect, by leveraging power. The authors identify several types of citation behaviours found in their own work.\n\nThe Cite-Me Cite can occur when submitting papers to journals and the editors pressure the authors to cite their work in return for an acceptance. This is particularly a concern/signal of predatory journal practices.\nThe Name-Agnostic Cite occurs when hard to recognize/pronounce/read names are othered, as in ” other authors have investigated…“, whereas Western names are clearly cited.\nThe In-the-Global-South and Unrelated-to-the-North sites fall along similar lines as othering or even making certain work irrelevant. See Linxen et al. (Linxen et al., 2021), for a study exploring this issue.\nThe Throwaway Cite occurs when citations are lumped together without individual attention or recognition, as in “studies in LIS have examined the the effect of unicorns (Name, 1986; Name, 1993; Name, 2000; Name et al., 2002; Name et al., 2013).” While this practice may be an intent to be exhaustive yet concise, who is this benefiting and for what purpose?\nLastly, the No Cite, in which references are not made as conscious decisions to omit, or unconscious decisions, the lack of attribution is still an error. While addressing this type of non-cite requires greater rigour, not doing so is a privilege that is being assumed. As Kumar & Karusala point out, the ‘failure to engage might suggest that the costs of our practices on the production of knowledge are not entirely visible to us’ (Kumar & Karusala, 2021).\n\nMarginalization – when a category of persons is excluded and thereby deprived, not only at the individual level but also at the collective level. This is evident in conferences that privilege some populations, such as the CHI conference that has never been held in the Global South. Some universities dominate some disciplines, which can lead to a misperception of enhanced value affecting acceptance and possibly citation. Women, scholars of color, gender diversity, also exhibit the effect of biases upon their communities as evidenced by citation gaps.\nPowerlessness – Those in the community that “lack significant power”, a voice, or opportunity to contribute to decision making. This occurs when assumptions are made, creating, or reinforcing norms that we expect to be accepted. These assumptions, without critical inquiry can shape not only our reader, but ourselves. This can include the assumptions that work from certain groups lacks rigour, works published at certain venues or in certain journals is not of high quality, Wikipedia is not a valid source of knowledge, papers written in other languages are not relevant, etc.\nCultural imperialism – an interpretation of normal within a society that reflects the dominant society’s cultural values, at once othering other groups within society and reinforcing stereotypes that maintains this power imbalance. In scholarly publications, this is evidenced by a combination of two previous effects, Othering then Uncited. As example, work in the Global South focuses on the poorest communities, focus on novelty or difference within other cultures, the universality of Western ethical standards, the expectation that English by those outside the Western North is of low quality. Cultural imperialism also occurs when the research of marginalized communities is interpreted within the frameworks of the dominant society for their own use.\nViolence – here it is important to bring in Young’s words from Kumar & Karusala:\n\n“While the frequency of physical attack on members of these and other racially or sexually marked groups is very disturbing, I also include in this category less severe incidents of harassment, intimidation, or ridicule simply for the purpose of degrading, humiliating, or stigmatizing group members.”\n\n\nThe authors continue to say that its less so the violent act, than it is the social conditions which continue to permit it to happen. They illustrate this face of oppression within scholarly publication with evidence from reviews in which questions are called about the relevance of a health topic if it only affects a small population, the criticism of research aims of Black scholars as outside of typical scholarship, how disabled persons are singled out for research, or the long-term bias against low citation counts and the assumptions of quality or relevancy.\nThese 5 faces of oppression attributed to Young from Kuman & Karusala’s article provide a framework for understanding and deconstructing biases that exist in our choices and assumptions affecting citational justice in scholarly communication. This is not the only framework or examples that can be found. Unethical citation practices have been around for quite some time, so there are more resources for understanding how these exist.\n\n12.3.1 How are citation styles contributing?\nLimitations of citation styles is recognized, by (MacLeod, 2021) as inadequate for appropriate attributions for indigenous ways of knowing as well as proposal for new templates. The APA style we often work in just has the first author and the year, though there may be many people that contributed equally to the paper, or conversely, don’t reflect the acknowledged contributions of each author.\n\n\n12.3.2 Chicken and Egg\nCitational metrics used for evaluation of career performance at the individual level needs to change or be reduced in its importance. Important for funding applications, peer review process, consideration for awards - But its not just citational justice, but the wider “engagement, recognition and valorization” (Kwon, 2022 citing smith) of ideas from diverse sources.\nMott and Cockayne (Mott & Cockayne, 2017) also recognize citations as “a problematic technology”, but citations as a currently or neoliberalization of impact or quality, they also call for citations as feminist and anti-racist “technology of resistance” as a tool to right the imbalance. However, this rebalancing seems to require the continued value of the citation as a symbolic reward system with real effects on researchers careers and evaluation.\nMoving beyond citing practices, the following section looks at how biases have been documented across scholarly communication."
  },
  {
    "objectID": "ch12.html#definitions-of-diversity-equity-inclusion-and-anti-racism",
    "href": "ch12.html#definitions-of-diversity-equity-inclusion-and-anti-racism",
    "title": "12  Equity, diversity, and inclusion in research",
    "section": "12.4 Definitions of diversity, equity, inclusion, and anti-racism",
    "text": "12.4 Definitions of diversity, equity, inclusion, and anti-racism\nFrom the Latonia Harris provided these definitions for a 2021 workshop on diversity, equity, inclusion, and anti-racism in STEMM organizations, (National Academies of Sciences, Engineering, and Medicine, 2021)\n\nDiversity: the numerical representation of groups of individuals based on their primary and secondary characteristics and identities.\nEquity: the treatment of individuals in terms of access, opportunity, and advancement.\nInclusion: the ability to meaningfully participate and contribute, both for the benefit of the individual and the organization.\nRacism: the devaluation and the denial of rights, dignity, and value of individuals due to their race or geographical origin.\n\nFrom a sociological perspective, which acknowledges and seeks to understand the cultural and structural context, biases can manifest in implicit or explicit ways that have long-term ramifications on perceptions of “confidence, capability, trustworthiness,” among others as idendified by Susan Fiske of Princeton University. (National Academies of Sciences, Engineering, and Medicine, 2021).\nOn an individual level biases are learned behaviours and associations that happen quickly, and over time, unconsciously. Biases are also complex with two levels or layers, with the first based upon observable qualities or traits, and the other associations or connections with behaviours that are then compared with those of the observer. Contextual conditions such as beliefs, can add validation and reinforce the biased associations and permit assumptions. The goal of diversity science is to bring awareness of those biases, to challenge the assumptions, so that acknowledgement of the “disparities in resources and opportunities across groups” can be addressed (National Academies of Sciences, Engineering, and Medicine, 2021)."
  },
  {
    "objectID": "ch12.html#measuring-biasdisparities-in-research",
    "href": "ch12.html#measuring-biasdisparities-in-research",
    "title": "12  Equity, diversity, and inclusion in research",
    "section": "12.3 Measuring bias/disparities in research",
    "text": "12.3 Measuring bias/disparities in research\nFrom a sociological perspective, which acknowledges and seeks to understand the cultural and structural context, biases can manifest in implicit or explicit ways that have long-term ramifications on perceptions of “confidence, capability, trustworthiness” among others (Scherer 2021). On an individual level, biases are learned behaviours and associations that happen quickly, and over time, unconsciously. Biases are also complex with two levels or layers, with the first based upon observable qualities or traits and the other associations or connections with behaviours that are then compared with those of the observer. Contextual conditions, such as beliefs, can add validation and reinforce biased associations and permit assumptions. The goal of diversity science is to bring awareness of those biases to challenge the assumptions so that acknowledgement of the “disparities in resources and opportunities across groups” can be addressed (Scherer 2021).\nStudies examining gender bias in scholarly communication utilize algorithms to categorize names within gender categories (typically binary) based on geographic and cultural inferences. NamSor, genderize.io, GenderAPI, and Wiki-Gendersort, are the main ones found in bibliometric studies investigating gender bias. The algorithms work by harvesting names from openly available databases and also collect other data such as the country of origin and the family name and language as cultural context identifiers. All names are assigned a gender with a certain probability calculated by the algorithms. These algorithms have some benefits: they are cheap, effective, and can be applied retrospectively to datasets. But they also have limitations, such as the fact that they rely on name-gender databases that may not include self-identification. Moreover, gender probabilities based on names and locations are obviously not perfect and may fail to attribute the right gender in some cases. That said, their accuracy remains acceptable at the aggregate level. While this still presents data along binary categories of gender (not to mention the common conflation of sex and gender as identity), the algorithms are often used to address and dismantle the historical and current oppression rather so that rejecting their use would deprive us from valuable knowledge around gender biases and disparities that exist at a large-scale. Here are several kinds of gender biases or disparities that have been observed in bibliometric studies.\n\nA citation disparity is observed by simply comparing citations indicator between groups (Traag and Waltman 2022). Studies have shown that works by women tend to get less cited than work by men (Larivière et al. 2013) and that women represent only 14% of the highly cited researchers (the group of researchers who publish highly cited publications) in the Web of Science (Meho 2022).\nA citation bias is observed when there is a causal relationship between a variable (e.g., gender) and the act of citing a paper (Traag and Waltman 2022). Causation is however difficult to demonstrate in bibliometrics because experiments are extremely rare in the field, and most studies are correlational. Because correlation does not imply causation, it is very difficult to demonstrate a bias (defined as a causal relationship) using bibliometric methods.\nCitation homophily is observed when members of one group tend to cite members of the same group more than researchers from other groups. Ghiasi et al. (2018) found that citation homophily occurs in all fields of science but that it is stronger in the Social Sciences and Humanities.\n\nWhile the examples above refer to citation disparities, biases, and homophily, the same situations or mechanisms can be observed for other indicators such as research outputs, collaboration, funding, awards, hiring, promotions, etc. Understanding how biases and discriminatory practices exist in academia is important for closing the gender gap. Furthermore, disparities, biases, and homophily can be observed for other variables than gender:\n\nBiases based on ethnicity or race impose disadvantages on persons based on their perceived identity. Ethnical biases can include race, ethnicity, and nationality. Secondary associations with race, ethnicity, nationality, and their intersections can further create or maintain harmful stereotypes when authors’ works are perceived as less than those of another group.\nBiases in the perceived value of works from certain countries or regions. Examples seen previously include ignoring or devaluing works that are from other countries or regions, the assumption that issues in X country are not applicable to one’s own situation, or assumptions of research quality or rigour if the author has institutional affiliations outside of the perceived ‘norm’. The Global North produces far more publications and receives more citations than the Global South, which also produces more local and geographically contextualized work than other geographic regions (Mongeon et al. 2022). Reinforcing this bias of geographic citations is the evaluation of works for quality, with Global North/Western authors possessing the privilege of not citing authors from other regions with any deleterious effect on their perceived quality, whereas non-Global North/Western authors must cite references from the Global North as evidence of their research quality Chakrabarty (2007). Other studies grouped countries by income level and found that research in low to middle-income countries tends to be evaluated less favourably than those in high-income countries (Harris et al. 2017).\nA devaluation or dismissal of work written in languages other than English exists in citation and pee-review (Lee et al. 2013). There are differences in acceptance rates of manuscripts from authors of English-speaking countries and those of non-English-speaking countries, and sometimes language and writing style is given as reasons for rejection when there is no other problem with the manuscript. Databases of Scopus and Web of Science have a disproportionate coverage of English articles, affecting fields such as social sciences and humanities, where there are more books in languages other than English due to their subject matter and regional specificity (Mongeon and Paul-Hus 2015). Compounding this is that US and English-speaking countries dominate web development, particularly academic web development, contributing to even more bias against non-English sources. As such, all indicators, including those that are web-based, are inherently biased toward English documents from database sources, social media outlets, or search tools (Mas-Bleda and Thelwall 2016).\nIn their study of peer-review biases, Lee et al. (2013) point to other forms of bias, including affiliation bias (evaluating more favourably work from prestigious institutions), content bias (favouring specific topics or methodologies), confirmation bias (favouring work that support one’s views), or publication bias (favouring positive results). Double-blind peer review has been found to be an effective mediation of these biases. However, manuscripts contain many identifiable characteristics that can provide a reviewer with enough information to correctly identify an author (Baggs et al. 2008), with highly specialized fields, such as bibliometrics, possibly making it easier."
  },
  {
    "objectID": "ch12.html#linguistic-biases",
    "href": "ch12.html#linguistic-biases",
    "title": "12  Equity, diversity, and inclusion in research",
    "section": "12.6 Linguistic biases",
    "text": "12.6 Linguistic biases\nSugimoto - devaluation or dismissal of work when authors write in languages other than English. Survey of bias in peer review (Lee et al., 2013), cite studies of difference in acceptance rates of manuscripts from authors of English-speaking countries and those of non-English speaking countries or that language and wirting style is given as reasons for rejection when there was no problem with the manuscript prosaically (Herrera, 1999).\nDatabases of Scopus and WoS are biased towards English articles, affecting fields such as social sciences and humanities where there are more books in languages other than English due to their subject matter and regional specificity. Compounding this is that US and English-speaking countries dominate web development, particularly academic web development, contributing to even more bias against non-English sources. As such, all indicators, including those that are web-based, are inherently biased towards English documents from the database sources, social media outlets, or seach tools (Mas-Bleda & Thelwall, 2016). F\nixes – multilingual tools, taxonomies, such as the Science-Metrix Classification system which is available in 18 languages (Archambault et al., 2011), (not all, granted, but a start.) However, more tools are needed to address the biases and permit non-English documents to be accessed and cited as the same rate as English documents (Mas-Bleda & Thelwall, 2016).\n\n12.6.1 Other biases?\nLack of data on sexual and gender minorities, (why lumped together?), disabilities (Kwon, 2022 citing Zurn) including the effect of intersectional identities on citation rates or pubication acceptance? From the Intersectional Design website at Stanford University (Jones et al., n.d.)i, we might also considered other factors that may manifest as bias within scholarly publication: age, disability, among others already discussed here.\nIn their review of peer-review bias, Lee, Sugimoto, Zhang, and Cronin (2013) explore other forms of bias that may affect manuscript acceptance rates, including affiliation bias, content-bias, confirmation bias, or publication bias. As these biases have been found to have effect on acceptance rates, double-blind peer review has been found to be an effective mediation on these biases, however, manuscripts contain many identifiable characteristics that can provide a reviewer with enough information to correctly identify an author (Baggs et al., 2008), with highly specialized fields, such as bibliometrics, possibly making it easier.\nBornmann (2011) identifies many of the challenges with bibliometrics, particularly with regards to the peer review process, including the base currently of citations, which are not clean measures, but are indicators with many dependent factors affecting them, be it the number of authors, the journal, the language, the region, institutional affililations, or even if the result is positive or negative. As a symbolic measure, a citation count can be interpreted to mean quality, impact, or just an ‘offer to the scientific community’ (Bornmann, 2011) of results, ideas, concepts, etc. As such, this convenient measure should be understood to be not only the raw data that much of bibliometrics is built upon, but that it symbolizes a complex interplay of multiple factors that are continually challenged by human judgement and behaviours that may or may not be as objective as we wish them to be. Bornmann examines factors affecting citaition counts, such as time-dependency, field-dependency, journal-level dependencies, article-level dependencies, author and reader dependent factors, and information source dependencies. Many of these can be exploited and manifest as dependencies as we’ve covered above. Despite the challenges with citations at the individual level, at the aggregate level, and over an acceptable time period, they are still considered to be an acceptable measure as an indicator of scientific performance and that it is the role of the evaluator, the one that applies the citation measure, to do so in a way that is thoughtful and critical of the factors and biases embedded within this imperfect system (van Raan, 2005).\n\n\n12.6.2 Data sources\nHaving explored a range, though not exhaustive, of biases affecting citation counts, let’s look at some of the data sources that have been utilized in studies examining bias in scholarly communication.\nStudies examining gender bias in scholarly communication utilize algorithms to categorize names within gender categories, (typically binary) based on geographic and cultural inferences. NamSor, genderize.io, GenderAPI, and Wiki-Gendersort, are the main ones found in bibliometric studies investigating gender bias. The algorithms work by harvesting names from openly available databases and also collect other data such as the country of origin and the family name and language as cultural context identifiers. As applications such as NamSor provide scores with each name, all names are assigned scores for probability of fitting within a category. Gender assignment errors are the basis for the effectiveness tests comparing the strengths of one app to another.\nA recent study in 2021 by Sebo (2021) found only one other peer-reviewed study examining the effectiveness of gender determining algorithms. Sebo acknowledged the benefits of these tools as they are cheap, effective, and can be applied retrospectively to datasets (Sebo, 2021). But they also recognize the limitations of such technology drawing on databases that may or may not include self-identification. Without data derived from databases with self-identified gender categories, there is a risk of gender assignment errors at the individual level.\nCurrently there does not seem to be any systematic reviews of these applications used in bibliometric analysis to evaluate gender bias in citations, publication rates, productivity, impact, etc. Like citations, this is an imperfect tool that while more error prone at the individual level, has an accuracy at the aggregate level that is considered acceptable. And while this still presents data along binary categories of gender, (not to mention the common conflation of sex and gender as identity), the purpose in such tools is to address and dismantle the historical and current oppression along the very lines it was trying to maintain. These tools will need to continue to evolve especially with regards to expanding the language around gender and its complex interplay of identity, social norms, and relations. An excellent resource is Gendered Innovations portal at Stanford University (Analyzing Gender, n.d.) for understanding how study design and outcomes affects outcomes from research.\n\n\n12.6.3 World Bank (country income-level)\nContributing to geographic bias, lumping together many countries based on region and economic status. World Bank data as continuing the colonialist ideology of the Global North or Western North, geographic/economic alignment. The World Bank data is widely used, and it appies classification terms, such as low income, middle income, or high income, and these can be considered “arbitrary, unhelpful, or outdated” (Price et al., 2022) not to mention the long-term associations that anything outside the high income countries is of low quality (Harris et al., 2017)."
  },
  {
    "objectID": "ch12.html#how-do-we-do-better",
    "href": "ch12.html#how-do-we-do-better",
    "title": "12  Equity, diversity, and inclusion in research",
    "section": "12.4 How do we do better?",
    "text": "12.4 How do we do better?\nRay et al. (2022) propose citation diversity statements as a reflexive tool to reinforce the commitment to your community of researchers. The following is an example citation diversity statement from Ray et al. (2022):\n\nWe are committed to promoting intellectual and social diversity in science and academic scholarship and took this commitment into consideration while researching and writing this article. We actively worked to promote diversity in our reference list while ensuring all the references cited were relevant and appropriate. We have included some references to enhance diversity but have not omitted any references for this purpose. To assess the diversity of our references, we obtained the predicted gender of the first and last author of each reference by using a database that stores the probability of a first name being carried by a woman (gender-api.com). Using this measure and removing self-citations, our references contain 30% woman(first)/woman(last), 11% man/woman, 15% woman/man, and 44% man/man. This method is limited in that a) names, pronouns, and social media profiles used to construct the database may not, in every case, be indicative of gender identity and b) it cannot account for intersex, non-binary, or transgender people. We look forward to future work that could help us to better understand how to support equitable practices in science.\n\nBecause it is easy to imagine how citation diversity statements could lead to tokenism (diversifying citations artificially for the sole purpose of “looking good”), Ray et al. (2022) insist on the ethical importance of citing works that provide information relevant to a paper, and not simply because of some box on a manuscript submission form that needs to be checked. That said, unconscious biases in citing behaviours may not support the best interests of researchers and their research community. Investment in thoughtful, purposeful citations of works one is engaged with will not only strengthen communities but, when done with an awareness of having diverse voices as a strengthening practice, will also improve the overall quality of scholarly works.\nGiven that disparities exist historically, basing decisions upon results such as these with stereotypes about the quality of all articles in the Global South (also problematic) contributes further to the disparity. From an emancipatory perspective, the path to fixing this is making time to explore, engage with, and understand scholarly production from geographic locations beyond the norm. This not only enriches one’s own writing through a more balanced view but also respects and recognizes advances by researchers.\nDoes the citation technology exist as compatible with “social equity, freedom, and cultural pluralism” or does its existence require centralized control through ownership, market forces, and power concentrations Winner (1980)? On the one hand, there is the rather functional view of the phenomena of social capital in which we see centers of power within scholarly communication and citations as part of the reward system of science, and that by citing, we associate our work with these centralized actors. On the other hand, there is an emancipatory view in which we view citations as a technology that enables us to redistribute and acknowledge those that we have engaged with, recognize, and proliferate ideas that are meaningful to us and our part within a community.\n\n12.4.1 Citational justice\nKumar and Karusala (2021) introduce Iris Marion Young’s faces of oppression as a framework for understanding and addressing citational (in)justice. They define justice as “a relational value of the actions, structures, and institutions in which persons stand to each other as social and political subjects, be they structures of the production and distribution of material goods or of the exercise of political power”,and view the citation as “anti-racist, feminist technologies” (Kumar and Karusala 2021) with the potential to correct the imbalances have occurred. The authors present some examples of ways in which injustices have shown up in their own work and reviews, which may provide an opportunity for self-reflection upon your own citation practices.\n\nExploitation – occurs when the balance of work and compensation is leveraged, creating inequality and power dynamics. This supports the rich-get-richer aspect of Merton’s Matthew effect by leveraging power. The authors identify several types of citation behaviours found in their own work.\n\nThe Cite-Me Cite can occur when submitting papers to journals and the editors pressure the authors to cite their work in return for an acceptance. This is particularly a concern/signal of predatory journal practices.\nThe Name-Agnostic Cite occurs when hard-to-recognize/pronounce/read names are othered, as in “other authors have investigated…”, whereas Western names are clearly cited.\nThe In-the-Global-South and Unrelated-to-the-North Cite falls along similar lines as othering or even making certain work irrelevant. See Linxen et al. (2021) for a study exploring this issue.\nThe Throwaway Cite occurs when citations are lumped together without individual attention or recognition, as in “studies in LIS have examined the effect of unicorns (Name, 1986; Name, 1993; Name, 2000; Name et al., 2002; Name et al., 2013).” While this practice may be an intent to be exhaustive yet concise, who is this benefiting and for what purpose?\nThe No Cite is when references are not made as conscious or unconscious decisions to omit. While addressing this type of non-cite requires greater rigour, not doing so is a privilege that is being assumed.\n\nMarginalization – when a category of persons is excluded and thereby deprived, not only at the individual level but also at the collective level. This is evident in conferences that privilege some populations, such as conferences that hve never been held in the Global South. Some universities dominate some disciplines, which can lead to a misperception of enhanced value, affecting acceptance and possibly citation. Women, scholars of colour, and gender diversity, also exhibit the effect of biases upon their communities, as evidenced by citation gaps.\nPowerlessness – Those in the community that “lack significant power”, a voice, or opportunity to contribute to decision making. This occurs when assumptions are made, creating or reinforcing norms that we expect to be accepted. These assumptions, without critical inquiry, can shape not only our readers but ourselves. This can include the assumptions that work from certain groups lacks rigour, works published at certain venues or in certain journals is not of high quality, Wikipedia is not a valid source of knowledge, papers written in other languages are not relevant, etc.\nCultural imperialism – an interpretation of normal within a society that reflects the dominant society’s cultural values, at once othering other groups within society and reinforcing stereotypes that maintain this power imbalance. In scholarly publications, this is evidenced by a combination of two previous effects othering then uncited. For example, work in the Global South focuses on the poorest communities, focus on novelty or differences within other cultures, the universality of Western ethical standards, and the expectation that English by those outside the Western North is of low quality. Cultural imperialism also occurs when the research of marginalized communities is interpreted within the frameworks of the dominant society for their own use.\nViolence – here, it is important to bring in Young’s words from Kumar and Karusala (2021):\n\n“While the frequency of physical attack on members of these and other racially or sexually marked groups is very disturbing, I also include in this category less severe incidents of harassment, intimidation, or ridicule simply for the purpose of degrading, humiliating, or stigmatizing group members.”\n\n\nThe authors continue to say that it’s less so the violent act than it is the social conditions which continue to permit it to happen. They illustrate this face of oppression in scholarly communications with evidence from reviews in which questions are called about the relevance of a health topic if it only affects a small population, the criticism of research aims of Black scholars as outside of typical scholarship, how disabled persons are singled out for research, or the long-term bias against low citation counts and the assumptions of quality or relevancy.\nThese five faces of oppression attributed to Young by Kumar and Karusala (2021) provide a framework for understanding and deconstructing biases in our choices and assumptions affecting citational justice in scholarly communication. This is not the only framework or examples that can be found. Unethical citation practices have been around for quite some time, so there are more resources for understanding how these exist.\n\n12.4.1.1 Chicken and Egg\nKwon (2022) argues that citation-based evaluation of individual researchers in any context (e.g., funding, publishing, promotion, awards) needs to change or be reduced in importance and replaced by engaging, recognizing and valorizing ideas from diverse sources. Mott and Cockayne (2017) also recognize citations as a problematic technology but also suggest that citations can act as a feminist and anti-racist “technology of resistance” to correct the imbalance. There appears to be a tension there between the need to reduce the influence of citations while at the same time exploiting that influence for the purpose of correcting the injustices perpetrated by them."
  },
  {
    "objectID": "ch12.html#conclusion",
    "href": "ch12.html#conclusion",
    "title": "12  Equity, diversity, and inclusion in research",
    "section": "12.5 Conclusion",
    "text": "12.5 Conclusion\nThis chapter has examined the imbalances that occur in scholarly communication through Merton, Bourdieu, and Rossiter’s respective sociological theories and how our citation behaviours as authors can represent citational injustice or even epistemic injustice through our conscious or unconscious choices. We critically examined a few of the data sources and tools used for analysis within bibliometrics, such as gender-determining algorithms and global income categorization, for their limitations and advantages, as part of the ongoing attempts by the scientific community to address equitable imbalances within our scholarly communication system. I closed with thoughts on thinking about our citation system objectively as functionalists or as emancipatory activists.\nIn writing such a chapter, it must be acknowledged that it was written by a privileged white person who has settled in Canada. Some of the sources I draw from are by self-identified persons of colour, and these sources should be read fully so that my filtered version does not take away from the significance of their words and experiences. This filtration is typical in Western society and represents the endemic cultural imperialism in our education system. I appreciate the opportunity, as a queer, transgender woman, to provide my perspective. Still, I recognize it is a very narrow lens as a participant within a global community of knowledge producers."
  },
  {
    "objectID": "ch12.html#references",
    "href": "ch12.html#references",
    "title": "12  Scientific fraud and questionable research practices",
    "section": "12.6 References",
    "text": "12.6 References\n\n\n\n\nAzoulay, Pierre, Jeffrey L Furman, Krieger, and Fiona Murray. 2015. “Retractions.” Review of Economics and Statistics 97 (5): 1118–36. https://doi.org/10.1162/REST_a_00469.\n\n\nBabbage, Charles. 1830. Reflections on the Decline of Science in England, and Some of Its Causes. London.\n\n\nBird, Stephanie J. 2002. “Self-Plagiarism and Dual and Redundant Publications: What Is the Problem? Commentary on ’Seven Ways to Plagiarize: Handling Real Allegations of Research Misconduct’.” Science and Engineering Ethics 8 (4): 543–44.\n\n\nBird, Steven B., and Marco L. A. Sivilotti. 2008. “Self-Plagiarism, Recycling Fraud, and the Intent to Mislead.” Journal of Medical Toxicology 4 (2): 69–70. https://doi.org/10.1007/BF03160957.\n\n\nBroad, William J. 1983. “Notorious Darsee Case Shakes Assumptions about Science.” New York Times.\n\n\nChubin, Daryl E. 1985. “Miconduct in Research : An Issue of Science Policy and Practice.” Minerva 23 (2): 175–202.\n\n\nCouzin, Jennifer. 2006. “Scientific Fraud.” Science 314 (5807): 1853–53. https://doi.org/10.1126/science.314.5807.1853.\n\n\nCulliton, Barbara J. 1974. “The Sloan-Kettering Affair: A Story Without a Hero.” Science 184 (4137): 644–50. https://doi.org/10.1126/science.184.4137.644.\n\n\nFanelli, Daniele. 2009. “How Many Scientists Fabricate and Falsify Research? A Systematic Review and Meta-Analysis of Survey Data.” PLOS ONE 4 (5): e5738. https://doi.org/10.1371/journal.pone.0005738.\n\n\nGrieneisen, Michael L, and Minghua Zhang. 2012. “A Comprehensive Survey of Retracted Articles from the Scholarly Literature.” PLoS ONE 7 (10): e44118–18.\n\n\nJudson, Horace Freeland. 2004. The Great Betrayal : Fraud in Science. Orlando: Harcourt.\n\n\nKarcz, Marcin, and Peter J Papadakos. 2011. “The Consequences of Fraud and Deceit in Medical Research.” Canadian Journal of Respiratory Therapy 47 (1): 18–27.\n\n\nKochen, Manfred. 1987. “How Well Do We Acknowledge Intellectual Debts?” Journal of Documentation 43 (1): 54–64.\n\n\nLarivée, Serge, and Maria G. Baruffaldi. 1993. La science au-dessus de tout soupçon : enquête sur les fraudes scientifiques. Laval, Québec: Méridien.\n\n\nMerton, Robert K. 1968. Social Theory and Social Structure. New York: Free Press.\n\n\nMongeon, Philippe. 2015. “Costly Collaborations.” JASIST 15 (2): 125–45.\n\n\nReich, Eugenie Samuel. 2009. Plastic Fantastic : How the Biggest Fraud in Physics Shook the Scientific World. New York: Palgrave Macmillan.\n\n\nTilden, Samuel J. 2010. “Incarceration, Restitution, and Lifetime Debarment: Legal Consequences of Scientific Misconduct in the Eric Poehlman Case: Commentary on Scientific Forensics: How the Office of Research Integrity Can Assist Institutional Investigations of Research Miscond.” Science and Engineering Ethics 16 (4): 737–41. https://doi.org/10.1007/s11948-010-9228-0.\n\n\nUS Public Health Service. 1989. “Responsibilities of awardee and applicant institutions for dealing with and reporting possible misconduct in science; final rule.” Federal register 54 (151): 32446–51.\n\n\nWeinstein, Deena. 1981. “Scientific Fraud and Scientific Ethics.” Connecticut Medicine 45 (10): 655–58.\n\n\nWestfall, Richard S. 1973. “Newton and the Fudge Factor.” Science, New series, 179 (4075). https://doi.org/10.2307/1735787.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Scientific fraud and questionable research practices</span>"
    ]
  },
  {
    "objectID": "ch12.html#introduction",
    "href": "ch12.html#introduction",
    "title": "12  Equity, diversity, and inclusion in research",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nLatonia Harris provided these definitions for a 2021 workshop on diversity, equity, inclusion, and anti-racism in STEMM organizations (Scherer 2021):\n\nDiversity: the numerical representation of groups of individuals based on their primary and secondary characteristics and identities.\nEquity: the treatment of individuals in terms of access, opportunity, and advancement.\nInclusion: the ability to meaningfully participate and contribute, both for the benefit of the individual and the organization.\nRacism: the devaluation and the denial of rights, dignity, and value of individuals due to their race or geographical origin.\n\nIn this chapter, we examine the ways in which diversity, equity, and inclusion are lacking and racism may be present in research, and how we may be able to address those issues.\nWe first revisit the social stratification of science by looking at sociological theories as a means of understanding how citations are valued outside of the monetary compensation of labour. We then examine citational justice and epistemic justice as frameworks to understand how we create imbalances as part of our citational behaviours. We then examine some of the biases that have been investigated in bibliometrics and suggest what other biases may be present. We also look at a few data sources to gain an understanding of the limitations and advantages of such tools as an example of the critical perspective needed to understand the complexity of the citation reward system."
  },
  {
    "objectID": "ch9.html#introduction",
    "href": "ch9.html#introduction",
    "title": "9  Visualizing research networks",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nThis chapter introduces network visualization as a tool for bibliometrics research and evaluation.\n\nIntroduction to networks and their terminology\nTypical networks for bibliometrics research and evaluation\nIntroduction to network visualization softwares"
  },
  {
    "objectID": "ch9.html#terminology",
    "href": "ch9.html#terminology",
    "title": "9  Visualizing research networks",
    "section": "9.2 Terminology",
    "text": "9.2 Terminology\nNetworks can be defined as a set of elements (or entities) that have a relationship to one another. Networks can be represented by matrices, lists of nodes and edges, or graphs.\n\n\n\n\n\n\n9.2.1"
  },
  {
    "objectID": "ch9.html#co-occurrence-networks",
    "href": "ch9.html#co-occurrence-networks",
    "title": "9  Visualizing research networks",
    "section": "9.3 Co-occurrence networks",
    "text": "9.3 Co-occurrence networks"
  },
  {
    "objectID": "ch9.html#citation-networks",
    "href": "ch9.html#citation-networks",
    "title": "9  Visualizing research networks",
    "section": "9.4 Citation networks",
    "text": "9.4 Citation networks\n\n9.4.1 Direct citation\n\n\n9.4.2 Bibliographic coupling\n\n\n9.4.3 Co-citations"
  },
  {
    "objectID": "ch2.html#the-organization-of-research",
    "href": "ch2.html#the-organization-of-research",
    "title": "2  The organization and evaluation of research",
    "section": "",
    "text": "Suggested reading\n\n\n\nIf you want to know more about the Royal Society of London, you can check out their website: https://royalsociety.org/about-us/history/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The organization and evaluation of research</span>"
    ]
  },
  {
    "objectID": "ch2.html#scholarly-communications-before-scholarly-journals",
    "href": "ch2.html#scholarly-communications-before-scholarly-journals",
    "title": "2  Journals, peer-review, and the organization and evaluation of research",
    "section": "2.2 Scholarly communications before scholarly journals",
    "text": "2.2 Scholarly communications before scholarly journals\n\n\n\n\n\n\nBenefits of the written work\n\n\n\nWritten works can be disseminated, copied, verified and referenced. they can contain detailed information, including images and figures. They establish priority in discoveries, can be used to generate a record of the scientific knowledge that does not die with the scientists.\n\n\nIf you have ever wondered what people did all day before the internet, you probably have the impression (maybe from Bridgerton) that they just wrote letters all day long. Thus, it is not surprising that the earliest scholarly communication took place through letter writing (“personal correspondence,” if you’re fancy.)\nScientists spread the news of observations and ongoing experiments through letters written to other scientists. Letters were a part of daily life due to their convenience, low cost, and lack of censorship. In the 17th century, letters could be sent across Europe within weeks – great speed for the time.\nHowever, even back then it was too much work to write a letter to each of your scientist friends announcing your most recent discovery. Instead, there were middlemen who collected and distributed these letters to other scientists. One of the most famous of these information gatekeepers was Henry Oldenburg, the secretary of London’s Royal Society. At Society meetings, Oldenburg would read aloud the correspondence he received that pertained to scientific matters. For example, the minutes of a Society meeting from 1667 reads as follows:\n\n“During the recess of the society Mr. OLDENBURG kept up his correspondences with several of the learned men abroad, and particularly HEVELIUS; the letters which passed between them being extent in the LetterBook….\nMr. OLDENBURG read an extract of Monsr. AUZOUT’s letter to him from Paris, Decemb. 28.1666. N.S. mentioning a new method esteemed by him better than any hitherto practised, of taking the diameters of the planets to seconds, and of knowing the parallax of the moon by means of her diameter.” (Fjällbrant 1997)\n\nThese meeting minutes show indications of an international scholarly communication network dating back to the 17th century.\nAnother prominent figure was Father Marin Mersenne, who was at the center of a correspondence network. By visualizing these networks, we are able to observe a part of what we could call the “scientific community” of the time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Journals, peer-review, and the organization and evaluation of research</span>"
    ]
  },
  {
    "objectID": "ch2.html#scholarly-journals",
    "href": "ch2.html#scholarly-journals",
    "title": "2  The organization and evaluation of research",
    "section": "2.3 Scholarly journals",
    "text": "2.3 Scholarly journals\nEven though it was preceded by a few months by the�Journal des Sçavans that eventually became the main dissemination platform of members of the Académie des Sciences, the Philosophical Transactions first published in 1665 by the Royal Society (and still published today) is generally referred to as the first scholarly journal. Its editor-in-chief was Harry Oldenburg. Eventually, these journals came to formalize and organize the process of dissemination of knowledge that had been before relying on informal networks and people like Father Mersenne.\n\n2.3.1 Organizing knowledge and research communities\nWith the growth of science came increased specialization, the creation of new disciplines, scholarly associations, and journals with varying degree of specialization, as well as national journals to accommodate the growth of national science systems that we mentioned above. In this way, journals play an important role in shaping and structuring research communities, and the knowledge that they produce.\n\n\n2.3.2 Quality control (peer review)\nAnother role of journals that may come to mind is the evaluation of research, which is done through a process known as peer review. Peer-review is one of the most central and defining feature of modern science and the scholarly communication process. It is directly related to the Mertonian norm of organized skepticism, and can be defined as “the evaluation of work by one or more people with similar competencies as the producers of the work” (Wikipedia). One job of the journal editor or their team is to invite experts (typically two) to evaluate the article and determine if it is suitable for publication in the journal. The peer review process can be single-blind (the identify of the author known to the reviewer, but the reviewer is anonymous), double-blind (both the reviewer and the author remain anonymous), or open (the identities of all parties are known). In recent years, the idea of post-publication peer review has been gaining some steam. This type of peer review, as the name suggests, occurs only after the article is published (usually on some online platform), and is another form of open and transparent peer review. The main appeal of post-publication peer-review is that it allows for the faster dissemination of knowledge (with the traditional pre-publication peer review can take months or years before a submitted article eventually gets published). The COVID-19 pandemic might have contributed to the recent hype around post-publication peer-review. Since the world needed to have access to research results to do things like developing and approving vaccines. However, letting everyone publish their work before the peer review process occurs can have perverse effects, such as the potential flooding of the web with bad science. Peer review is a quality control mechanism, after all. This short article in Nature discusses some of the challenges encountered by pre-preprint servers bioRxiv.org during the pandemic: https://www.nature.com/articles/d41586-020-01394-6.\n\n\n\n\n\n\nDid you know?\n\n\n\nPeer review as it is conducted today (by external reviewers invited by the editor) is a relatively new thing in science. For hundreds of years following the creation of the Philosophical Transaction the journal editors were the ones who handled the quality control of the works they published. However, as science became more and more institutionalized and the number of researchers and research areas grew, it became impossible for editors to handle all that work. They could also not be expert in all the areas of research that their journals would publish. So, they turned to external reviewers for help.\nSome eminent researchers, like Albert Einstein, were not huge fans of this new process. When the Physical Review journal editor rejected one of his papers based on the external reviewers’ comments, this was Einstein’s response:\n\nDear Sir,\nWe (Mr. Rosen and I) had sent you our manuscript for publication and had not authorized you to show it to specialists before it is printed. I see no reason to address the—in any case erroneous—comments of your anonymous expert. On the basis of this incident I prefer to publish the paper elsewhere.\nRespectfully,\nP.S. Mr. Rosen, who has left for the Soviet Union, has authorized me to represent him in this matter.\n\nBy the way, the reviewer turned out to be right, and Einstein ended up correcting the paper before it was published in another journal.\n\n\nEven though peer review is now a standard and widely accepted element of the scholarly communication process, there are still a lot of researchers who are critical of the process. Here are some of the main critics of peer review.\n\nPeer review is arbitrary. The outcome of the peer review is difficult to predict and the same publication might receive a completely different assessment depending on the reviewers involved.\nPeer review is biased. There are all kinds of ways in which reviews may be biased, prominent scientists get more favorable reviews than those who are unknown. Gender, ethnicity, topics, methods, and innovativeness are all factors that may affect the outcome of the review in one way or another.\nPeer review does not detect error or fraud. In the last two decades, we have seen a surge in the retractions of peer reviewed articles that were found to be erroneous or fraudulent, leading some people to argue that the fact that these papers made it past peer review demonstrates that the process does not work.\nPeer review is exploitative. Peer review is performed voluntarily by members of the scientific community. It also provides little returns in symbolic capital for the reviewer. However, they help increase the quality and reputation of the journals, many of which are owned by corporations who extract large profits from the reputation of their journals.\nPeer review is slow. It may take time for an editor to find suitable reviewers for a paper. Then reviewers are given usually about a month for their review. Then the comments are sent to the authors who are invited to submit a response to the reviewers and an updated manuscript. This process is repeated until the manuscript gets accepted for publication. Here is a meme to illustrate just how slow peer review can be:\n\n\n\n2.3.2.1 Reviewer 2\nThis is probably a good time to introduce the dreaded and infamous reviewer 2. Indeed, if it takes such a long time for a paper to get published, it is a well-known fact that it is partly because of reviewer 2. But who is reviewer 2?\n\n\n\n\n\nEveryone fears reviewer 2, but what they fear even more is being reviewer 2.\n\n\n\n\n\n\n\nDid you know?\n\n\n\nPeer review is not only a part of the scholarly communication process but occurs in almost all stages of the research process (e.g., research funding) and throughout the reward system of science (hiring, tenure, promotion, honorific awards, etc.).\n\n\n\n\n\n2.3.3 Other uses (or misuses) of journals\n\n2.3.3.1 Research evaluation\nJournals having different levels of prestige (symbolic capital), they are often used as proxy for the quality of publications, researchers, and institutions. Publishing in some specific journals can be determinant and even make or break scientific careers, bring universities up in university rankings, etc. For example, the Financial Times produces a business schools ranking in which they use the number of articles published in 50 top journals as an indicator of research excellence.\nIn bibliometrics, the Journal Impact Factor (JIF) is one way in which the prestige of a journal is quantified and measured (based on average number of citations papers published in the journal have received). To appreciate the importance of a journal, one does not have to be a member of the community or be able to recognize the prominence of the editorial board members or the quality of the content published by the journal. One just needs to know 5 is a higher number than 4 and hang on to the belief that this number is a true representation of a journal’s quality or prestige, so that a journal with a JIF of 5 is better than a journal with a JIF of 4. By extension, a scientist who publishes their work in a journal with a JIF of 5 has to be a better scientist than one who publishes in a journal with a JIF of 4. This of course is a widely criticized use of the JIF, which were actually designed to help academic librarians with collection development, not to evaluate science or scientists.\n\n\n2.3.3.2 Generating profits\nMost of the journals published today are owned by a handful of large corporations that control what is one of the most profitable industries of modern times. This is done largely by selling back to the scientific community (mainly through academic libraries) the product of its free labor (research, peer-review). We will address this topic again in a later chapter about open access publishing.\n\n\n\n\n\n\nSuggested readings\n\n\n\nThis article published in the Guardian is a fascinating dive into the recent history of the scholarly publishing industry and how it came to be one of the most profitable industry of modern times.\nThis blog post discusses the profits generated through the article processing charges (APCs) that publishers typically charge to make research articles openly accessible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The organization and evaluation of research</span>"
    ]
  },
  {
    "objectID": "ch2.html#references",
    "href": "ch2.html#references",
    "title": "2  Data structures",
    "section": "2.4 References",
    "text": "2.4 References",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "ch2.html#scholarly-communication-before-scholarly-journals",
    "href": "ch2.html#scholarly-communication-before-scholarly-journals",
    "title": "2  The organization and evaluation of research",
    "section": "2.2 Scholarly communication before scholarly journals",
    "text": "2.2 Scholarly communication before scholarly journals\n\n\n\n\n\n\nBenefits of the written work\n\n\n\nWritten works can be disseminated, copied, verified and referenced. they can contain detailed information, including images and figures. They establish priority in discoveries, can be used to generate a record of the scientific knowledge that does not die with the scientists.\n\n\nIf you have ever wondered what people did all day before the internet, you probably have the impression (maybe from Bridgerton) that they just wrote letters all day long. Thus, it is not surprising that the earliest scholarly communication took place through letter writing (“personal correspondence,” if you’re fancy.)\nScientists spread the news of observations and ongoing experiments through letters written to other scientists. Letters were a part of daily life due to their convenience, low cost, and lack of censorship. In the 17th century, letters could be sent across Europe within weeks – great speed for the time.\nHowever, even back then it was too much work to write a letter to each of your scientist friends announcing your most recent discovery. Instead, there were middlemen who collected and distributed these letters to other scientists. One of the most famous of these information gatekeepers was Henry Oldenburg, the secretary of London’s Royal Society. At Society meetings, Oldenburg would read aloud the correspondence he received that pertained to scientific matters. For example, the minutes of a Society meeting from 1667 reads as follows:\n\n“During the recess of the society Mr. OLDENBURG kept up his correspondences with several of the learned men abroad, and particularly HEVELIUS; the letters which passed between them being extent in the LetterBook….\nMr. OLDENBURG read an extract of Monsr. AUZOUT’s letter to him from Paris, Decemb. 28.1666. N.S. mentioning a new method esteemed by him better than any hitherto practised, of taking the diameters of the planets to seconds, and of knowing the parallax of the moon by means of her diameter.” (Fjällbrant 1997)\n\nThese meeting minutes show indications of an international scholarly communication network dating back to the 17th century.\nAnother prominent figure was Father Marin Mersenne, who was at the center of a correspondence network. By visualizing these networks, we are able to observe a part of what we could call the “scientific community” of the time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The organization and evaluation of research</span>"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Working with data",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nUpon completion of this course, students should be able to:\n\nManipulate, assess, analyze, and share raw data of different types (including quantitative data, qualitative data, and GIS data)\nIdentify the nature and extent of data needed and collected for given managerial purposes\nLocate, collect, and integrate data of multiple types from multiple sources\nUse data to inform evidence-based decision-making\nUnderstand the data lifecycle (including collection, management, analysis, and application)\nDemonstrate critical evaluation at various stages of the data lifecycle\nAssess data visualizations and analyses for accuracy, relevance, and objectivity\nEmploy a range of data skills to address diverse, real-world scenarios/problems in multiple sectors and disciplines\nUse data ethically and with awareness of the capabilities and limitations of data analysis",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Working with data",
    "section": "Schedule",
    "text": "Schedule\nComing soon…\n\n\n\n\n\n\nImportant\n\n\n\nUse the course Brightspace to:\n- Access the detailed instruction for the assignments.\n- Submit your assignments.",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "index.html#other-information-and-resources",
    "href": "index.html#other-information-and-resources",
    "title": "Working with data",
    "section": "Other information and resources",
    "text": "Other information and resources\n\nUse APA style for your references in all assignments.\nUse Zotero (or another reference manager of your choice) to store your references and to cite them in your work. (not mandatory, but highly recommended).\nYou can use this Word template to write your assignments (not mandatory). Whether you use this template or not, use Word styles to format and structure your document.\nHave fun.",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "ch3.html#mertonian-framework",
    "href": "ch3.html#mertonian-framework",
    "title": "3  The social structure of science",
    "section": "3.2 Mertonian framework",
    "text": "3.2 Mertonian framework\nMerton was a highly influential American sociologist who is said to have essentially created the modern sociology of science. For Merton, the institutional goal of science is the extension of certified knowledge, and the ethos of science comprises that goal and a set of norms that ensure its fulfillment. The following sections provide an overview of the part of his work, as well as the work of some of his students and close collaborators, through which they sought to understand the conditions under which science as a social system operates in accordance with the ethos of science and sometimes deviate from it.\n\n3.2.1 The norms of science\nMerton (1942) defined four norms of science which are binding to all scientists:\n\nCommunism: Science is a common good. This emphasizes the importance of the dissemination of discoveries, which is necessary for their inclusion in the common stock of knowledge.\nUniversalism: Contributions to the advancement of knowledge need to be judged for their own merit irrespective of the characteristics of the individuals or groups involved. Similarly, access to scientific careers must be based on relevant criteria and not sociodemographic characteristics or other irrelevant factors.\nDisinterestedness refers to the pursuit of knowledge for its own sake and for the benefit of humankind, and not for personal gains. Scientists are characterized by a passion for knowledge, curiosity, and the desire to ameliorate the human condition. Merton emphasizes that if scientists, as a group, disproportionally exhibit these characteristics, it’s because the scientific system rewards disinterestedness and not so much because naturally disinterested people decide to become scientists.\nOrganized skepticism:This norm operates at two levels. First, the institution of science exercises a collective form of skepticism towards beliefs and truth claims and seeks to subject them to thorough examination and empirical validation. Second, every scientific contribution must be subjected to the unbiased scrutiny of peers before being accepted as such and incorporated into the common stock of scientific knowledge.\n\n\n\n3.2.2 Reward system of science\nBecause of the norms of communism and disinterestedness, the motivation of researchers is the recognition they receive from their peers (Zuckerman 1977a). As J. R. Cole and Cole (1973) put it:\n\nBecause recognition is so important to scientists, there must be a reward system that identifies and honors scientific excellence wherever it is found. If a scientist desires to acquire “property”, he can only do so through recognition by the system, since there are no other legitimate ways to obtain property in science. (p. 46)\n\nBecause the role of researchers is to advance knowledge, the scientific community gradually developed a reward system through which those who best fulfill this role are compensated (Merton 1957). This system works well when those who deserve recognition receive it, and the most promising researchers are provided with the resources they need to realize their potential. According to Merton (1973) this benefits both the individual researchers and science as a whole.\nThe reward system comprises many reward mechanisms that form a hierarchy. For Merton (1957), the most permanent and prestigious form of institutionalized recognition in science is eponymic reward: the practice of attaching a researcher’s name to a discovery (e.g., Zipf law, Planck’s constant, or Copernican system), a field (Comte, father of sociology), or a period (e.g., the Freudian era, the Darwinian era). Other prestigious forms of reward include honorific awards like the Nobel prize, membership to academies of science, or other nobility titles. Those who receive such prestigious titles are often seen as the scientific elite and are at the top of the stratified social structure of science (J. R. Cole and Cole 1973).\nBecause there are limited spots at the top of the hierarchy, there are many researchers whose contributions may be just as extraordinary (if not more) than those of Nobel prize winners but are not awarded such honours (Merton 1968). The scientific community thus creates less prestigious awards and prizes to highlight these accomplishments. However, these are still few in number and only bestowed upon a minority of researchers (Zuckerman 1977b; S. Cole and Cole 1967). So again, there are other mechanisms to reward accomplished scientists, such as nomination to important positions in scientific institutions or as editors of scholarly journals. At the bottom of the hierarchy, visibility is the most basic form of scientific reward: being published and cited by peers (J. R. Cole and Cole 1973).\n\n\n3.2.3 Social stratification\nThe social stratification of science is a systematic effect of the differentiation and evaluation of researchers who are positioned in a structure that determines their access to resources and opportunities Merton (1968) based on their recognized contributions or potential. Prestigious institutions with more resources recruit Individuals with recognized potential which confers to these individuals (and to the institutions) a competitive advantage. And so does the stratified social structure of science take form through a mixture of self and social selection (Merton 1979). Through their contributions to science, the researchers will then accumulate power and authority, maintain or grow their advantage, and contribute to the asymmetric distribution of resources, productivity, visibility and prestige (J. R. Cole and Cole 1973).\n\n\n3.2.4 Accumulation of advantages and disadvantages\nThe social stratification of science can generate the accumulation of advantages and disadvantages by scientists, thus increasing the gap between the haves and the have-nots (Zuckerman 1998). Identifying promising researchers and providing them with a certain advantage has a short and long-term effect on their careers. When it is working optimally, the divide between the haves and the have-nots grows exponentially. The best performing researchers obtaining more resources, which they use to perform even better and in return obtain even more resources, and so on. According to Zuckerman (1998), the system would become dysfunctional if resource allocation were based on criteria that don’t relate to performance (e.g., gender, religion, ethnicity) or if resources were equally or randomly distributed. Overall, the accumulation of advantages and disadvantages contributes to the social stratification of science which, according to Zuckerman (1998), is essential for the optimization of the scientific system and the advancement of knowledge.\n\n3.2.4.1 Matthew effect\nThe value of a scientific contribution and the recognition that the researchers obtain in return is inevitably based on the subjective judgment of their peers, which can introduce some dysfunctions in the reward system, such as what Merton (1968) called the Matthew Effect, defined as “the accruing of greater increments of recognition for particular scientific contributions to scientists of considerable repute and the withholding of such recognition from scientists who have not yet made their mark” (p. 53). According to J. R. Cole and Cole (1973), the Matthew effect is a direct effect of the social stratification of science, which amplifies the accumulation of advantages and disadvantages by overestimating the merit of some researchers at the detriment of others. The Matthew effect occurs at many levels, such as the recognition of one’s accomplishments, in peer-review, awards, research funding and so on.\n\n\n3.2.4.2 Mathilda effect\nRossiter (1993) showed three decades ago that women in science tended to be disadvantaged by being deprived partly or totally of recognition for their scientific contributions. Like the Matthew effect, this violates the norm of universalism. Some of the most significant scientific discoveries made by women led to their male collaborators winning the Nobel Prize. Even if today the gap between men and women in science has narrowed, women remain less likely then men to obtain awards or important positions Moss-Racusin et al. (2012) and tend to be less cited (Larivière et al. 2013). It should be acknowledged here that these theories were developed before the generalized recognition of gender as a non-binary concept, and that the disadvantages discussed here have been shown to apply to minorities and equity-deserving groups more broadly. I should also mention here that still to this day, bibliometric studies tend to to operationalize gender as a binary concept, which is mostly due to the fact that bibliographic databases do not usually provide the gender of authors, and we must then rely on other available data like first names and geographical location to guess the gender of authors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The social structure of science</span>"
    ]
  },
  {
    "objectID": "ch3.html#bourdieusian-framework",
    "href": "ch3.html#bourdieusian-framework",
    "title": "3  The social structure of science",
    "section": "3.3 Bourdieusian framework",
    "text": "3.3 Bourdieusian framework\nWe now turn our attention to the theories developed by French sociologist Pierre Bourdieu, whose perspective on science is fundamentally different than Merton’s as it is centred not around a set of institutional goals and norms but around power struggles between pragmatic self-interested agents.\n\n3.3.1 Field and agents\nAccording to Bourdieu (1975), the social space is divided in several distinct fields that are relatively autonomous and have specific interests and stakes. Science can be understood as a field with the specific goal of advancing knowledge. The field is composed of agents that possess different forms of capital (discussed below) and can be defined as entities recognized by their peers and who internalize the goals and norms of the field. There are two key concepts in this definition. First, the concept of recognition here highlights that certain conditions must be met for an agent to integrate a field, and the concept of internalization, which is tied to the concept of habitus discussed further..\nThe structure of a field is determined by the relative position occupied by all of the agents in the field based on the type and amount of capital they possess. This structure is dynamic because agents within a field constantly compete and mobilize their capital to acquire more capital, increase their symbolic power, and dominate the field.\nThere are also meta-fields which have some degree of influence on many other fields. The state is an example of a meta-field since it can, to some degree, impose regulations on other fields and play an important role in distributing resources within fields (e.g., funding corporations and universities). Bourdieu’s theory also includes a field of power composed of agents with a lot of capital that they can use to gain influence in other fields. Think, for example, of movie stars who can mobilize that stardom to gain political influence.\nThe concept of sub-fields is also important. For instance, the scientific community is formed by a set of agents that share a goal (advancing knowledge), but it is also composed of many sub-fields with their own specific goals (e.g., the production of knowledge in a certain area), practices and norms. The existence of meta-fields and sub-fields highlights the limited autonomy of fields.\n\n\n3.3.2 Capital\nBourdieu’s theory of capital incorporates Durkheim’s concept of cultural capital and Marx’s concept of economic capital, to which it adds the concepts of social capital and symbolic capital. Every field also has its form of capital (e.g., scientific capital) that can only be acquired within the field and has limited utility outside it.\n\n3.3.2.1 Economic capital\nEconomic capital comprises an agent’s financial resources, material assets, revenues, and means of production. This capital can be transformed into other types of capital. For instance, in the scientific field, a researcher might invest various forms of economic capital (research funds, work, equipment) to produce a contribution to knowledge and thus obtain some scientific capital.\n\n\n3.3.2.2 Social capital\nBourdieu (1980) defines social capital as resources available or potentially available to an agent by virtue of their belonging to a group, to a network of agents that recognize the links that unite them. The social capital of an agent is thus the extent of the network (number of nodes), the type and amount of capital that the connected agents possess, and the capacity of the agent to mobilize this capital. Social capital is acquired through efforts to create and maintain relationships through events (e.g., conferences), places (e.g., research laboratories), and practices (e.g., collaboration, peer review).\n\n\n3.3.2.3 Cultural capital\nAccording to Bourdieu (1979), cultural capital can be embodied, objectified, or institutionalized. Embodied cultural capital is the knowledge of an agent. Objectified cultural capital can be things like books, works of art, or musical instruments. Finally, an example of institutionalized cultural capital is a diploma. While agents may not recognize embodied capital within a field, diplomas and similar credentials can often guarantee a minimal degree of recognition within a field.\n\n\n3.3.2.4 Symbolic capital\nThe knowledge and recognition by agents of a field of the different forms of capital than an agent possesses confers symbolic capital to that agent Bourdieu (1987). Thus, symbolic capital is a form of meta-capital that is the supreme objective of agents’ actions in a field, as it is the recognition of one’s position within it. The distribution of symbolic capital ultimately determines the power structure at play in the field.\n\n\n3.3.2.5 Scientific capital\nScientific capital is a form of symbolic capital specific to the scientific field which determines its structure (Bourdieu 1975). It is the recognition by peers of one’s contribution to scientific progress (Bourdieu 1997). It provides its owner scientific authority and legitimacy in scientific matters. Scientific capital is provided by peers, which are also competitors. This is one of the most important features of science (peer review) and is related to the organized skepticism norm. Furthermore, not all contributions are equal, so the amount of scientific capital an agent may possess is determined by the value and originality of their contribution. The scientific article (or other forms of scholarly publications) materializes that contribution and is an example of what Bourdieu (1971) calls “symbolic goods”: objects that have some value in symbolic capital within a specific field.\n\n\n\n3.3.3 Habitus\nThe habitus of an individual is their mental representation of the world and of their position and the position of others within it. It is the structure of the field in its embodied form Bourdieu (1989). Agents acquire the habitus of a specific field through learning and experience within it. Through this process, agents eventually recognize the goals, norms, and structure of the field and are thus able to invest their capital strategically. The field and the habitus are thus interdependent. The habitus is structured by the field of which it is an embodied form, but it also structures the field since it guides the action of agents, which alter the structure of the field.\nThe habitus produces some regularity in the action of agents because it is derived from an objective structure common to all. However, Bourdieu’s theory insists that agents are guided by pragmatism more than by norms or rules. As Bourdieu (1986) points out, “we must avoid seeing in agent’s behaviour more logic than there is, because the logic of pragmatism is to be logical to a point where being logical ceases to be practical”. (p. 41, my translation).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The social structure of science</span>"
    ]
  },
  {
    "objectID": "ch3.html#summary",
    "href": "ch3.html#summary",
    "title": "3  The social structure of science",
    "section": "3.4 Summary",
    "text": "3.4 Summary\nThis chapter explored science as a social system with a set of goals, norms, and a distribution of capital that together define the system’s structure. By looking at both the Mertonian and Bourdieusian perspectives, we can understand this system as one where agents are at the same time guided by institutional norms common to all and by their strategies aimed at improving their position in the field. We described some mechanisms through which the system recognizes and rewards researchers for their contributions, this recognition being the main driver of one’s ascension in the stratified structure of science. We also explored some factors that can distort these recognition mechanisms, such as the Matthew effect and the Mathilda effect. These remind us that, ultimately, recognition and reward are subjective processes.\nDespite their differences, Merton and Bourdieu’s theories have a lot in common, most importantly the principle of social stratification providing some researchers with an advantage over others. Furthermore, both theories highlight the importance of accumulating peer recognition (Merton 1957) or symbolic capital (Bourdieu 1987) for researchers. This recognition is mainly achieved by contributing to the advancement of knowledge and playing by the rules of the game, at least as long as one sees this as the best advancement strategy.\n\n\n\n\nBourdieu, Pierre. 1971. “Le Marché Des Biens Symboliques.” L’Année Sociologique 3 (22): 49–126. https://doi.org/10.2307/27887912.\n\n\n———. 1975. “The Specificity of the Scientific Field and the Social Conditions of the Progress of Reason.” Social Science Information 14 (6): 19–47. https://doi.org/10.1177/053901847501400602.\n\n\n———. 1979. “Les Trois États Du Capital Culturel.” Actes de La Recherche En Sciences Sociales 30 (1): 3–6. https://doi.org/10.3406/arss.1979.2654.\n\n\n———. 1980. “Le Capital Social. Notes Provisoires.” Actes de La Recherche En Sciences Sociales 31 (1): 2–3.\n\n\n———. 1986. “The Forms of Capital.” In, edited by John G. Richardson, 241–58. New York: Greenwood.\n\n\n———. 1987. Choses Dites. Paris: Editions de minuit.\n\n\n———. 1989. La Noblesse d’état: Grandes Écoles Et Esprit de Corps. Les Editions de minuit.\n\n\n———. 1997. Les usages sociaux de la science: pour une sociologie clinique du champ scientifique ; une conférence-débat organisée par le groupe Sciences en questions Paris, INRA, 11 mars 1997. Sciences en questions. Paris: Institut national de la recherche agronomique.\n\n\nCole, Jonathan R, and Stephen Cole. 1973. Social Stratification in Science. Chicago, IL: University of Chicago Press.\n\n\nCole, Stephen, and Jonathan R Cole. 1967. “Scientific Output and Recognition: A Study in the Operation of the Reward System in Science.” American Sociological Review 32 (3): 377–90. http://www.jstor.org/stable/2091085.\n\n\nLarivière, Vincent, Chaoqun Ni, Yves Gingras, Blaise Cronin, and Cassidy R. Sugimoto. 2013. “Bibliometrics: Global Gender Disparities in Science.” Nature 504 (7479): 211–13. https://doi.org/10.1038/504211a.\n\n\nLincoln, Anne E., Stephanie Pincus, Janet Bandows Koster, and Phoebe S. Leboy. 2012. “The Matilda Effect in Science: Awards and Prizes in the US, 1990s and 2000s.” Social Studies of Science 42 (2): 307–20. https://doi.org/10.1177/0306312711435830.\n\n\nMerton, Robert K. 1942. “A Note on Science and Democracy.” Journal of Legal and Political Sociology, no. 1-2: 115–26. https://doi.org/2027/mdp.39015008014428.\n\n\n———. 1957. “Priorities in Scientific Discovery: A Chapter in the Sociology of Science.” American Sociological Review 22 (6): 635–35. https://doi.org/10.2307/2089193.\n\n\n———. 1968. Social Theory and Social Structure. New York: Free Press.\n\n\n———. 1973. “Recognition’and ’Excellence’: Instructive Ambiguities.” RK Merton, The Sociology of Science. Theoretical and …. http://scholar.google.ca/scholar?hl=fr&q=recognition+and+excellence+merton&btnG=&lr=#0.\n\n\n———. 1979. The Sociology of Science: An Episodic Memoir. Carbondale, IL: Southern Illinois University Press.\n\n\nMoss-Racusin, Corinne A, John F Dovidio, Victoria L Brescoll, Mark J Graham, and Jo Handelsman. 2012. “Science Faculty’s Subtle Gender Biases Favor Male Students.” Proceedings of the National Academy of Sciences 109 (41): 16474–79. https://doi.org/10.1073/pnas.1211286109.\n\n\nRossiter, Margaret W. 1993. “The Matthew Matilda Effect in Science.” Social Studies of Science 23 (2): 325–41. https://doi.org/10.2307/285482.\n\n\nZuckerman, Harriet A. 1977a. “Scientific Elite: Nobel Laureates in the United States.” http://books.google.ca/books?hl=fr&lr=&id=HAHCzJfmD5IC&oi=fnd&pg=PR13&dq=zukerman+nobel+prize&ots=5P05DL9tA2&sig=KcXglSng7NvCOudkwv5uCK3eo3I.\n\n\n———. 1977b. “Scientific Elite: Nobel Laureates in the United States.” http://books.google.ca/books?hl=fr&lr=&id=HAHCzJfmD5IC&oi=fnd&pg=PR13&dq=zukerman+nobel+prize&ots=5P05DL9tA2&sig=KcXglSng7NvCOudkwv5uCK3eo3I.\n\n\n———. 1998. “Accumulation of Advantage and Disadvantage: The Theory and Its Intellectual Biography.” In, edited by Carlo Mongardini and Simonetta Tabboni, 139–61. New Brunswick, N.J.: Transaction Publishers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The social structure of science</span>"
    ]
  },
  {
    "objectID": "ch4.html#introduction",
    "href": "ch4.html#introduction",
    "title": "4  Bibliometric data sources",
    "section": "",
    "text": "Source: https://askabiologist.asu.edu/explore/anatomy-of-an-article\n\n\n\n\n\nThey index more metadata elements from the paper\nThey enrich the metadata by adding things like disciplinary classifications, unique identifiers for authors and other entities, etc.\nThey index citations: this is why these databases are often called citation indexes. (or scientific knowledge graph (SKG), which may be a better name since these databases generally include more than citations).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bibliometric data sources</span>"
    ]
  },
  {
    "objectID": "ch5.html#introduction",
    "href": "ch5.html#introduction",
    "title": "5  Classifying research",
    "section": "",
    "text": "What are disciplines\n\n\n\nThe following quote highlights a few key points about disciplines and what defines them:\n\n“Disciplines are defined (in part) and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties within colleges and universities to which their practitioners belong” (Wikipedia)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classifying research</span>"
    ]
  },
  {
    "objectID": "ch6.html#introduction",
    "href": "ch6.html#introduction",
    "title": "6  Visualizing research networks",
    "section": "",
    "text": "Introduction to networks and their terminology\nTypical networks for bibliometrics research and evaluation\nIntroduction to network visualization softwares",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing research networks</span>"
    ]
  },
  {
    "objectID": "ch6.html#terminology",
    "href": "ch6.html#terminology",
    "title": "6  Visualizing research networks",
    "section": "6.2 Terminology",
    "text": "6.2 Terminology\nNetworks can be defined as a set of elements (or entities) that have a relationship to one another. Networks can be represented by matrices, lists of nodes and edges, or graphs. In Figure 1, the same network containing elements A, B, C, D, and E (the nodes) with links (edges) between some of them, represented in a matrix, a list of nodes and a list of edges, and displayed in a graph. Because in this case the links between the\n\n\n\nFigure 1. Example of a binary network represented by a matrix, a list of nodes and a list of edges, and a graph.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing research networks</span>"
    ]
  },
  {
    "objectID": "ch6.html#typical-networks-in-bibliometrics-research-and-evaluation",
    "href": "ch6.html#typical-networks-in-bibliometrics-research-and-evaluation",
    "title": "6  Visualizing research networks",
    "section": "6.3 Typical networks in bibliometrics research and evaluation",
    "text": "6.3 Typical networks in bibliometrics research and evaluation\nUnit of analysis (nodes)\nThe unit of analysis, or the nodes of the networks, represent the entities: the things that are connected to one another in the network. In Figure 1 and Figure 2, the nodes were labelled A, B, C, D, and E. What could these letters represent in a bibliometric network? Essentially, the nodes can represent scholarly works (i.e., every node represents a single work) or any entity that can be identified in the bibliographic record of the works (e.g., authors, departments, institutions, countries, journals, research areas, keywords, funding organization, grant number, etc.) or that extracted from elements of the bibliographic records (e.g., words or terms extracted from the titles and/or abstracts).\nThe networks we typically encounter in the bibliometrics literature and research assessments usually fall into two broad categories: Co-occurrence networks and citation-based networks. We discuss these two categories of networks with examples below.\n\n6.3.1 Co-occurrence networks\n\n6.3.1.1 Co-authorship networks\n\n\n6.3.1.2 Term co-occurrence networks\n\n\n\n6.3.2 Citation-based networks\n\n\n6.3.3 Direct citation\n\n\n6.3.4 Bibliographic coupling\n\n\n6.3.5 Co-citations",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing research networks</span>"
    ]
  },
  {
    "objectID": "ch7.html#the-functions-of-scientific-authorship",
    "href": "ch7.html#the-functions-of-scientific-authorship",
    "title": "7  Measuring research output",
    "section": "7.2 The functions of scientific authorship",
    "text": "7.2 The functions of scientific authorship\nAccording to Birnholtz (2006), the functions of authorship in science are to assign credit, ownership, and responsibility for discoveries, as well as to enable the existence of a reputation economy.\n\n7.2.1 Credit\nPeer recognition is the main reward obtained by researchers; this symbolic capital then allows them to obtain other rewards with symbolic but also economic value. It is therefore important for researchers to obtain this credit for their contributions and the codes of ethics of disciplinary associations (and, to a lesser extent, the editorial policies of journals) emphasize the importance of giving all contributors the credit due to them. It is thanks to authorship that the system of recognition of science can function, by granting an institutionalized form of credit (awards and positions, for instance) to researchers who have made important contributions to the advancement of knowledge.\n\n\n7.2.2 Ownership and responsibility\nThe link between authorship and ownership applies less to science than to other domains (e.g. the literary domain), because of the Mertonian norm of disinterestedness. According to Biagioli et al. (2003), only the original expression of scientific discoveries, generally under textual form, is the property of the authors, which grants them protection against plagiarism. Responsibility, which can be considered the other side of the ownership coin, has more relevance in science. The proper functioning of the scientific reward system relies on its ability to reward those who contribute to the goal and follow the norms, and its ability to punish (or at least not reward) those who act against the goals and norms of the system. Scientific fraud (e.g. fabrication, falsification, or plagiarism) is an example of non-compliance with the standards of science having a negative impact on the career of the researchers involved (Mongeon 2015), and even on their institutions and their disciplines (Azoulay et al. 2015). Although they remain relatively rare, the increase in cases of scientific fraud has contributed to the development of authorship guidelines which, as we will see later, emphasize the responsibility of researchers for the validity of the articles’ content.\n\n\n7.2.3 Reputation economy\nAuthorship, among other things, provides scientific capital to the researcher. Once known and recognized by the other researchers in the field, this scientific capital is converted into symbolic capital (Bourdieu 1987). Authorship is the basis of the social stratification of scientific fields, and all the layers of the hierarchy of rewards, from the simple citation to the Nobel Prize (Cole and Cole 1973). There is no social structure of science without authorship.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measuring research output</span>"
    ]
  },
  {
    "objectID": "ch7.html#authorship-practices-an-norms",
    "href": "ch7.html#authorship-practices-an-norms",
    "title": "7  Measuring research output",
    "section": "7.3 Authorship practices an norms",
    "text": "7.3 Authorship practices an norms\nThe byline of an article ultimately depends on two elements:\n\nThe nature and extent of the contribution of those involved in the works (who did what?)\nThe decisions made about naming and ordering authors (who will be an author, and in what order will the names be listed?)\n\nThese questions can be difficult to answer in a context where science is increasingly complex and collaborative (Wuchty, Jones, and Uzzi 2007).\n\n7.3.1 Collaboration and division of labour\nLaudel (2002) identified six types of collaboration related to the types of contributions that individuals can make to a given research project:\n\nCollaboration involving a division of intellectual labour. Collaborators with a shared goal who make substantial intellectual contribution towards that goal. They are to some degree co-leading the research.\nService collaboration. Researchers who are called upon to produce routine work that require a specific expertise.\nProvision of access to research equipment. The collaborator does not perform any tasks related to the project but provides material, equipment, data, etc. used for the research.\nTransmission of know-how. The non-creative transmission of information stored in memory that is useful for the research.\nMutual stimulation: The stimulation and engagement through informal interactions that helps researchers develop their ideas.\nTrusted assessorship: Providing feedback on the work.\n\nNot all these forms of collaboration lead to authorship. Laudel (2002) found that authorship is usually attributed to collaborators of the first two types only.\nSubramanyam (1983) proposed four types of collaboration related to the hierarchical status of the involved individuals:\n\nSame-status collaborations.\nProfessor-student collaborations.\nSupervisor-assistant collaborations.\nResearcher-consultant collaborations.\n\nThe number of people required for a given project can have an influence on the types of collaboration that will take place. According to Walsh and Lee (2015), larger teams tend to lead to more bureaucratic organizational structures, characterized by increased division of labour, specialization and standardization of tasks, hierarchical relationships, and decentralized decision-making.\nAssigning authorship can be difficult when a work is the outcome of  many individuals with different statuses, roles, and contributions. Decisions have to be made about 1) who will be an author (not all contributions lead to authorship) and 2) the order of the names, which is usually meant to indicate what share of the credit each one deserves (Pontille 2006). It can be extremely difficult for an external observer to determine who did what or what share of credit everyone deserves (Rennie, Yank, and Emanuel 1997).\n\n\n7.3.2 Naming authors\nUnlike the concept of author in literature, which is linked with writing, the concept of author in science is not and other forms of contributions can lead to authorship. Moreover, as Pontille (2006) points out, the act of writing is not necessarily sufficient to obtain author status, and it is also possible to be an author without writing. So what kind of contributions do lead to authorship? Contributions involving the division of intellectual labour (the first type of collaboration identified by Laudel (2002)). This link between substantial intellectual contribution and authorship generates little to no debate in the literature. However, whether authorship should be awarded to individuals with technical, routine, or less substantial contributions is not so clear.\nHagstrom (1964) tackles the question of authorship for technical contributions by looking at the historical context and distinguishing traditional collaboration from modern collaboration. He portrays the traditional technician as having few qualifications and being involved in the search for solutions to scientific problems but performing simple tasks designed and assigned by the scientist in exchange for economic capital. In contrast, the modern technician is a qualified professional performing complex tasks that the researcher employing them may not know how to perform themselves. This professionalization of the technician is nicely illustrated by Knorr-Cetina (1999) who tells the story of an established researcher who mastered the theories of his field but was unable to execute the technical tasks their research required. This type of relationship between the scientist and the technician is not generalized and technical contributions do not always lead to authorship today (Haeussler and Sauermann 2015; Larivière et al. 2016; Laudel 2002).\nThe type of contribution is not always the only determinant. The same task performed by a paid technician or by another researcher may lead to authorship for the latter but not the former (Pontille 2016; Shibayama, Walsh, and Baba 2012).\nThe importance of a discovery can also affect authorship attribution. Jabbehdari and Walsh (2017) found that the individuals who performed technical or less substantial contributions are more often excluded from the byline of highly cited work.\nThe relationship between the nature and extent of a contribution and authorship is also determined by implicit disciplinary norms. Some type of contributions that often lead to authorship in a discipline may not (and may even be considered unethical) in another (Bozeman and Youtie 2016). For example, in sociology, writing remains one of the most important contribution and tends to be required to be an author (Pontille 2004). However, Pontille distinguishes French sociology from American sociology, where authorship norms are more inclusive. This highlights that the disciplinary context is not entirely independent from the local context. In physics, we find the special case of mega-collaborations leading to articles signed by hundreds (or thousands) of authors; a phenomena that Cronin (2001) calls “hyperauthorsip”. In this context, it is the affiliation to the collective and not a specific contribution to the publication, that justifies authorship (Biagioli et al. 2003; Birnholtz 2006; Knorr-Cetina 1999).\n\n7.3.2.1 Guest authors, ghost authors, and acknowledgees\nIn some cases, the relationship between an individual’s contribution to a work and the authorship status can be weak or nonexistent. It is frequent for individuals to be named as (guest) authors despite having made very little or no contribution to the work, and for individuals to not appear on the byline despite having offered a significant contribution to the work (ghost authors) (Bates et al. 2004; Flanagin et al. 1998; Sismondo 2009; Mowatt et al. 2002; Wislar et al. 2011). Contributors excluded from the byline are sometimes acknowledged in the acknowledgements section of the article to signal their contribution to the work (Kassirer and Angell 1991). Paul-Hus et al. (2017) found that the disciplinary differences in the number of authors are greatly reduced when we consider combined authors and acknowledgees, which suggests that these differences are not a reflection of the different number of contributors involved, but also of disciplinary differences in authorship practices. Overall, the existence of guest authors, ghost authors, and acknowledgees suggests that the byline can misrepresent the composition and size of research teams.\n\n\n\n7.3.3 Ordering authors\nAuthorship decisions are not only about who will be an author but also in what order the names will be listed, which is usually intended to reflect the importance of individual contributions to the work (Zuckerman 1968). In this section, we survey ordering approaches used in science.\n\n7.3.3.1 Alphabetical order\nAlphabetical order is sometimes used to distribute credit equally between members of the research team. It is also standard practice in some fields like Mathematics and Economics. This mode of ordering is also often used in hyperauthorship situations where specific individual contributions are difficult to capture and order (Knorr-Cetina 1999). Sometimes the authors will be grouped by institution or country first, and then ordered alphabetically within each group. The use of alphabetical in some cases but not always can generate some ambiguity since the first (or last) authors might be perceived as having made more important contributions than others even if that is not the case. Furthermore, it is possible for alphabetical ordering to occur by chance, sending a signal that authors may have contributed equally despite it not being the case (Zuckerman 1968).\n\n\n7.3.3.2 Decreasing order of contribution\nAuthors can also be listed based on the importance of their contribution, the first authors having typically contributed the most. This is a dominant mode of name ordering in Social Sciences and Humanities where writing is key (Pontille 2004). One limitation of this approach is that there is only one author at any position in the list. This issue is sometimes addressed by adding a note in the byline indicating that some authors contributed equally to the work (Hu 2009). According to Zuckerman (1967), prominent researchers sometimes let their less well-known collaborators take first authorship, even if they may have contributed less.\n\n\n7.3.3.3 From the outside in\nA variant of the decreasing order of contribution exists predominantly some fields of natural or biomedical sciences where research often occurs in laboratories. The difference is that the two ends of the byline (the first and last author positions) are the most important. The first author tends to be a junior researcher (a PhD student or postdoc) who lead the experiment. The last author is the lab director. Other contributors are listed in decreasing order of contribution between these two poles, those who contributed the least being in the middle of the byline (Pontille 2006).\n\n\n7.3.3.4 Hybrid (partial alphabetical order)\nZuckerman (1968) described another type of ordering that I call partial alphabetical order, in which a subset of authors are listed alphabetically to highlight the contribution of the other authors who are not part of the alphabetical list. This phenomenon was studied by Mongeon et al. (2017) who showed how alphabetical order is used in biomedical research to distinguish primary authors (at the beginning of the byline), the supervisory authors (at the end of the byline), and the others (in the middle of the byline, in alphabetical order).\n\n\n\n7.3.4 Categorizing authors\nSome scholars have proposed to divide authors in categories. Perneger et al. (2017) identified three types of authors in biomedical research:\n\nThe thinker who designs the work, acquires funding for it, and revises the article.\nThe soldier who provides material, administrative or technical support or participates in the data collection.\nThe scribe who analyzes and interprets the results and drafts the article.\n\nSimilarly, Baerlocher et al. (2007) proposed the following categories:\n\nPrimary authors participate in the planning and execution of the study, and in drafting the article. They are able to explain all the findings and share responsibility for the exactitude of the information reported in the article. They generally appear at the beginning of the byline.\nSenior/supervisory authors plan and supervise the study, and substantially revise the draft. Like primary authors, they can explain all the findings in the study and share responsibility for the exactitude of the information reported in the article. They are generally at the end of the byline.\nContributing authors make a substantial contribution to the work but do not meet the criteria or primary and senior/supervisory authorship. They are not necessarily able to explain all of the findings of the study and they are not responsible for the exactitude of the information reported in the article. They generally appear in the middle of the byline.\n\n\n\n7.3.5 Authorship guidelines\nScholarly societies and journal editors have developed authorship guidelines in response to the lack of standards in authorship practices (Osborne and Holland 2009), the increased number of authors on articles, and the increase in cases of scientific misconduct (Pontille 2016; Steen 2010) bringing forth the relationship between authorship and responsibility (Rennie and Flanagin 1994). Large research teams are most frequent in Natural Sciences, Engineering, and Health Sciences (Wuchty, Jones, and Uzzi 2007), and misconduct is most frequent in the Health Sciences (Fang, Steen, and Casadevall 2012). It is thus not surprising that these are research areas where authorship guidelines are frequently found (Bošnjak and Marušić 2012; Wager 2007).\nJournals rarely create their own authorship guidelines. Instead, they refer to the code of ethics of professional associations or publishers’ guidelines, which tend to refer to the recommendations of the International Committee of Medical Journal Editors (ICMJE). The ICMJE recommendations include four criteria that must be met by all authors of a paper:\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work;\nDrafting the work or revising it critically for important intellectual content;\nFinal approval of the version to be published;\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\nEven though their adoption is not generalized even in the Medical field (Bošnjak and Marušić 2012) these recommendation have a wide influence beyond this field. For instance, the Committee on Publication Ethics (COPE), the European Association of Science Editors (EASE), and major publishers like Elsevier, Springer-Nature, and Wiley-Blackwell refer to it.\nDespite the existence of these guidelines, there is no consensus on what constitutes a substantial contribution that should lead to authorship (Claxton 2005). Even when they do mention the type of tasks that matter for authorship, guidelines remain vague regarding the magnitude of the contribution required, simply stating that it should be substantial. Editors themselves are not convinced of the efficacy of the guidelines for reducing practices like guest and ghost authorship (Wager 2009). In sum, while guidelines might be able to help research teams with their authorship decisions, they seem to have a limited effect on these practices, and to be unable to address unethical authorship issues.\n\n\n7.3.6 Contribution statements\nRennie, Yank, and Emanuel (1997) proposed a model that would replace authors with a list of contributors and guarantors. In their model, contributors include everyone who contributed, no matter the nature and extent of their contribution, and guarantors are those contributors who are responsible for the integrity of the work as a whole. In addition, the article should include a description of the work done by each contributor. The model is recommended (but not required) by the ICMJE guidelines and the Council of Science Editors, and it is applied (although partially) by an increasingly large number of journals requiring that the respective contributions of the authors be indicated, typically in a contribution statement.\nHowever, the informative value of these statements remains limited. Firstly, only the type (and not the extent) of contributions is indicated. According to Ilakovac et al. (2006), contribution statements are also imprecise as they rely on the limited memory (and limited motivation) of researchers who are asked to produce them. A third of the respondants to a survey indicated that contribution statements provide are not more informative than the order in which the authors are listed (Sauermann and Haeussler 2017).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measuring research output</span>"
    ]
  },
  {
    "objectID": "ch7.html#bibliometric-indicators",
    "href": "ch7.html#bibliometric-indicators",
    "title": "7  Measuring research output",
    "section": "7.4 Bibliometric indicators",
    "text": "7.4 Bibliometric indicators\n\n7.4.1 Research output\nThere are two basic ways of counting the research contributions of an individual researcher or another entity (institution, country, etc.): one that does not take into account collaboration (full counting), and one that does take into account collaboration (fractional counting, harmonic counting, geometric counting, and arithmetic counting.\n\nMethods of counting publications in bibliometrics\n\n\n\n\n\n\nIndicator\nShare of credit for the author in the ith position in a byline of N authors\n\n\n\n\nFull counting\n1\n\n\nFractional counting\n\\[                                                                                                                                                            \n                                                                                                                                                           \\dfrac{1}{N}                                         \n                                                                                                                                                                                                        \\]\n\n\nHarmonic counting\n\\[                                                                                                                                                            \n                                                                                                                                       \\dfrac{\\dfrac{1}{i}}{1+\\dfrac{1}{2}+...+\\dfrac{1}{N}}                    \n                                                                                                                                                                                                        \\]\n\n\nGeometric counting\n\\[                                                                                                                                                            \n                                                                                                                                                      \\dfrac{2^{N-i}}{2^N-1}                                    \n                                                                                                                                                                                                        \\]\n\n\nArithmetic counting\n\\[                                                                                                                                                            \n                                                                                                                                                     \\dfrac{N+1-i}{1+2+...+N}                                   \n                                                                                                                                                                                                        \\]\n\n\n\nA study by Hagen (2010) suggests that harmonic counting best fits empirical data on credit allocation between co-authors in chemistry, medicine and chemistry. However, full and fractional counting remain the most widely used indicators.\n\n\n7.4.2 Collaboration\nBibliometric indicators can also be used to measure and characterize collaboration at the article level. The table below summarizes commonly used indicators of collaboration.\n\nBibliometric indicators of research collaboration\n\n\n\n\n\n\n\nIndicator\nVariable type\nDescription\n\n\n\n\nCollaboration\nCategorical\nThere is more than one author on the byline.\n\n\nInterinstitutional collaboration\nCategorical\nThere is more than one institution on the byline.\n\n\nInternational collaboration\nCategorical\nThere is more than one country on the byline.\n\n\nIntersectoral collaboration\nCategorical\nThere are institutions from more than one sector (e.g., university, industry, government) on the byline.\n\n\nTeam size\nNumerical\nThe number of authors on the byline.\n\n\n\nNote that collaboration can also be represented as a co-authorship network, in which two entities are linked when they appear together on the same publication.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measuring research output</span>"
    ]
  },
  {
    "objectID": "ch8.html#learning-objectives",
    "href": "ch8.html#learning-objectives",
    "title": "8  Measuring research impact",
    "section": "",
    "text": "What is research impact?\nWhat are the most commonly used citation-based impact indicators?\nWhat are the limitations of citation-based impact indicators?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measuring research impact</span>"
    ]
  },
  {
    "objectID": "ch8.html#what-is-research-impact",
    "href": "ch8.html#what-is-research-impact",
    "title": "8  Measuring research impact",
    "section": "8.2 What is research impact",
    "text": "8.2 What is research impact\nIn the previous chapter, we focused on the scholarly publication as a measure of the output or productivity of an individual or group. The research article can then be considered as a knowledge unit, to which the authors of that unit have attached their name. However, for a multitude of reasons, not all publications are created equal. Some may be highly original and significant advances to knowledge in a field or across fields (a single publication may even revolutionize fields) while other may make a smaller contribution. So, while research output may be thought of as the number of units produced, research impact can be thought of as the difference that these units have made, their influence. So how can we measure that?\nWe have seen in Chapter 2 that being cited by peers is the most basic form of recognition that researchers receive within the reward system of science (Cole and Cole 1973). Citations have been shown to strongly correlate with prestigious awards and other institutionalized forms of recognition and can thus reflect the amount of scientific capital (Bourdieu 1975) that one possesses. As Merton (1968) put it:\n\n“The reference serves both instrumental and symbolic functions in the transmission and enlargement of knowledge. Instrumentally, it tells us of work we may not have known before, some of which may hold further interest for us; symbolically, it registers in the enduring archives the intellectual property of the acknowledged source by providing a pellet of peer recognition of the knowledge claim, accepted or expressly rejected, that was made in that source” (p. 622)\n\nCitations thus have gained acceptance as a measure of scientific excellence and as a tool for research evaluation (Narin 1976).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measuring research impact</span>"
    ]
  },
  {
    "objectID": "ch8.html#citation-based-impact-measures",
    "href": "ch8.html#citation-based-impact-measures",
    "title": "8  Measuring research impact",
    "section": "8.3 Citation-based impact measures",
    "text": "8.3 Citation-based impact measures\n\n8.3.1 Citation count\nThe citation count is the most basic (and perhaps most used) indicator of academic impact. It is obtained by counting the number of times that the paper appears in the reference lists of other papers. Its premise is simple the more a publication is cited, the more it has been influential. While the citation count cannot reveal what the nature of this influence might have been, it is considered to indicate, to some degree, the amount of that influence.\nOnce we obtain the citation count of every article in a set representing a research unit (e.g., a researcher, a journal, an institution), we can then compute the total citations for the set. For example, my Google Scholar profile shows the total number of citations of all my publications combined.\n\n\n8.3.2 Normalized citation count\nIs 37 a high number of citations for a publication? It depends on several factors, mainly the research field, the date of publication, and the type of document.\nDifferent fields or area of research have different epistemic cultures (Knorr Cetina 1991) and scholarly communication practices that can determine the potential number of citations that a publication can receive. This can limit the validity of an assessment based on citation counts alone, especially when this assessment is performed by someone with little knowledge of the field and what might be an excellent, or above-average citation count. Field-normalized indicators take into account field differences in citation rates by comparing the number of citations a paper has received to the citation counts of other papers in the same field.\n\n\n\n\n\n\nPause and reflect\n\n\n\nIn Chapter 5, we discussed the classification of research and research outputs and showed that there exists many different classification schemes, and that it can be challenging to determine which field (or fields) a paper belongs to. Consider the implications of that for research evaluation using field-normalized indicators.\n\n\nTime is an important factor as well since the potential citation count of an article published in 2022 is obviously much lower than the potential citation count of an article published in the same field in 2005.\nEditorials, or letters to the editors tend to receive fewer citations than research articles, which tend to receive fewer citations than review articles. Conference proceedings are also often less cited than journal publications. Therefore, it is generally good practice to take into account the document type when normalizing indicators.\nNormalized citation counts are usually calculated by simply dividing the citation count of a publication by the average citation count of all publication of the same type, in the same field, and published in the same year.\n\n\n\n\n\n\n\n\n\n\n\n\nArticle\nYear\nField\nType\nCitations\nMean for the same field, year and type\nNormalized citations\n\n\n\n\nA\n2017\nInformation science\nArticle\n84\n72\n1.167\n\n\nB\n2017\nInformation science\nReview\n75\n79\n0.950\n\n\nC\n2017\nComputer Science\nArticle\n95\n120\n0.792\n\n\nD\n2018\nInformation Science\nArticle\n50\n50\n1.000\n\n\nE\n2019\nInformation Science\nArticle\n40\n25\n1.600\n\n\n\n\n8.3.2.1 Challenges with normalization\nNormalizing indicators is not so easy. It implies that we have an adequate classification of research outputs that does not systematically disadvantage subgroups of publications. For example, classifying information science publications with computer science publications may be problematic because these are different fields with different scholarly communication practices. It also implies that we have consistent metadata on the document type, which is not always the case. Finally, and perhaps most importantly, the calculation of the denominator in the normalization formula requires that we have access to the citation counts of all articles published in the same year, field, and document type (essentially, the entire database is required to calculate the denominators). This is problematic in two ways: 1) access to the entire databases is costly (and thus rare), and 2) the normalized citation scores will depend on the database used (Web of Science, Scopus, Dimensions, OpenAlex, PubMed, etc.).\n\n\n\n8.3.3 Highly cited publications (HCP)\nAnother popular indicator is highly cited publications (HCP). This is a rank-based indicator that is obtained by ranking a set of publications from the same field and year (again, normalization is important), and then setting a threshold to distinguish HCPs from the rest. The 1st, 5th and 10th percentiles are often used as thresholds, but these choices are always arbitrary. Using this approach, we get a dichotomous variable (0 or 1) indicating whether each paper in a set belongs to the HCP group or not, which allows us to calculate the share of publications within the set that are highly cited as a size-independent indicator of impact.\n\n\n\n\n\n\nImportant note\n\n\n\nJust like the normalized citations, the identification of HCPs should also take into account the field, publication year and document type. The process thus suffers from the same challenges as citation normalization.\n\n\nThe table below provides an example of what the data could look like. We can see that article A and B are published in the same year and field, but only one is above the HCP threshold and thus considered an HCP. We also see that because the HCP threshold is higher in Computer Science, the 95 citations received by article C are not sufficient to make that paper an HCP. We also see that the threshold varies by year, so paper D and E both have 50 citations but only paper E is an HCP.\n\n\n\n\n\n\n\n\n\n\n\nArticle\nYear\nField\nCitation count\nHCP threshold\nHCP\n\n\n\n\nA\n2017\nInformation science\n84\n80\n1\n\n\nB\n2017\nInformation science\n75\n80\n0\n\n\nC\n2017\nComputer Science\n95\n97\n0\n\n\nD\n2018\nInformation Science\n50\n67\n0\n\n\nE\n2019\nInformation Science\n50\n45\n1\n\n\n\nOnce we have determined whether or not each paper meets the HCP threshold, we can calculate the share of HCPs for the set, which in this case would be 40%.\n\n\n8.3.4 H-index\nIn 2005, a physicist named Jorge Hirsch introduced a composite indicator that combines the output and impact dimensions of research performance into a single number: the h-index.\nThe h-index is equal to the number of publications with a citation number greater than or equal to h For instance, a researcher (or another unit) has an h-index of 10 if they published at least 10 articles cited at least 10 times.\n\n\n\nVisual representation of the calculation of the h-index\n\n\nAs we can see in the conclusion of Hirsch’s paper, he was arguing for the use of his indicator as an unbiased measure of scientific achievement that can be used to compare researchers competing for resources.\n\nIn summary, I have proposed an easily computable index, h, which gives an estimate of the importance, significance, and broad impact of a scientist’s cumulative research contributions. I suggest that this index may provide a useful yardstick with which to compare, in an unbiased way, different individuals competing for the same resource when an important evaluation criterion is scientific achievement. (Hirsch 2005)\n\nThe h-Index (along with its many variations) has been widely criticized (this blog post covers some of those criticisms) for not accounting for author position in the byline and for field differences. It also undervalues highly influential work since it is bounded by the number of publications, and it is correlated with academic age and with the much simpler total number of citations. Even Hirsch himself recognized the limits of his indicator and the potential adverse effects of its popularity.\nLudo Waltman and Nees Jan van Eck (Waltman and Eck 2011) argued that the h-index can create inconsistencies between single authors considered individually or as a group. For instance, five authors who co-authored the same five papers each cited five teams will have an h-index of 5 when considered individually or as a research unit. However, 5 authors who separately published two papers with 10 citations each will have individual h-indices of 2 and a collective h-index of 10. Waltman and van Eck Waltman and Eck (2011) argue that the share of HCPs is a better indicator since it doesn’t suffer from this limitation: researchers who perform better individually than others will maintain this advantage once aggregated into a unit.\n\n\n8.3.5 The journal impact factor\nThe Journal Impact Factor is a citation-based indicator designed to evaluate the relative influence of a journal. It is quite simply the average number of citations received during a given year by the articles published in the journal over the two previous years:\n\\[                                                                                                       \n                                                                                                     JIF_y = \\dfrac{Citations_y}{Publications_{y-1} + Publications_{y-2}}                                 \n                                                                                                                                               \\]\nSo the 2017 JIF for the journal X would be calculated by counting all citations received in 2017 by the articles published in the journal in 2015 and 2016 and dividing this citation count by the number of articles published in the journal for 2015 and 2016. Because citations take llonger to accumulate in the social sciences and humanities, a variant of the JIF that uses a five-year citation window rather than a two-year one was eventually introduced.\n\n\n\n\n\n\nOne of many\n\n\n\nThe term Journal Impact Factor is a trademark of Clarivate Analytics (the company that owns the Web of Science). Other similar indicators have been proposed such as the CiteScore, and the Source Normalized Impact per Paper (SNIP) and the Scimago Journal Ranking (SJR) used by Elsevier.\n\n\nAs we know, accumulating citations take time, which means that it is difficult to determine early on whether a piece of research is a “significant contribution” to science or not. Partly because they are immediately available, the JIF or other journal indicators or rankings are commonly used in the evaluation process. Such practices have, however, been widely criticized and using the journal as a proxy for the importance of a single publication is often considered a misuse of these journal indicators (some of these criticisms can be found in Larivière and Sugimoto (2019)).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measuring research impact</span>"
    ]
  },
  {
    "objectID": "ch8.html#limits-of-citation-based-indicators",
    "href": "ch8.html#limits-of-citation-based-indicators",
    "title": "8  Measuring research impact",
    "section": "8.4 Limits of citation-based indicators",
    "text": "8.4 Limits of citation-based indicators\n\n8.4.1 Diversity in citation practices\nAn important factor that is not always at the forefront of discussions around citation-based indicators and research evaluation is the fact that a paper doesn’t just get cited by virtue of existing or based on its intrinsic characteristics; it is cited because another researcher referred to it in their own work. We might then ask: why do researchers cite other works?\nThere are two dominant theories of citation behaviour: the normative and the socio-constructivist theories. The normative view suggests that researchers mainly cite in order to acknowledge the work of their peers and predecessors and give credit where credit is due. It is the Mertonian view that was mentioned at the beginning of this chapter which supports the use of citations for evaluative purposes. The socio-constructivist view focuses on citations as a rhetorical device that is not meant to acknowledge, but rather to convince. It emphasizes the strategic and biased nature of citation choices. For instance, one might omit to cite sources that don’t align with their findings or arguments, or cite work of little relevance by prestigious scholars at the expense of more relevant work by less known scholars.\nBornmann and Daniel (2008) performed a review of studies that sought to empirically test those two theories. Their review suggests that reasons for citing are mixed and partly support both views, although support for the normative theory is stronger. They do however suggest that this is mostly true at higher level of aggregation (e.g., researchers with substantial publication records, research units, institutions).\nThis leads us to a second main limit of citation-based impact indicators: their limited reliability when working with small datasets (e.g., individual papers or researchers). This is partly due to the ambiguities surrounding the concept of impact and what citations are actually supposed to measure, and by the general concept of statistical power according to which small sample sizes increase the likelihood of failing to reject a null hypothesis.\nFinally, one of the most important limits of citation-based indicators is that they fail to account for other forms of impact. Indeed, citations are often seen to measure relatively accurately the academic impact of a publication or research unit, that is the contribution of theoretical or methodological advances to the field. However, the impact of academic research is not limited to the scientific realm. Research can also lead to economic development (economic impact), fuel technological innovation (technological impact), and have impact on policy, on public health, on the environment and on culture and society at large. In the next chapter, we will discuss “Altmetrics” (alternative metrics), which are a range of bibliometric indicators that seek to measure the impact of a publication outside of scholarly communication system. Could those new metrics succeed in measuring the social impact of research? We shall see.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measuring research impact</span>"
    ]
  },
  {
    "objectID": "ch8.html#references",
    "href": "ch8.html#references",
    "title": "8  Measuring research impact",
    "section": "8.6 References",
    "text": "8.6 References\n\n\n\n\nBornmann, Lutz, and Hans-Dieter Daniel. 2008. “What Do Citation Counts Measure? A Review of Studies on Citing Behavior.” Journal of Documentation 64 (1): 45–80. https://doi.org/10.1108/00220410810844150.\n\n\nBourdieu, Pierre. 1975. “The Specificity of the Scientific Field and the Social Conditions of the Progress of Reason.” Social Science Information 14 (6): 19–47. https://doi.org/10.1177/053901847501400602.\n\n\nCole, Jonathan R, and Stephen Cole. 1973. Social Stratification in Science. Chicago, IL: University of Chicago Press.\n\n\nHirsch, J E. 2005. “An Index to Quantify an Individual’s Scientific Research Output.” Proceedings of the National Academy of Sciences of the United States of America 102 (46): 16569–72. https://doi.org/10.1073/pnas.0507655102.\n\n\nKnorr Cetina, Karin. 1991. “Epistemic Cultures: Forms of Reason in Science.” History of Political Economy 23 (1): 105–22. https://doi.org/10.1215/00182702-23-1-105.\n\n\nLarivière, Vincent, and Cassidy R. Sugimoto. 2019. “The Journal Impact Factor: A Brief History, Critique, and Discussion of Adverse Effects.” In, 3–24. Springer International Publishing. https://doi.org/10.1007/978-3-030-02511-3_1.\n\n\nMerton, Robert K. 1968. Social Theory and Social Structure. New York: Free Press.\n\n\nNarin, Francis. 1976. Evaluative Bibliometrics: The Use of Publication and Citation Analysis in the Evaluation of Scientific Activity. Computer Horizons Washington, D. C.\n\n\nWaltman, Ludo, and Nees Jan van Eck. 2011. “The Inconsistency of the h-Index.” Journal of the American Society for Information Science and Technology 63 (2): 406–15. https://doi.org/10.1002/asi.21678.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measuring research impact</span>"
    ]
  },
  {
    "objectID": "ch12.html#what-is-research-fraud",
    "href": "ch12.html#what-is-research-fraud",
    "title": "12  Scientific fraud and questionable research practices",
    "section": "",
    "text": "[The] fabrication, falsification, plagiarism or other practices that seriously deviate from those that are commonly accepted within the scientific community for proposing, conducting, or reporting research. It does not include honest error or honest differences in interpretation or judgments of data (US Public Health Service 1989).\n\n\n\n12.1.1 Fabrication\nData fabrication is probably the most serious type of scientific fraud and occurs when researchers makes up all or some of their data from scratch without having carried out the alleged experiments. Data fabrication can take many forms.\nA widely publicized example of fabrication is that of Diederik Stapel, a former Dutch social psychology researcher. Before being found guilty of scientific fraud, Stapel was a star researcher in his field, having, among other things, founded the Tilburg Institute for Behavioral Economics Research in 2007, obtained the Career Trajectory Award from the Society of Experimental Social Psychology in 2009, and being appointed rector of his faculty in 2010. However, in 2011, after some of the students under his supervision expressed concern about the validity of some of his data, an investigation revealed that for more than a decade Stapel had invented the data for most of his research. More than 50 articles signed by Stapel and published in the most important journals in the field have been identified as fraudulent and subsequently retracted from the scientific literature.\nAnother well-known case, this time in physics, is that of Hendrik Schön, who fabricated data for more than four years (1997-2002) while he was a postdoctoral fellow at Bell Laboratories. He was able to fool the scientific community with his discovery of a method to manufacture plastic superconductors that would revolutionize the world of nanotechnology. Schön’s articles were published in major journals like Science and Nature. Some researchers began to question Schön’s results when several attempts to replicate Schön’s findings were unsuccessful. However, it was Schön’s use of the same fabricated figure in two different papers that ultimately uncovered the fraud (Reich 2009).\n\n\n12.1.2 Falsification\nBabbage (1830) described two types of falsification. The first one, cooking, consists in the cherry-picking of data in order to achieve a desired finding. The second, which Babbage called trimming, consists in eliminating data points that deviate from the average and artificially increase the precision and homogeneity of the data. Larivée and Baruffaldi (1993) argue that falsification is not only about cooking and trimming data but can also include the misuse of statistical tests or the altering of equipment or data collection processes. The main difference between falsification and fabrication is that falsification involves real data, whereas fabrication involves data artificially created.\nIsaac Newton, one of the most important figures in the history of science, apparently falsified some of the data described in his masterpiece Philosophiæ Naturalis Principia Mathematica. He improved the precision of some measurements, namely his calculations on the acceleration of gravity, the velocity of sound, and axial precession (Westfall 1973), to make his theory more convincing and accepted by the scientific community at the time Broad (1983).\n\n\n12.1.3 Plagiarism\nPlagiarism, which is said to be the most frequent type of scientific fraud (Merton 1957), can be defined as:\n\n[T]he appropriation of another person’s ideas, processes, results, or words without giving appropriate credit, including those obtained through confidential review of others’ research proposals and manuscripts (US Public Health Service 1989).\n\nThis can take the form of excessive paraphrasing or intentional, unconscious, or self-plagiarism. Intentional plagiarism is the use of someone else’s text in part or in whole without citing the source. Paraphrasing is a perfectly acceptable practice, but it becomes abusive and akin to intentional plagiarism when the words have been changed in order to give the illusion that it is a new text, and when the sources are not cited (Kochen 1987). Self-plagiarism consists of the reuse of one’s own published text. It can be challenging to determine whether or not an instance of self-plagiarism is fraudulent, because the reuse of certain parts of texts may sometimes be acceptable or even preferable (e.g., the description of a very technical and complex methodology) provided that the source is duly cited (S. B. Bird and Sivilotti 2008). The concept of self-plagiarism itself is debated, some arguing that it is impossible to steal one’s own ideas or text, and that the publishing the same results in different articles aimed at different communities is not only acceptable, but desirable from an knowledge dissemination standpoint (S. J. Bird 2002).\nA famous case of plagiarism in medicine is that of Elias Alsabti. In addition to falsely pretending that he came from a Jordanian royal family and that he held a doctorate, he managed to plagiarize several hundred articles. He found these articles in obscure journals, replaced the author’s name with his own (and sometimes that of fictitious co-authors), then submitted this copied article to another, equally obscure, journal. Building up an impressive publication record, he obtained research positions in several universities and hospitals in the United States. Although Alsabti’s fraud was known to many in the institutions where he worked during his career, it took three years before it was made public (Broad 1983; Judson 2004).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Scientific fraud and questionable research practices</span>"
    ]
  },
  {
    "objectID": "ch12.html#consequences-of-research-fraud",
    "href": "ch12.html#consequences-of-research-fraud",
    "title": "12  Scientific fraud and questionable research practices",
    "section": "12.2 Consequences of research fraud",
    "text": "12.2 Consequences of research fraud\nFor the guilty researchers, fraud has informal consequences in the form of a loss of reputation (Larivée and Baruffaldi 1993) and the questioning of all their work by the scientific community (Culliton 1974). There are also formal consequences that vary from case to case. These can include a ban on obtaining funding for a certain number of years, suspension, dismissal, house arrest, community service, fine and imprisonment (Couzin 2006; Karcz and Papadakos 2011; Tilden 2010). Mongeon (2015) found that most researchers found guilty of fraud in the biomedical field did not publish in the 5 years following the retraction of their fraudulent papers. That same study found that fraud can also significantly affect the careers of innocent co-authors, especially junior researchers who, in most cases, stopped publishing and appear to have abandoned their research careers (see the figure below).\n\n\n\nShare of research output by innocent collaborators published in the five years before and after a retracted publication in the biomedical field. (Mongeon 2015)\n\n\nIt is perhaps worth noting that, according to these findings, retracting a publication because of an honest error also appears to significantly affect the careers of junior researchers (who are typically listed as first authors in biomedical research).\nBecause researchers build on existing research to advance knowledge, the publication of false results can lead to considerable waste of time, money and effort for a large number of researchers, not to mention the waste resulting from the fraudulent research itself. Fraud can reduce mutual trust between researchers, potentially hindering the advancement of knowledge by reducing the sharing and use of information, or pushing researchers to systematically reproduce the experiments (Chubin 1985; Weinstein 1981).\nCases of fraud can also negatively affect the discipline in which they occur. According to Azoulay et al. (2015), there would be a decrease in both the research funds granted to researchers in a discipline where a fraud has been discovered, new researchers entering it, and citations received by articles dealing with subjects similar to those of fraudulent articles.\nFunding for research comes largely from public funds, and it important that science be seen as trustworthy by governments, funding agencies and the public. Research fraud, and especially the cases that generate a lot of media coverage, can harm the credibility and legitimacy of science.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Scientific fraud and questionable research practices</span>"
    ]
  },
  {
    "objectID": "ch12.html#detecting-research-fraud",
    "href": "ch12.html#detecting-research-fraud",
    "title": "12  Scientific fraud and questionable research practices",
    "section": "12.3 Detecting research fraud?",
    "text": "12.3 Detecting research fraud?\nGiven the significant consequences that research fraud can have, it is important for the scientific community be able to detect it. It is sometimes taken for granted that science is self-correcting and has mechanisms that will ensure that sooner or later errors (and frauds) will be discovered. However, the surge of research fraud observed over the last decades raises questions as to the effectiveness of these mechanisms.\nOne of these mechanisms is the evaluation of research manuscripts by several experts before they get published, also known as peer-review. In theory, peer-review ensures that only relevant, reliable, and rigorous research will make it into the scientific literature. However, peer review is often not effective in detecting errors and fraud. Researchers who are part of the elite will not be judged as harshly as other less prominent researchers (Merton 1968). A researcher who is prominent or affiliated to a prestigious organization may manage to publish fabricated or falsified data since the reviewers may not dare question their work. The case of John Long is a good example. From 1970 until the early 1980s, he published the results of his work on tumor cells from Hodgkin’s disease that he supposedly managed to grow in a test tube. Since he worked at the prestigious Massachusetts General Hospital under the supervision of Paul Zamecnik, a well-known researcher and member of the National Academy of Sciences, the peer review system took certain aspects of his work for granted (Broad 1983).\nThe peer review system is also quite ineffective in detecting plagiarism, and even more so in the pre-digital era. This allowed fraudsters like Alsabti to take advantage of this flaw. While it is now much easier to detect cases of plagiarism by using, for instance, plagiarism detection software, plagiarism remains relatively frequent (Grieneisen and Zhang 2012).\nReplication of experiments by other researchers is the most effective mechanism for validating findings and detecting errors and fraud. The replication of (or the possibility to replicate) findings is made possible by the fact that research publications include a detailed description of the research data and process (a.k.a., the methods). In practice, the reproduction of experiments is relatively rare. One of the reasons for this is inaccurate or incomplete description of the methods, or inaccessibility of the required data or equipment. Even when replication would in theory be possible, there is a lack of resources and incentives for replication studies which are difficult to get funded and published due to the emphasis on originality in the reward system of science. Rather, the research will be validated indirectly by other researchers who replicate or build on results to improve them (not to verify them).\nWhistle-blowing is often involved in the process of uncovering research fraud. For instance, Diederik Stapel’s case was uncovered by some of his students who grew suspicious when they asked to see the raw data for an experiment they were involved in and Stapel said he didn’t have it anymore. One of the whistleblowers thought Stapel’s results were too good to be true, and their suspicions were validated when they found two rows of data that appeared to have been copy-pasted.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Scientific fraud and questionable research practices</span>"
    ]
  },
  {
    "objectID": "ch12.html#prevalence-of-research-fraud",
    "href": "ch12.html#prevalence-of-research-fraud",
    "title": "12  Scientific fraud and questionable research practices",
    "section": "12.4 Prevalence of research fraud",
    "text": "12.4 Prevalence of research fraud\nJust how much fabricated, falsified, or plagiarized research is there? Given that fraud is by nature something that its author seeks to hide and hope that it will never be discovered, and given that research fraud is not easy to detect, it is plausible that the cases that are discovered are really just the tip of the iceberg. Some attempted to estimate the prevalence of fraud using surveys. This approach is expected to underestimate the prevalence of research fraud since researchers are probably not very inclined to admit their fraud. Nevertheless, in a meta-analysis of these surveys Fanelli (2009), reported that 2% of researchers admit to having distorted data themselves, while 14% claim to know at least one colleague who has done so.\nRetractions are one of the ways by which the scholarly community treats fraud when it is discovered. Retractions, in principle, clean up the scientific record by flagging invalid or untrustworthy research. The census of retracted articles thus makes it possible to estimate the prevalence of cases of fraud, but only on condition that the cases in question meet four conditions:\n\nThe fraudulent research has been published.\nThe fraud was detected after publication.\nThe fraudulent article was retracted by the journal.\nA notice of retraction has been circulated and is easily identifiable, and ideally the article itself has been identified as retracted in the databases.\n\nThus, retractions also only provide an underestimate of the prevalence of scientific fraud since obviously not all cases of scientific fraud or error will meet these four conditions.\nAccording to the Retraction Watch blog and its searchable database, more than 40,000 articles have been retracted as of November 13th, 2022. However, not all of these retractions are due to fraud. There are many reasons why articles get retracted, including honest errors and a range of issues that undermine the trustworthiness of the studies. Also, it is important to keep in mind that there are millions of studies published every year, so despite the increased number of retractions in recent decades, retractions remain extremely rare. Less rare are questionable research practices, which we discuss in the next section.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Scientific fraud and questionable research practices</span>"
    ]
  },
  {
    "objectID": "ch12.html#questionable-research-practices",
    "href": "ch12.html#questionable-research-practices",
    "title": "12  Scientific fraud and questionable research practices",
    "section": "12.5 Questionable research practices",
    "text": "12.5 Questionable research practices\nQuestionable research practices (QRPs) exists in the grey zone between the responsible conduct of research (RCR) and fabrication, falsification, and plagiarism (FFP).\n\n\n\nResearch behaviours continuum\n\n\nQRPs encompass a whole range of behaviours and practices that drift away from the ideal of the responsible conduct of research. They can occur at different stages of the research process and they are sometimes considered to not only apply to the individuals performing the research but also to other actors such as editors and reviewers. They also vary in their perceived prevalence and severity and these perceptions also vary between fields. Here are examples of QRPs:\n\nImproper referencing: Attributing an idea or concept to the wrong source, failing to cite sources, or abusive self-citations. This include citing only studies that are in agreement with one’s findings.\nSelective reporting: Reporting only some of the data and results of a study (typically only the positive results). Failing to provide adequate level of details in the different parts of the manuscript. Publishing only positive results (publication bias).\nSalami slicing: Splitting what could and should have been a single work into multiple smaller publications, sometimes called “smallest publishable units”.\nNot sharing data: While sharing data is not always possible for ethical or legal reason. Sharing data is increasingly expect by journals and other researchers and not doing so is often considered a QRP.\nP-hacking: Performing all kinds of test and retaining those that produce significant results.\nHarking: Coming up with the hypothesis after looking at the results and presenting it as an a priori hypothesis.\nAuthorship misattribution: failing to give credit where credit is due by excluding significant contributors from the byline, or giving credit where it is not due by including co-authors that did not significantly contribute to the work. Not ordering authors in a way that reflects their respective contribution to the work.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Scientific fraud and questionable research practices</span>"
    ]
  },
  {
    "objectID": "ch13.html",
    "href": "ch13.html",
    "title": "13  Visualizing geographical data",
    "section": "",
    "text": "Coming soon…",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Visualizing geographical data</span>"
    ]
  },
  {
    "objectID": "ch13.html#introduction",
    "href": "ch13.html#introduction",
    "title": "13  Equity, diversity, and inclusion in research",
    "section": "",
    "text": "Diversity: the numerical representation of groups of individuals based on their primary and secondary characteristics and identities.\nEquity: the treatment of individuals in terms of access, opportunity, and advancement.\nInclusion: the ability to meaningfully participate and contribute, both for the benefit of the individual and the organization.\nRacism: the devaluation and the denial of rights, dignity, and value of individuals due to their race or geographical origin.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Equity, diversity, and inclusion in research</span>"
    ]
  },
  {
    "objectID": "ch13.html#the-social-stratification-of-science-revisited",
    "href": "ch13.html#the-social-stratification-of-science-revisited",
    "title": "13  Equity, diversity, and inclusion in research",
    "section": "13.2 The social stratification of science (revisited)",
    "text": "13.2 The social stratification of science (revisited)\nMerton wrote of citations as the scientific community’s recognition and signification of knowledge and its source that has been accepted or rejected by the community (as a citation is agnostic whether you cited it to confirm or reject their results). In his psychosociological analysis of science as a social institution (this is a sociological perspective), Merton (1968) finds correlations in the self-assurance in highly acknowledged researchers that is concurrently inherent, yet also socially constructed and supported, so that pursuit of high-risk problems yields disproportionate recognition when communicated. This process of social selection leads to a “concentration of science resources and talent” (Merton 1968). Merton’s original 1968 article pointed out two aspects of the Matthew effect, one in which a person is given over-recognition for contributions and the other as misattribution of work for which they did not do (Merton 1968). However, the term is used most commonly for the first condition to describe the accumulation effect of ‘the rich get richer’ in which those at the center, who have already been recognized, are attributed more than those at the margins. To put this another way, the prestige of an author can affect citation behaviours and subsequent patterns.\nBourdieu recognized this accumulation in the form of citation amounts as social capital (Bourdieu 1975), which draws those without towards those that have it. Similarly, those that have it are often awarded funding and resources which may support those around them, not only gaining more social capital for themselves in the process but also benefiting those contributing to it. This dynamic process results in communities with centralized figures that are highly recognized and rewarded with other agents surrounding them, not only trying their best to be a central figure but also preventing their movement to the margins. The accumulation, then, is a dynamic driving force in a society, regardless of whether the accumulation is in citations, resources, attention, or otherwise that contributes to their social capital.\nCitations differ from other forms of capital, however, in that while aggregate citation rates may follow a normally distributed curve over the course of an author’s career, the number of citations only continues to grow as a symbolic reward (Cole and Cole 1973). Citations can be thought of as a currency that can only accumulate, and the gaps between those who have a lot of it and those who do not keep getting larger (Kwon 2022).\nPerhaps less central in the scholarship of Merton and Bourdieu is the recognition of the behaviours which result in the oppression, marginalization, or erasure to maintain control of the centre. Margaret Rossiter names the “Matilda effect” (Rossiter 1993) after the suffragist and feminist Matilda Joslyn Gage, who criticized the phenomena in which “women scientists …have been ignored, denied credit, or otherwise dropped from sight” (Rossiter 1993). Rossiter seizes upon Merton’s original definition of the Matthew effect to explore the phenomena of misattribution through purposeful or innocent ignorance of the contributions of women. She also points out the beautiful irony that the book of Matthew was not written until “two or three generations after his death”, further reinforcing her point of misattribution. Rossiter’s key contribution is the criticism of Merton’s functionalist approach to the effect that there was no emancipatory call as a result of discovering this but rather a map for how new scientists could capitalize on this effect. The Matilda effect represents not just disproportionate attributions benefiting those who already accumulated recognition but the purposeful obfuscation or erasure of women in science whose accomplishments were attributed to men.\nThis obfuscation and erasure of information are explained as epistemic injustice by Fricker (2007) as both testimonial and hermeneutical injustices in the information world. Testimonial injustice is a form of social power in which identity is used as an oppressive instrument of power. It occurs when one receives or is denied credibility by another, such as when a listener dismisses what a speaker is saying. Hermeneutical injustice is another form of social power and occurs when the speaker is denied “interpretive resources” to understand the unfair disadvantages that they are experiencing. For example, when someone is denied education on domestic violence, they may lack the ability to interpret their situation as being abused.\nBoth testimonial and hermeneutical injustices exist within LIS (Patin et al. 2020), and without a critical examination of our own contributions, we will continue to unknowingly support systems that contribute to the eradication or devaluation of information. As librarians, we have been part of a history of “privileging certain knowledge systems and ways of knowing over others” (Patin et al. 2020). While academic institutions work with a very small amount of the knowledge that humanity produces, it is a critical part of the scientific contribution to society. As such, moving beyond individual awareness to collective action against epistemic injustices in our information systems is important for LIS to address all injustices, even the accidental omissions that have occurred historically. Within the context of scholarly communication, citations are one of the main ways we can see evidence of biased behaviours and injustices from the scientific community.\nIn contrast to the observed phenomena from Merton and Bourdieu, Sara Ahmed reframes, from a feminist lens, citations as reproductive technology or “a way of reproducing the world around certain bodies” (Ahmed 2013). “Who appears, who does not appear” (Ahmed 2013) is both a conscious and unconscious choice supported by assumptions and beliefs. Are we inserting ourselves into a viewpoint/perspective, or are we establishing our own perspective or that of our community? With Ahmed’s perspective in mind, perhaps there is an emancipatory approach to citations.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Equity, diversity, and inclusion in research</span>"
    ]
  },
  {
    "objectID": "ch13.html#measuring-biasdisparities-in-research",
    "href": "ch13.html#measuring-biasdisparities-in-research",
    "title": "13  Equity, diversity, and inclusion in research",
    "section": "13.3 Measuring bias/disparities in research",
    "text": "13.3 Measuring bias/disparities in research\nFrom a sociological perspective, which acknowledges and seeks to understand the cultural and structural context, biases can manifest in implicit or explicit ways that have long-term ramifications on perceptions of “confidence, capability, trustworthiness” among others (Scherer 2021). On an individual level, biases are learned behaviours and associations that happen quickly, and over time, unconsciously. Biases are also complex with two levels or layers, with the first based upon observable qualities or traits and the other associations or connections with behaviours that are then compared with those of the observer. Contextual conditions, such as beliefs, can add validation and reinforce biased associations and permit assumptions. The goal of diversity science is to bring awareness of those biases to challenge the assumptions so that acknowledgement of the “disparities in resources and opportunities across groups” can be addressed (Scherer 2021).\nStudies examining gender bias in scholarly communication utilize algorithms to categorize names within gender categories (typically binary) based on geographic and cultural inferences. NamSor, genderize.io, GenderAPI, and Wiki-Gendersort, are the main ones found in bibliometric studies investigating gender bias. The algorithms work by harvesting names from openly available databases and also collect other data such as the country of origin and the family name and language as cultural context identifiers. All names are assigned a gender with a certain probability calculated by the algorithms. These algorithms have some benefits: they are cheap, effective, and can be applied retrospectively to datasets. But they also have limitations, such as the fact that they rely on name-gender databases that may not include self-identification. Moreover, gender probabilities based on names and locations are obviously not perfect and may fail to attribute the right gender in some cases. That said, their accuracy remains acceptable at the aggregate level. While this still presents data along binary categories of gender (not to mention the common conflation of sex and gender as identity), the algorithms are often used to address and dismantle the historical and current oppression rather so that rejecting their use would deprive us from valuable knowledge around gender biases and disparities that exist at a large-scale. Here are several kinds of gender biases or disparities that have been observed in bibliometric studies.\n\nA citation disparity is observed by simply comparing citations indicator between groups (Traag and Waltman 2022). Studies have shown that works by women tend to get less cited than work by men (Larivière et al. 2013) and that women represent only 14% of the highly cited researchers (the group of researchers who publish highly cited publications) in the Web of Science (Meho 2022).\nA citation bias is observed when there is a causal relationship between a variable (e.g., gender) and the act of citing a paper (Traag and Waltman 2022). Causation is however difficult to demonstrate in bibliometrics because experiments are extremely rare in the field, and most studies are correlational. Because correlation does not imply causation, it is very difficult to demonstrate a bias (defined as a causal relationship) using bibliometric methods.\nCitation homophily is observed when members of one group tend to cite members of the same group more than researchers from other groups. Ghiasi et al. (2018) found that citation homophily occurs in all fields of science but that it is stronger in the Social Sciences and Humanities.\n\nWhile the examples above refer to citation disparities, biases, and homophily, the same situations or mechanisms can be observed for other indicators such as research outputs, collaboration, funding, awards, hiring, promotions, etc. Understanding how biases and discriminatory practices exist in academia is important for closing the gender gap. Furthermore, disparities, biases, and homophily can be observed for other variables than gender:\n\nBiases based on ethnicity or race impose disadvantages on persons based on their perceived identity. Ethnical biases can include race, ethnicity, and nationality. Secondary associations with race, ethnicity, nationality, and their intersections can further create or maintain harmful stereotypes when authors’ works are perceived as less than those of another group.\nBiases in the perceived value of works from certain countries or regions. Examples seen previously include ignoring or devaluing works that are from other countries or regions, the assumption that issues in X country are not applicable to one’s own situation, or assumptions of research quality or rigour if the author has institutional affiliations outside of the perceived ‘norm’. The Global North produces far more publications and receives more citations than the Global South, which also produces more local and geographically contextualized work than other geographic regions (Mongeon et al. 2022). Reinforcing this bias of geographic citations is the evaluation of works for quality, with Global North/Western authors possessing the privilege of not citing authors from other regions with any deleterious effect on their perceived quality, whereas non-Global North/Western authors must cite references from the Global North as evidence of their research quality Chakrabarty (2007). Other studies grouped countries by income level and found that research in low to middle-income countries tends to be evaluated less favourably than those in high-income countries (Harris et al. 2017).\nA devaluation or dismissal of work written in languages other than English exists in citation and pee-review (Lee et al. 2013). There are differences in acceptance rates of manuscripts from authors of English-speaking countries and those of non-English-speaking countries, and sometimes language and writing style is given as reasons for rejection when there is no other problem with the manuscript. Databases of Scopus and Web of Science have a disproportionate coverage of English articles, affecting fields such as social sciences and humanities, where there are more books in languages other than English due to their subject matter and regional specificity (Mongeon and Paul-Hus 2015). Compounding this is that US and English-speaking countries dominate web development, particularly academic web development, contributing to even more bias against non-English sources. As such, all indicators, including those that are web-based, are inherently biased toward English documents from database sources, social media outlets, or search tools (Mas-Bleda and Thelwall 2016).\nIn their study of peer-review biases, Lee et al. (2013) point to other forms of bias, including affiliation bias (evaluating more favourably work from prestigious institutions), content bias (favouring specific topics or methodologies), confirmation bias (favouring work that support one’s views), or publication bias (favouring positive results). Double-blind peer review has been found to be an effective mediation of these biases. However, manuscripts contain many identifiable characteristics that can provide a reviewer with enough information to correctly identify an author (Baggs et al. 2008), with highly specialized fields, such as bibliometrics, possibly making it easier.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Equity, diversity, and inclusion in research</span>"
    ]
  },
  {
    "objectID": "ch13.html#how-do-we-do-better",
    "href": "ch13.html#how-do-we-do-better",
    "title": "13  Equity, diversity, and inclusion in research",
    "section": "13.4 How do we do better?",
    "text": "13.4 How do we do better?\nRay et al. (2022) propose citation diversity statements as a reflexive tool to reinforce the commitment to your community of researchers. The following is an example citation diversity statement from Ray et al. (2022):\n\nWe are committed to promoting intellectual and social diversity in science and academic scholarship and took this commitment into consideration while researching and writing this article. We actively worked to promote diversity in our reference list while ensuring all the references cited were relevant and appropriate. We have included some references to enhance diversity but have not omitted any references for this purpose. To assess the diversity of our references, we obtained the predicted gender of the first and last author of each reference by using a database that stores the probability of a first name being carried by a woman (gender-api.com). Using this measure and removing self-citations, our references contain 30% woman(first)/woman(last), 11% man/woman, 15% woman/man, and 44% man/man. This method is limited in that a) names, pronouns, and social media profiles used to construct the database may not, in every case, be indicative of gender identity and b) it cannot account for intersex, non-binary, or transgender people. We look forward to future work that could help us to better understand how to support equitable practices in science.\n\nBecause it is easy to imagine how citation diversity statements could lead to tokenism (diversifying citations artificially for the sole purpose of “looking good”), Ray et al. (2022) insist on the ethical importance of citing works that provide information relevant to a paper, and not simply because of some box on a manuscript submission form that needs to be checked. That said, unconscious biases in citing behaviours may not support the best interests of researchers and their research community. Investment in thoughtful, purposeful citations of works one is engaged with will not only strengthen communities but, when done with an awareness of having diverse voices as a strengthening practice, will also improve the overall quality of scholarly works.\nGiven that disparities exist historically, basing decisions upon results such as these with stereotypes about the quality of all articles in the Global South (also problematic) contributes further to the disparity. From an emancipatory perspective, the path to fixing this is making time to explore, engage with, and understand scholarly production from geographic locations beyond the norm. This not only enriches one’s own writing through a more balanced view but also respects and recognizes advances by researchers.\nDoes the citation technology exist as compatible with “social equity, freedom, and cultural pluralism” or does its existence require centralized control through ownership, market forces, and power concentrations Winner (1980)? On the one hand, there is the rather functional view of the phenomena of social capital in which we see centers of power within scholarly communication and citations as part of the reward system of science, and that by citing, we associate our work with these centralized actors. On the other hand, there is an emancipatory view in which we view citations as a technology that enables us to redistribute and acknowledge those that we have engaged with, recognize, and proliferate ideas that are meaningful to us and our part within a community.\n\n13.4.1 Citational justice\nKumar and Karusala (2021) introduce Iris Marion Young’s faces of oppression as a framework for understanding and addressing citational (in)justice. They define justice as “a relational value of the actions, structures, and institutions in which persons stand to each other as social and political subjects, be they structures of the production and distribution of material goods or of the exercise of political power”,and view the citation as “anti-racist, feminist technologies” (Kumar and Karusala 2021) with the potential to correct the imbalances have occurred. The authors present some examples of ways in which injustices have shown up in their own work and reviews, which may provide an opportunity for self-reflection upon your own citation practices.\n\nExploitation – occurs when the balance of work and compensation is leveraged, creating inequality and power dynamics. This supports the rich-get-richer aspect of Merton’s Matthew effect by leveraging power. The authors identify several types of citation behaviours found in their own work.\n\nThe Cite-Me Cite can occur when submitting papers to journals and the editors pressure the authors to cite their work in return for an acceptance. This is particularly a concern/signal of predatory journal practices.\nThe Name-Agnostic Cite occurs when hard-to-recognize/pronounce/read names are othered, as in “other authors have investigated…”, whereas Western names are clearly cited.\nThe In-the-Global-South and Unrelated-to-the-North Cite falls along similar lines as othering or even making certain work irrelevant. See Linxen et al. (2021) for a study exploring this issue.\nThe Throwaway Cite occurs when citations are lumped together without individual attention or recognition, as in “studies in LIS have examined the effect of unicorns (Name, 1986; Name, 1993; Name, 2000; Name et al., 2002; Name et al., 2013).” While this practice may be an intent to be exhaustive yet concise, who is this benefiting and for what purpose?\nThe No Cite is when references are not made as conscious or unconscious decisions to omit. While addressing this type of non-cite requires greater rigour, not doing so is a privilege that is being assumed.\n\nMarginalization – when a category of persons is excluded and thereby deprived, not only at the individual level but also at the collective level. This is evident in conferences that privilege some populations, such as conferences that hve never been held in the Global South. Some universities dominate some disciplines, which can lead to a misperception of enhanced value, affecting acceptance and possibly citation. Women, scholars of colour, and gender diversity, also exhibit the effect of biases upon their communities, as evidenced by citation gaps.\nPowerlessness – Those in the community that “lack significant power”, a voice, or opportunity to contribute to decision making. This occurs when assumptions are made, creating or reinforcing norms that we expect to be accepted. These assumptions, without critical inquiry, can shape not only our readers but ourselves. This can include the assumptions that work from certain groups lacks rigour, works published at certain venues or in certain journals is not of high quality, Wikipedia is not a valid source of knowledge, papers written in other languages are not relevant, etc.\nCultural imperialism – an interpretation of normal within a society that reflects the dominant society’s cultural values, at once othering other groups within society and reinforcing stereotypes that maintain this power imbalance. In scholarly publications, this is evidenced by a combination of two previous effects othering then uncited. For example, work in the Global South focuses on the poorest communities, focus on novelty or differences within other cultures, the universality of Western ethical standards, and the expectation that English by those outside the Western North is of low quality. Cultural imperialism also occurs when the research of marginalized communities is interpreted within the frameworks of the dominant society for their own use.\nViolence – here, it is important to bring in Young’s words from Kumar and Karusala (2021):\n\n“While the frequency of physical attack on members of these and other racially or sexually marked groups is very disturbing, I also include in this category less severe incidents of harassment, intimidation, or ridicule simply for the purpose of degrading, humiliating, or stigmatizing group members.”\n\n\nThe authors continue to say that it’s less so the violent act than it is the social conditions which continue to permit it to happen. They illustrate this face of oppression in scholarly communications with evidence from reviews in which questions are called about the relevance of a health topic if it only affects a small population, the criticism of research aims of Black scholars as outside of typical scholarship, how disabled persons are singled out for research, or the long-term bias against low citation counts and the assumptions of quality or relevancy.\nThese five faces of oppression attributed to Young by Kumar and Karusala (2021) provide a framework for understanding and deconstructing biases in our choices and assumptions affecting citational justice in scholarly communication. This is not the only framework or examples that can be found. Unethical citation practices have been around for quite some time, so there are more resources for understanding how these exist.\n\n13.4.1.1 Chicken and Egg\nKwon (2022) argues that citation-based evaluation of individual researchers in any context (e.g., funding, publishing, promotion, awards) needs to change or be reduced in importance and replaced by engaging, recognizing and valorizing ideas from diverse sources. Mott and Cockayne (2017) also recognize citations as a problematic technology but also suggest that citations can act as a feminist and anti-racist “technology of resistance” to correct the imbalance. There appears to be a tension there between the need to reduce the influence of citations while at the same time exploiting that influence for the purpose of correcting the injustices perpetrated by them.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Equity, diversity, and inclusion in research</span>"
    ]
  },
  {
    "objectID": "ch13.html#conclusion",
    "href": "ch13.html#conclusion",
    "title": "13  Equity, diversity, and inclusion in research",
    "section": "13.5 Conclusion",
    "text": "13.5 Conclusion\nThis chapter has examined the imbalances that occur in scholarly communication through Merton, Bourdieu, and Rossiter’s respective sociological theories and how our citation behaviours as authors can represent citational injustice or even epistemic injustice through our conscious or unconscious choices. We critically examined a few of the data sources and tools used for analysis within bibliometrics, such as gender-determining algorithms and global income categorization, for their limitations and advantages, as part of the ongoing attempts by the scientific community to address equitable imbalances within our scholarly communication system. I closed with thoughts on thinking about our citation system objectively as functionalists or as emancipatory activists.\nIn writing such a chapter, it must be acknowledged that it was written by a privileged white person who has settled in Canada. Some of the sources I draw from are by self-identified persons of colour, and these sources should be read fully so that my filtered version does not take away from the significance of their words and experiences. This filtration is typical in Western society and represents the endemic cultural imperialism in our education system. I appreciate the opportunity, as a queer, transgender woman, to provide my perspective. Still, I recognize it is a very narrow lens as a participant within a global community of knowledge producers.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Equity, diversity, and inclusion in research</span>"
    ]
  },
  {
    "objectID": "ch13.html#references",
    "href": "ch13.html#references",
    "title": "13  Equity, diversity, and inclusion in research",
    "section": "13.6 References",
    "text": "13.6 References\n\n\n\n\nAhmed, Sara. 2013. “Making Feminist Points.” https://feministkilljoys.com/2013/09/11/making-feminist-points/.\n\n\nBaggs, Judith Gedney, Marion E. Broome, Molly C. Dougherty, Margaret C. Freda, and Margaret H. Kearney. 2008. “Blinding in Peer Review: The Preferences of Reviewers for Nursing Journals.” Journal of Advanced Nursing 64 (2): 131–38. https://doi.org/10.1111/j.1365-2648.2008.04816.x.\n\n\nBourdieu, Pierre. 1975. “The Specificity of the Scientific Field and the Social Conditions of the Progress of Reason.” Social Science Information 14 (6): 19–47. https://doi.org/10.1177/053901847501400602.\n\n\nChakrabarty, Dipesh. 2007. Provincializing Europe: postcolonial thought and historical difference. Princeton studies in culture / power / history. Princeton (N.J.): Princeton university press.\n\n\nCole, Jonathan R, and Stephen Cole. 1973. Social Stratification in Science. Chicago, IL: University of Chicago Press.\n\n\nFricker, Miranda. 2007. Epistemic Injustice. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780198237907.001.0001.\n\n\nGhiasi, G., P. Mongeon, C. Sugimoto, and V. Larivière. 2018. “Gender Homophily in Citations.” In, 1519–25. Centre for Science; Technology Studies (CWTS). https://hdl.handle.net/1887/65291.\n\n\nHarris, Matthew, James Macinko, Geronimo Jimenez, and Pricila Mullachery. 2017. “Measuring the Bias Against Low-Income Country Research: An Implicit Association Test.” Globalization and Health 13 (1). https://doi.org/10.1186/s12992-017-0304-y.\n\n\nKumar, Neha, and Naveena Karusala. 2021. “CHI ’21: CHI Conference on Human Factors in Computing Systems.” In, 1–9. Yokohama Japan: ACM. https://doi.org/10.1145/3411763.3450389.\n\n\nKwon, Diana. 2022. “The Rise of Citational Justice: How Scholars Are Making References Fairer.” Nature 603 (7902): 568–71. https://doi.org/10.1038/d41586-022-00793-1.\n\n\nLarivière, Vincent, Chaoqun Ni, Yves Gingras, Blaise Cronin, and Cassidy R. Sugimoto. 2013. “Bibliometrics: Global Gender Disparities in Science.” Nature 504 (7479): 211–13. https://doi.org/10.1038/504211a.\n\n\nLee, Carole J., Cassidy R. Sugimoto, Guo Zhang, and Blaise Cronin. 2013. “Bias in Peer Review.” Journal of the American Society for Information Science and Technology 64 (1): 2–17. https://doi.org/10.1002/asi.22784.\n\n\nLinxen, Sebastian, Christian Sturm, Florian Brühlmann, Vincent Cassau, Klaus Opwis, and Katharina Reinecke. 2021. “CHI ’21: CHI Conference on Human Factors in Computing Systems.” In, 1–14. Yokohama Japan: ACM. https://doi.org/10.1145/3411764.3445488.\n\n\nMas-Bleda, Amalia, and Mike Thelwall. 2016. “Can Alternative Indicators Overcome Language Biases in Citation Counts? A Comparison of Spanish and UK Research.” Scientometrics 109 (3): 2007–30. https://doi.org/10.1007/s11192-016-2118-8.\n\n\nMeho, Lokman I. 2022. “Gender Gap Among Highly Cited Researchers, 20142021.” Quantitative Science Studies, November, 1–21. https://doi.org/10.1162/qss_a_00218.\n\n\nMerton, Robert K. 1968. Social Theory and Social Structure. New York: Free Press.\n\n\nMongeon, Philippe, and Adèle Paul-Hus. 2015. “The Journal Coverage of Web of Science and Scopus: A Comparative Analysis.” Scientometrics 106 (1): 213–28. https://doi.org/10.1007/s11192-015-1765-5.\n\n\nMongeon, Philippe, Adèle Paul-Hus, Maria Henkel, and Vincent Larivière. 2022. “On the Impact of Geo-Contextualized and Local Research in the Global North and South.” https://doi.org/10.5281/ZENODO.6956977.\n\n\nMott, Carrie, and Daniel Cockayne. 2017. “Citation Matters: Mobilizing the Politics of Citation Toward a Practice of ‘Conscientious Engagement’.” Gender, Place & Culture 24 (7): 954–73. https://doi.org/10.1080/0966369X.2017.1339022.\n\n\nPatin, Beth, Melinda Sebastian, Jieun Yeon, and Danielle Bertolini. 2020. “Toward Epistemic Justice: An Approach for Conceptualizing Epistemicide in the Information Professions.” Proceedings of the Association for Information Science and Technology 57 (1). https://doi.org/10.1002/pra2.242.\n\n\nRay, Keisha S., Perry Zurn, Jordan D. Dworkin, Dani S. Bassett, and David B. Resnik. 2022. “Citation Bias, Diversity, and Ethics.” Accountability in Research 0 (0): 1–15. https://doi.org/10.1080/08989621.2022.2111257.\n\n\nRossiter, Margaret W. 1993. “The Matthew Matilda Effect in Science.” Social Studies of Science 23 (2): 325–41. https://doi.org/10.2307/285482.\n\n\nScherer, Layne, ed. 2021. Addressing Diversity, Equity, Inclusion, and Anti-Racism in 21st Century STEMM Organizations. National Academies Press. https://doi.org/10.17226/26294.\n\n\nTraag, V. A., and L. Waltman. 2022. “Causal Foundations of Bias, Disparity and Fairness.” https://doi.org/10.48550/ARXIV.2207.13665.\n\n\nWinner, Langdon. 1980. “Do Artifacts Have Politics?” Daedalus 109 (1): 121–36. http://www.jstor.org/stable/20024652.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Equity, diversity, and inclusion in research</span>"
    ]
  },
  {
    "objectID": "ch9.html#data-sources",
    "href": "ch9.html#data-sources",
    "title": "9  Altmetrics",
    "section": "9.2 Data sources",
    "text": "9.2 Data sources\n\n9.2.1 Commercial data sources\n\n9.2.1.1 Altmetric\nAltmetric (https://www.altmetric.com/) is an appropriately named company that supplies altmetric data. Using a variety of sources ranging from social media to policy documents, Altmetric tracks conversations around scholarly content. Their goal is to illuminate the attention that scientific articles are receiving in real-time, which allows authors and publishers to know what people are saying about their work.\nThe Altmetric donut illustrated below is designed to convey the amount and type of attention a scholarly article has received at a quick glance.\n\n\n\n\n\nThe colors of the donut each represent a different attention source. This means that the amount of color in the donut varies based on how much attention an article has received from different sources.\n\n\n\n\n\nThe number in the middle of the donut is the Altmetric Attention Score, which is a weighted count of all the attention a scholarly article has received. It is calculated based on three factors:\n\nVolume: The more people mention an article, the higher its score will be.\nSources: Different sources of mentions contribute a different weighted amount to the Altmetric Attention Score. For example, being mentioned in a news article carries more weight than being mentioned in a tweet and thus contributes to a higher score.\nThe sharing frequency of the source: The amount that an author of a mention generally talks about scholarly articles is considered in the score. For example, a doctor who shares a link with other doctors on Twitter counts more than a journal’s Twitter account that automatically pushes out links to all their articles.\n\nA good way to collect altmetric data for a publication is to use the Altmetric Bookmarklet.\nOnce added to your bookmarks, you can click on the Altmetric it! bookmark when you are looking at an article online, and the altmetric badge will pop up.\n\nThen if you click on “click for more details”, you will be able to see a complete altmetric profile for the article.\n\n\n\n9.2.1.2 Plum Analytics\nPlum Analytics (https://plumanalytics.com/) is a competitor of Altmetric, owned by Elsevier. Because it is owned by Elsevier, the altmetrics indicators obtained from Plum Analytics (which they call PlumX Metrics) are usually available on Elsevier’s journals and databases like Scopus and Scival.\nHere is an example of the PlumX metrics displayed on an article’s record in Scopus.\n\n\n\n\n\nAgain, you can click on “View PlumX details” to obtain the full PlumX Metrics profile for the article.\n\n\n\n\n\n\n\n9.2.1.3 Overton\nWhile we will not go into details here, it is worth mentioning the relatively recent launch of a new altmetric startup, Overton (https://www.overton.io/), that focuses exclusively on tracking the policy impact of research.\n\n\n\n9.2.2 Open data sources\n\n9.2.2.1 Crossref Event Data\nCrossref Event Data is a free source of altmetric data that collect events involving digital objects (things with DOIs). The data sources and the types of events collected are described here.\nOne challenge of using Crossref Event Data is that is that there is no easy-to-use browser plugin, website, or search engine. Data can be retrieved using the API, which has a bit of a learning curve.\nTo make things simple, here is a code that you can run in R to collect the events for a single DOI.\n\nlibrary(httr)\nlibrary(stringr)\nlibrary(dplyr)\n\nemail &lt;- \"YOUR EMAIL HERE\"\ndoi &lt;- \"DOI HERE\"\nfilename &lt;- \"event_data_output.csv\" # change the name of the output file as needed (as to be a .csv). \n\nevents_raw &lt;- content(GET(str_c(\"https://api.eventdata.crossref.org/v1/events?mailto=\",\n                    email,\"&obj-id=\",doi,\"&rows=1000\", sep = \"\")))$message$event\n\nevents &lt;- tibble()\nfor (i in 1:length(events_raw)) {\n  data&lt;-tibble()\n  subj_id &lt;- events_raw[[i]]$subj_id\n  relation &lt;- events_raw[[i]]$relation_type_id\n  obj_id &lt;- events_raw[[i]]$obj_id\n  date &lt;- events_raw[[i]]$occurred_at\n  events &lt;- bind_rows(events, tibble(subj_id, relation, obj_id, date))\n}\n\nwrite.csv(events, filename)\n\nThe Crossref Even Data API provides much more information, but the code above will extract the essential:\n\nsubj_id: the source of the event.\nrelation: the type of event (references, mentions, discusses).\nobj_id: the object of the event.\ndate: the date of the event.\n\nThis file is an example of the data you would obtain if you ran this code for the DOI 10.1371/journal.pone.0127502.\n\n\n\n\n\n\nNote\n\n\n\nThe metadata retrieval app that we are developing will soon allow you to automatically collect event data from a list of DOI or OpenAlex work IDs. Stay tuned!\n\n\n\n\n9.2.2.2 Impactstory Profiles\nImpactstory profiles is a tool launched in 2011 by what is now the non-profit organization OurResearch. Researchers can join through their Twitter account and then link their Impactstory profile to their ORCID account. The profiles are public, so researchers can easily share their Impactstory profiles with others. Impactstory doesn’t have a search engine to find or browse profiles. But you can see if someone has a profile by finding them on ORCID and sticking their ORCID idea at the end of this URL: https://profiles.impactstory.org/u/\nHere’s an example ImpactStory profile: https://profiles.impactstory.org/u/0000-0003-1021-059X",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Altmetrics</span>"
    ]
  },
  {
    "objectID": "ch9.html#conclusion",
    "href": "ch9.html#conclusion",
    "title": "9  Altmetrics",
    "section": "9.3 Conclusion",
    "text": "9.3 Conclusion\nThis chapter introduced the concept of altmetrics and provided examples of indicators and where to get them. While the idea of capturing research output and impact beyond the traditional scholarly communication system is appealing, caution remains necessary when using and interpreting altmetric data. Here are a few things to keep in mind:\n\nThe data is only captured for documents with a DOI.\nAltmetrics do not predict or replace citations.\nMedia and other altmetrics data sources are heterogenous, so composite indicators like the altmetric score can be misleading and difficult to compare for different documents. It is often better to consider the different altmetric indicators separately rather than using a composite indicator.\nCaution is necessary when attributing meaning to altmetric events. What does a tweet, a mention, or a download mean?\nAltmetrics are often counted because they are easy to capture, which does not necessarily mean that they are valid or useful. Does everything that can be counted actually count?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Altmetrics</span>"
    ]
  },
  {
    "objectID": "ch5.html#classifications",
    "href": "ch5.html#classifications",
    "title": "5  Classifying research",
    "section": "5.2 Classification(s)",
    "text": "5.2 Classification(s)\nA classification is essentially a list of categories in which entities can be classified (what disciplines exist). Many classifications have been developed by organizations around the world, and the best classification to use is often determined by availability as well as the subject and goals of the analysis. Below are a few examples of general classifications (that cover all disciplines) and disciplinary classifications (that cover single disciplines with a higher level of granularity than the general classifications).\n\n5.2.1 General classifications\n\nThe Science Metrix classification of research outputs categorizes scientific journals and articles in 5 domains, 20 fields and 174 subfields. The classification can be downloaded here.\nThe Scopus All Science Journal Classification (ASJC) divides journals in 334 fields and 4 research areas. The list of ASJC fields can be found here.\nThe National Science Foundation (NSF) classification is a tried and tested mutually exclusive classification used in the Science & Engineering Indicators since the 1970s. It was originally designed by CHI Research (Archambault, Beauchesne, and Caruso 2011). It contains three layers: 2 domains, 14 fields, and 143 specialties.\nThe Web of Science Subject Categories is a journal-level non-exclusive classification of journals in 250 subject categories available in the Web of Science. More information can be found here.\nThe Field of Science and Technology (FOS) classification of the OECD has 40 FOS grouped in six broad fields. Details can be found here. The Web of Science provides a mapping of the FOS and the Web of Science subject categories here.\nThe fields of research (FOR) from the Australian and New Zealand Standard Research Classification (ANZSRC). The FOR have three levels: divisions, groups, and fields. You can find more details on the ANZSRC website. This classification is used by the Dimensions database to assign FOR to articles and to calculate the field citation ratio (FCR), which we will discuss again in chapter 7.\n\n\n\n5.2.2 Disciplinary classifications\nSome disciplines have developed their own classification. Here are some examples.\n\nThe Medical Subject Headings (MeSH) are used in PubMed and Medline databases to facilitate searching (details here).\nThe Mathematics Subject Classification (MSC) (details here).\nThe Journal of Economics Literature (JEL) classification in economics (details here)\n\n\n\n5.2.3 Computational (bottom up) approaches\nThere exists a variety of computational approaches to divide any set of entities into groups that can (both do not always) make sense. Topic models are an example. That said, topic models are not that popular in bibliometric studies, for which researchers tend to adopt citation-based networks approaches (discussed in more details below).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classifying research</span>"
    ]
  },
  {
    "objectID": "ch5.html#classifying",
    "href": "ch5.html#classifying",
    "title": "5  Classifying research",
    "section": "5.3 Classifying",
    "text": "5.3 Classifying\nAssigning one or multiple discipline to documents or other entities can be a challenging task. Depending on the objective of your analysis, it may be preferable to use the classification already available in the database you are using (or to choose a database that uses the classification that best suits your needs).\nBut how do we assign a discipline to another entity, like a researcher?\n\nDo we use the discipline of their PhD?\nDo we use the discipline of their current department or faculty?\nDo we use the discipline classification of their articles or the journals in which they are published?\n\nThere is no right answer to these questions. Most often our main guide will be data availability and quality. The discipline of the Ph.D., for instance, is an information rarely available other than on the CV of the researcher. Furthermore, not all bibliographic records include the department of the authors, and when they do, they do not usually use controlled vocabulary so the same departments can come up with different names or spelling, and sometimes it will require some web searching to figure out the discipline to which the department could be assigned to. Moreover, the department names might not match your disciplinary classification which can make the pairing of departments with disciplines challenging. If the publications in our database are assigned to one or many disciplines, we can infer the discipline of the researcher using the papers they published, or the journals in which they published. What do we do when a researcher has 5 publications in Physics, 3 in chemistry and 4 in economics? Is that person a Physicist, a Chemist, an Economist? A mix?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classifying research</span>"
    ]
  },
  {
    "objectID": "ch1.html#data-driven-decision-making",
    "href": "ch1.html#data-driven-decision-making",
    "title": "1  Introduction",
    "section": "1.2 Data-Driven Decision Making",
    "text": "1.2 Data-Driven Decision Making\nOne of the consequences of this rise is that managers are now pressed to make data-driven decisions and prioritize them above other ways of deciding [3]. Data-driven decision-making has a few advantages. First, by making decisions based on data, our decisions can be based on some objective measure. This helps us keep our biases in check, helping us make better decisions based on evidence. Better decisions translate into better outcomes, whether those are profits, improved well-being, or a reduction in environmental impact.\nA second advantage of data-driven decisions is that it allows us to know things that are difficult to know otherwise. For example, a shop owner running an e-commerce website might be able to measure the impact of their newsletter advertising decisions on their customer retention rate [4]. Without data, it would be hard to measure those impacts. Even with very effective word-of-mouth feedback, a shop owner might, at best, be able to interpret the feedback of a few hundred customers. With data, you can measure the impacts for all customers, giving a comprehensive approach. Data allows managers to not just decide, but to make decisions based on a more comprehensive understanding of the facts. This is not a perfect solution; sources of data can be biased. Data can also be dehumanizing, especially in sensitive contexts [5]. However, data-driven decision-making can drive drastic improvements in an organization. This is why it is so widely adopted today and helps explain the widespread popularity of data science, the art of making use of large or complex data, over the previous decade [6].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch1.html#defining-data-information-knowledge-and-wisdom",
    "href": "ch1.html#defining-data-information-knowledge-and-wisdom",
    "title": "1  Introduction",
    "section": "1.3 Defining Data, Information, Knowledge, and Wisdom",
    "text": "1.3 Defining Data, Information, Knowledge, and Wisdom\nBefore we can dive into the details of how this works, we need to define some concepts. To start, what do we mean by data? A classic understanding is that data are symbols that represent objects in the world, usually in a highly usable and algorithmic form [7]. Data are the building blocks of a modern data-driven decision system. Data are used to create information. In the context of decision-making, information is the way through which data is understood and used, often contained in descriptions and answers to questions. A related concept, an information system, is a collection of humans and machines that make use of data for some purpose. Information is used to generate knowledge. Knowledge is a very broad concept, but in the context of data-driven decisions, it is the know-how that transforms information into action. Knowledge is usually transferred between people or machines to help them make effective decisions. It is also closely related to intelligence (i.e., “business intelligence”), one of the main functions of a data analyst. For the purposes of this book, knowledge generated from data is synonymous with business intelligence. Finally, wisdom can be described as the capacity to interpret knowledge in context. Human wisdom informs effective judgement of information and knowledge. Wisdom is accumulated through constant feedback and experience. Individuals often generate wisdom over time, though organizations can be thought of as having wisdom, especially in the “know-how” to develop and execute their strategies. These four concepts are captured by the DIKW hierarchy, illustrated in Figure 1.1. An effective data-driven decision system considers this hierarchy. Data is valuable, but without the ability to understand its informational context, it is not useful. Information may be useful, but if an organization can’t act on it, it cannot achieve its goals. Without the wisdom to apply all of this in context, managers may miss critical decision factors. We will return to this concept a few times throughout the book; this pyramid ultimately explains how value is created from data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch1.html#learning-objectives",
    "href": "ch1.html#learning-objectives",
    "title": "1  What is data",
    "section": "",
    "text": "Understanding the concepts of data, dataset, database, data management, and database management systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data</span>"
    ]
  },
  {
    "objectID": "ch1.html#what-is-data",
    "href": "ch1.html#what-is-data",
    "title": "1  What is data",
    "section": "1.2 What is data?",
    "text": "1.2 What is data?\nThe Merriam-Webster online dictionary provides three definitions of the word data:\n\nfactual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation.\n\n\ninformation in digital form that can be transmitted or processed\n\n\ninformation output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful\n\n— Merriam-Webster dictionary\n\n\nTogether, these definitions offer us a set of key elements from which we can build a broad understanding of the concept of data. The first key term is factual information or fact suggesting that data is objective and, like the rest of the definition shows, is used for a given purpose, such as discussing reasoning or decision-making.\nThe second definition is related to using the word data in a computational or communicational sense, where data is the “thing” that is being stored, transmitted, received, processed, etc.\nWhile the first definition suggests that humans and machines use data for processes such as decisions and calculations, the third definition highlights that data does not only exist in nature but can also be created by humans and machines, either purposefully or not.\nWhile we often think of data as things found in spreadsheets and stored in computers or filing cabinets, data is much more than that. Data is everywhere around us all the time in the form of energy and sound or light waves, for instance. Our sensory organs are data captors that pick up data from our environment. Our brains process, structure, and possibly store the data so we can consciously or unconsciously use it now or later as a basis for decisions and actions. That said, in this course, we will not concern ourselves with this kind of data and process. Instead, we will focus on digitally recorded data, the kind that we can store in a computer.\nOne way to try to make sense of the concept of data is by situating it in relation to other concepts. The data, information, knowledge and wisdom (DIKW) hierarchy or pyramid (pictured below) can be helpful to us as it offers a visual representation of such a relationship.\nThe pyramid suggests that data generates information, information generates knowledge, and knowledge generates wisdom. However, there is no consensus on the definition of each level of the pyramid, no consensus regarding the number of layers the pyramid should have, and no consensus regarding the hierarchical nature of the relationship between the concepts (Rowley 2007). Couldn’t we capture knowledge and store it in the form of data, for example?\nRowley (2007) reviewed the literature on the pyramid and found that data is typically understood as discrete objects, facts or observations, or recorded descriptions of things, events, activities, or transactions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data</span>"
    ]
  },
  {
    "objectID": "ch1.html#datasets",
    "href": "ch1.html#datasets",
    "title": "1  What is data",
    "section": "1.3 Datasets",
    "text": "1.3 Datasets\nWe often encounter the term “dataset” on the web or in our workplaces, and I think it is worth writing a few lines to relate the terms to the other terms we will use in this course. The terms data and dataset will often be used interchangeably since dataset literally means a set of data, and data is the plural of datum. One difference, in principle, is that datasets are usually assembled for a given purpose. In research, for instance, a dataset will be the exact collection of data collected for the analysis. In supervised machine learning, we distinguish between training and testing datasets. When a professor sends you an excel file with data to work with for an assignment, that’s a dataset. You find datasets when you browse websites like kaggle.com, zenodo.org, or dataverse.org. Datasets are also static, whereas databases can be dynamic.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data</span>"
    ]
  },
  {
    "objectID": "ch1.html#databases",
    "href": "ch1.html#databases",
    "title": "1  What is data",
    "section": "1.4 Databases",
    "text": "1.4 Databases\nWhat is a database? According to the Merriam-Webster dictionary, a database is “a usually large collection of data organized especially for rapid search and retrieval (as by a computer)”. The keyword here is organized, highlighting that databases are both products and tools for data management.\nDatabases are usually created and managed for some purposes. These purposes may be specific (e.g. keeping track of a store’s inventory) or broad (tracking socioeconomic trends). Depending on their purposes, databases can vary in size and complexity. Any organized data collection could be considered a database, even if it is as basic as an Excel spreadsheet with the names and addresses of your friends or your to-do list.\nA database can contain or be used to create multiple datasets, but a dataset would typically not contain multiple databases. Of course, this does not mean that datasets are always drawn from databases. For example, datasets can be created by surveying or interviewing people or recording observations of natural phenomena.\nNote, however, that those differences are not hard truths, as some datasets may serve a greater variety of users and purposes than some databases.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data</span>"
    ]
  },
  {
    "objectID": "ch1.html#database-management-systems-dbms",
    "href": "ch1.html#database-management-systems-dbms",
    "title": "1  What is data",
    "section": "1.5 Database management systems (DBMS)",
    "text": "1.5 Database management systems (DBMS)\nA Database Management System (DBMS) is software that supports the development, maintenance, security, and use of databases. You will often come across the DBMS acronym with different suffixes attached to it, such as RDBMS (Relational DBMS), OODBMS (Object-Oriented DBMS), or ORDBMS (Object-Relational DBMS). Note that all these DBMS generally offer the same basic features. The main difference is that they work with different data types and structures. You will not working with a DBMS in this course but we will mention some of the most popular ones that work with the specific data structures that we encounter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data</span>"
    ]
  },
  {
    "objectID": "ch1.html#data-management",
    "href": "ch1.html#data-management",
    "title": "1  What is data",
    "section": "1.6 Data management",
    "text": "1.6 Data management\nSo far in this chapter, we explored the concepts of data and its different levels of structure, datasets, databases and data management systems. Aside from related to data, what do all these concepts have in common? They are what data managers work with. Data managers unlock the potential of data for a given purpose, individual, group or organization, by developing and implementing data strategies and processes such as data retrieval, processing, cleaning, storage and analysis.\nThe value of data depends on the purpose it serves. Thus, good data management requires a good understanding of both the data and the needs of its users so that optimal data strategies can be developed and implemented.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data</span>"
    ]
  },
  {
    "objectID": "ch1.html#from-unstructured-to-structured-data",
    "href": "ch1.html#from-unstructured-to-structured-data",
    "title": "1  Thinking about data",
    "section": "1.7 From unstructured to structured data",
    "text": "1.7 From unstructured to structured data\nData can take many forms that we can situate along a continuum with unstructured data at one extreme, structured data at the other, and semi-structured data in between. In this section, we briefly explore what these concepts mean.\n\n1.7.1 Unstructured data\nLet us consider a simple example to better grasp the difference between structured and unstructured data and, at the same time, to get a sense of the process of structuring data so that it can be more effectively strored and analyzed.\nHere is a brilliant piece of writing:\n\nA data story\nPhilippe Mongeon\nChapter 1\nOnce upon a time, a random internet user created a short text and abandoned it in a sea of unstructured data.\nChapter 2\nAn algorithm passed by and decided to add XML tags to the text.\nChapter 3\nA data manager stumbled upon the data and thought it would be relevant for the database users. So they decided to add it to their database.\nThe end.\n\nThat’s unstructured data (not entirely, the line breaks are a very basic structure). We humans who have read a book before can see the structure. We see a title, an author name, and three chapters that contain some text. This is our brains structuring the data that our eyes capture. The computer does not perform that structuring process by itself. For it, this is just a bunch of symbols. In fact, for a computer this is all 1s and 0s (binary data) because that’s the only type of data computers actually stores and understands. Because the computers we use are made by and for humans, we have programmed them to show us symbols we understand.\n\n\n1.7.2 Semi-structured data\nAnother way to think about the data above is that it has an implicit structure. But humans have invented different tools to make the structure of data explicit. One of these tools is XML (Extensible Markup Language). It is very similar to the Hypertext Markup Language (HTML), which web browers use to interpret the structure of a webpage (such as the one you are currently viewing). These tagging systems use tags to allow humans and computer to differentiate between different parts of the data, and to indicate what it represents (e.g., a title, an author, a paragraph, etc.)\nHere is how, for example, we can apply XML tags to give our data some structure.\n\n&lt;book&gt;\n  &lt;title&gt;A data story&lt;/title&gt;\n  &lt;author&gt;Philippe Mongeon&lt;/author&gt;\n  &lt;chapter&gt;\n    &lt;title&gt;Chapter 1&lt;/title&gt;\n    &lt;paragraph&gt;Once upon a time a short text was created by a random internet user\n    and abandoned in a sea of unstructured data.&lt;/paragraph&gt;\n  &lt;/chapter&gt;\n  &lt;chapter&gt;\n    &lt;title&gt;Chapter 2&lt;/title&gt;\n    &lt;paragraph&gt;An algorithm passed by and decided to add xml tags to the text.&lt;/paragraph&gt;\n  &lt;/chapter&gt;\n  &lt;chapter&gt;\n    &lt;title&gt;Chapter 3&lt;/title&gt;\n    &lt;paragraph&gt;A data manager stumbled upon the data, thought it would be relevant\n    for the database users, and so she decided to add it to include it in her\n    database.&lt;/paragraph&gt;\n    &lt;paragraph&gt;The end.&lt;/paragraph&gt;\n  &lt;/chapter&gt;\n&lt;/book&gt;\n\nAs you can see, the XML tags already help us (and the computer) identify different text components, such as the title, the author, headers and paragraphs. XML documents are a typical example of semi-structured data.\nAnother frequently used semi-structured format is JSON. It is a name-value pair format. We can use it to structure our text by identifying the different parts (the names) and their content (the values).\n\n{\n  \"title\": \"A data story\",\n  \"author\": \"Philippe Mongeon\",\n  \"Chapter 1\": \"Once upon a time a short text was created by a random internet user and abandoned in a sea of unstructured data.\",\n  \"Chapter 2\": \"An algorithm passed by and decided to add xml tags to the text.\",\n  \"Chapter 3\": \"A data manager stumbled upon the data, thought it would be relevant for the database users, and so she decided to add it to her database. The end.\"\n}\n\nOne feature of semi-structured data formats such as XML and JSON is that they are very flexible, which can be great but can also be a problem for the end users who may often have to deal with complex or unclear data structures that are not easy to process and analyze. Notice how I included the Chapter number in the tags in the JSON example above? That was an arbitrary choice, and I could have chosen another structure for my file, such as this one:\n\n{\n  \"title\": \"A data story\",\n  \"author\": \"Philippe Mongeon\",\n  \"Chapters\": [\n      {\n        \"title\": \"Chapter 1\",\n        \"content\": \"Once upon a time a short text was created by a random internet user and abandoned in a sea of unstructured data.\"\n      }\n      {\n        \"title\": \"Chapter 2\",\n        \"content\": \"An algorithm passed by and decided to add xml tags to the text.\"\n      }\n      {\n        \"title\": \"Chapter 3\",\n        \"content\": \"A data manager stumbled upon the data, thought it would be relevant for the database users, and so she decided to add it to her database. The end.\"\n      }\n  ]\n}\n\nThis structure (or semi-structure) is just as good as the previous one. That flexibility is one of the reasons why semi-structured data formats and databases are growing in popularity. Because the data structure is included directly in the document, the users do not need to follow a predefined structure and use predefined tags to store their data in the database. This means that, theoretically, every record could have a different structure, which would be fine, but most likely not optimal.\n\n\n1.7.3 Structured data\nStructured data and databases are typically tabular (like an Excel spreadsheet). Each row is a record or entry, and each column is a field, feature, variable, etc. The structure is part of the database design (and not each database entry, as in the semi-structured format), and so every new record has to follow that same format. For example, here is the same work of art in a structured format.\n\n\n\nExample of structured data\n\n\nTitle\nAuthor\nSection\nContent\n\n\n\n\nA data story\nPhilippe Mongeon\nChapter 1\nOnce upon a time a data was created by a random internet user and abandoned in a see of unstructured data.\n\n\nA data story\nPhilippe Mongeon\nChapter 2\nAn algorithm passed by and decided to add xml tags to the text.\n\n\nA data story\nPhilippe Mongeon\nChapter 3\nA data manager stumbled upon the data, thought it would be relevant for the database users, and so she decided to add it to include it in her relational database. The end.\n\n\n\n\n\n\n\nHere, the structure is determined by the columns I chose for my table (title, author, section, and content). But, again, this was an arbitrary choice, and I could have chosen a completely different structure (although the realm of possibilities is somewhat limited by the data and by common sense). The point is that there is rarely an absolute best way of structuring data, and the best structure is the one that best suits the needs of the users. Sometimes the same data may be duplicated and structured differently to suit different users and uses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Thinking about data</span>"
    ]
  },
  {
    "objectID": "ch1.html#practice",
    "href": "ch1.html#practice",
    "title": "1  Thinking about data",
    "section": "1.8 Practice",
    "text": "1.8 Practice\nSince this course is intended to be accessible to students with little to no experience working with data, this week is dedicated to developing or polishing your Microsoft Excel skills.\nWhile Excel has some data management capabilities, that’s not what it is designed and most commonly used for. You can still think of it as a very rudimentary form of DBMS. However, Excel is a flexible tool that you can use to quickly explore, manipulate and structure data before creating those structures in an actual DBMS. Excel will also always be relevant in your data management workflow because it remains one of the most accessible ways to work with data for downstream processes (before data gets stored in a DMBS) and upstream processes (after the data is retrieved from the DBMS).\n\n1.8.0.1 Formatting exercise\n\nDownload the Easy_Excel.xlsx file (provided by Julie Marcoux, a data librarian at the Killam Library)\nFollow the instructions in the instructions sheet.\nThe exercise sheet contains the data to format.\nAt the end of the exercise, your table should look similar to the one found in the Results sheet.\nThe Useful functions sheet contains tips on using some Excel formatting functions.\n\n\n\n1.8.0.2 Analyzing exercise\n\nWatch the demo on pivot tables in Excel below.\nCreate a pivot table to explore the data in the _The exercise sheet of the Easy_Excel.xlsx used for the first exercise.\nExplore ways of structuring your pivot table and combining and filtering columns.\nWhich publisher has the most books in the dataset?\nOn average, are cloth-bound books more or less expensive than paper-bound books?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Thinking about data</span>"
    ]
  },
  {
    "objectID": "ch1.html#the-data-information-knowledge-and-wisdom-pyramid",
    "href": "ch1.html#the-data-information-knowledge-and-wisdom-pyramid",
    "title": "1  Thinking about data",
    "section": "1.3 The data, information, knowledge, and wisdom pyramid",
    "text": "1.3 The data, information, knowledge, and wisdom pyramid\nOne way to try to make sense of the concept of data is by situating it in relation to related terms. The data, information, knowledge and wisdom (DIKW) hierarchy or pyramid (pictured below) can be helpful to us as it offers a visual representation of the relationship between these concepts.\nThe hierarchy connects wisdom, knowledge, information, and data to identify how entities at the higher levels are created from those at lower levels. According to the pyramid, data generates information, information generates knowledge, and knowledge generates wisdom. However, there is no consensus on the definition of each level of the pyramid, no consensus regarding the number of layers the pyramid should have, and no consensus regarding the hierarchical nature of the relationship between the concepts (Rowley 2007).\nRowley (2007) reviewed the literature on the pyramid to analyze how each concept was defined, and found that most definitions of data point to it being discrete objects, facts or observations, or recorded descriptions of things, events, activities, or transactions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Thinking about data</span>"
    ]
  },
  {
    "objectID": "ch1.html#references",
    "href": "ch1.html#references",
    "title": "1  What is data",
    "section": "1.7 References",
    "text": "1.7 References\n\n\n\n\nRowley, Jennifer. 2007. “The Wisdom Hierarchy: Representations of the DIKW Hierarchy.” Journal of Information Science 33 (2): 163180. https://doi.org/10.1177/0165551506070706.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data</span>"
    ]
  },
  {
    "objectID": "ch3.html#why-data-types-matter",
    "href": "ch3.html#why-data-types-matter",
    "title": "3  Data Types",
    "section": "",
    "text": "Name\nAge\n\n\n\n\nCharlie Brown\n12\n\n\nSnow White\nBanana\n\n\nHarry Potter\n14",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "ch3.html#text",
    "href": "ch3.html#text",
    "title": "3  Data Types",
    "section": "3.2 Text",
    "text": "3.2 Text\nText is the most simple data type to work with because it can contain any sequences of letters, numbers, or symbols.\n\n\n\nExamples of the text data type\n\n\n\n\nI love sour gummies\n\n\n2 + 2 = 4\n\n\nI have 2 cats and 1 gold fish.\n\n\nthisisnotarealemail@somesite.com\n\n\n2\n\n\n=average(B2:4)\n\n\n\nNotice that in the second to last example, the text cell only contains what we would understand as a number. If we have not specified that the cell should contain text, then Excel will automatically store 2 as a number. But if we have set the type of the cell to text then Excel will store the character (and not the number) 2.\nThe last example is an Excel formula, however, stored as text the it won’t be understood as a formula and will not perform the intended operation, which in this case would be to calculate the average of the values contained in cells B2 to B4 of the Excel spreadsheet.\nText data is very limited in terms in terms of the type of analysis that we can do with it. We can perform operation like counting the number of characters a cell contains, or testing whether a cell contains a specific character sequences (e.g., identify all the cells that contain the word data in a spreadsheet). The most frequent use of text data is calculating frequencies (counting the number of times a specific text appears in a spreadsheet. We will explore this in more detail later in the course, but for now, here is an quick example of a dataset of names of which we could calculate frequencies.\n\n\n\nName\n\n\n\n\nJohn\n\n\nJulia\n\n\nJohn\n\n\nJim\n\n\nJulia\n\n\nJulia\n\n\n\nUsing this data, we can determine the frequency of each name, which essentially means counting the number of rows that contain each specific name in the dataset, which in this case would yield the following result:\n\n\n\nName\nFrequency (or count)\n\n\n\n\nJohn\n2\n\n\nJim\n1\n\n\nJulia\n3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "ch3.html#numbers",
    "href": "ch3.html#numbers",
    "title": "3  Data Types",
    "section": "3.3 Numbers",
    "text": "3.3 Numbers\nThe second most fundamental data type that every one needs to understand is numbers or numerical data. As we have seen, it is perfectly possible to store the symbol 2 as text, but by default or if specified by the user, Excel would treat the symbol 2 as a number.\nThere are two basic types of numbers: numbers without decimals (also know as integer) and numbers with decimals. In real life, we encounters numbers like temperature, prices. But what they represented does not matter for a computer. The number 7 is the number 7, whether it is dollars or degrees is irrelevant. However, Excel is a flexible tools that allows users to format and present numbers in ways that are meaningful to them, so you can choose to display a number as a monetary value, as a percentage, a fraction, or in scientific notation.\nUnlike text data, a wide range of mathematical operations can be applied to numerical data, such as addition, substraction, multiplication, division. We can summarize numerical data by calculating things like averages, sum, minimum, maximum, standard deviation, and frequencies, whereas, as we learned, we can only calculate the frequencies for textual data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "ch3.html#dates-and-times",
    "href": "ch3.html#dates-and-times",
    "title": "3  Data Types",
    "section": "3.4 Dates and times",
    "text": "3.4 Dates and times\nDates and times are a special data type because dates are represented as a day, a month, and a year, and times are represented as an hour, a minute, and a number of second. In fact, they are stored in the computer as simple numbers. For example, if you ask Excel to represent the number 1 as a date, you will get the first of January, 1900, or 1900-01-01. Times are stored by the computer as decimals. For example, the first of January goes from 12:00 am (1.0000) to 11:59:59 pm (1.99999).\nWhile different systems may use different methods to store dates as numbers. The most important thing to remember is that what you see as a date in Excel or in other software is actually stored as a number. Why does that matter? Because it means that you can apply to dates the same operations as you would to numbers. For example, if you add 1 to the date 1900-01-31, you obtain 1900-02-01 (and not something weird like 1900-01-32, which would not be valid).\nDates and times can be decomposed into their different parts. We will revisit this in the data processing chapter, but for now here’s an example of a date and time (2012-03-21 8:35:00 AM) decomposed using excel functions (year, month, day, hour, minute, second) that we will learn more about later.\n\n\n\nFormula\nOutcome\n\n\n\n\n=YEAR(“2012-03-21 8:35:00 AM”)\n2012\n\n\n=MONTH(“2012-03-21 8:35:00 AM”)\n3\n\n\n=DAY(“2012-03-21 8:35:00 AM”)\n21\n\n\n=HOUR(“2012-03-21 8:35:00 AM”)\n8\n\n\n=MINUTE(“2012-03-21 8:35:00 AM”)\n35\n\n\n=SECOND(“2012-03-21 8:35:00 AM”)\n0\n\n\n\n\n3.4.1 Summary\nIn this chapter, you learn that:\n\nData types are imporant for storage efficiency, data integrity, and data analysis\nThe two fundamental data types are text and numbers.\nNumbers can be displayed in different ways such as monetary values, dates, and times.\nA wide range of mathematical functions can be applied to numerical data, but not to textual data.\nDates and times are really stored as numbers\nDates and times can be decomposed in year, month, day, hour, minute, and second.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "ch2.html#from-unstructured-to-structured-data",
    "href": "ch2.html#from-unstructured-to-structured-data",
    "title": "2  Data structures",
    "section": "",
    "text": "2.1.1 Unstructured data\nLet us consider a simple example to better grasp the difference between structured and unstructured data and, at the same time, to get a sense of the process of structuring data so that it can be more effectively strored and analyzed.\nHere is a brilliant piece of writing:\n\nA data story\nPhilippe Mongeon\nChapter 1\nOnce upon a time, a random internet user created a short text and abandoned it in a sea of unstructured data.\nChapter 2\nAn algorithm passed by and decided to add XML tags to the text.\nChapter 3\nA data manager stumbled upon the data and thought it would be relevant for the database users. So they decided to add it to their database.\nThe end.\n\nThat’s unstructured data (not entirely, the line breaks are a very basic structure). We humans who have read a book before can see the structure. We see a title, an author name, and three chapters that contain some text. This is our brains structuring the data that our eyes capture. The computer does not perform that structuring process by itself. For it, this is just a bunch of symbols. In fact, for a computer this is all 1s and 0s (binary data) because that’s the only type of data computers actually stores and understands. Because the computers we use are made by and for humans, we have programmed them to show us symbols we understand.\n\n\n2.1.2 Semi-structured data\nAnother way to think about the data above is that it has an implicit structure. But humans have invented different tools to make the structure of data explicit. One of these tools is XML (Extensible Markup Language). It is very similar to the Hypertext Markup Language (HTML), which web browers use to interpret the structure of a webpage (such as the one you are currently viewing). These tagging systems use tags to allow humans and computer to differentiate between different parts of the data, and to indicate what it represents (e.g., a title, an author, a paragraph, etc.)\nHere is how, for example, we can apply XML tags to give our data some structure.\n\n&lt;book&gt;\n  &lt;title&gt;A data story&lt;/title&gt;\n  &lt;author&gt;Philippe Mongeon&lt;/author&gt;\n  &lt;chapter&gt;\n    &lt;title&gt;Chapter 1&lt;/title&gt;\n    &lt;paragraph&gt;Once upon a time a short text was created by a random internet user\n    and abandoned in a sea of unstructured data.&lt;/paragraph&gt;\n  &lt;/chapter&gt;\n  &lt;chapter&gt;\n    &lt;title&gt;Chapter 2&lt;/title&gt;\n    &lt;paragraph&gt;An algorithm passed by and decided to add xml tags to the text.&lt;/paragraph&gt;\n  &lt;/chapter&gt;\n  &lt;chapter&gt;\n    &lt;title&gt;Chapter 3&lt;/title&gt;\n    &lt;paragraph&gt;A data manager stumbled upon the data, thought it would be relevant\n    for the database users, and so she decided to add it to include it in her\n    database.&lt;/paragraph&gt;\n    &lt;paragraph&gt;The end.&lt;/paragraph&gt;\n  &lt;/chapter&gt;\n&lt;/book&gt;\n\nAs you can see, the XML tags already help us (and the computer) identify different text components, such as the title, the author, headers and paragraphs. XML documents are a typical example of semi-structured data.\nAnother frequently used semi-structured format is JSON. It is a name-value pair format. We can use it to structure our text by identifying the different parts (the names) and their content (the values).\n\n{\n  \"title\": \"A data story\",\n  \"author\": \"Philippe Mongeon\",\n  \"Chapter 1\": \"Once upon a time a short text was created by a random internet user and abandoned in a sea of unstructured data.\",\n  \"Chapter 2\": \"An algorithm passed by and decided to add xml tags to the text.\",\n  \"Chapter 3\": \"A data manager stumbled upon the data, thought it would be relevant for the database users, and so she decided to add it to her database. The end.\"\n}\n\nNotice how I included the Chapter number in the tags in the JSON example above? That was an arbitrary choice, and I could have chosen another structure for my file, such as this one:\n\n{\n  \"title\": \"A data story\",\n  \"author\": \"Philippe Mongeon\",\n  \"Chapters\": [\n      {\n        \"title\": \"Chapter 1\",\n        \"content\": \"Once upon a time a short text was created by a random internet user and abandoned in a sea of unstructured data.\"\n      }\n      {\n        \"title\": \"Chapter 2\",\n        \"content\": \"An algorithm passed by and decided to add xml tags to the text.\"\n      }\n      {\n        \"title\": \"Chapter 3\",\n        \"content\": \"A data manager stumbled upon the data, thought it would be relevant for the database users, and so she decided to add it to her database. The end.\"\n      }\n  ]\n}\n\nThis structure (or semi-structure) is just as good as the previous one. That flexibility is one of the reasons why semi-structured data formats and databases are growing in popularity. Because the data structure is included directly in the document, the users do not need to follow a predefined structure and use predefined tags to store their data in the database. This means that, theoretically, every record could have a different structure, which would be fine, but most likely not optimal for an end user trying to make sense of a dataset and extract useful knowledge from it.\n\n\n2.1.3 Structured data\nStructured data and databases are typically tabular (consisting of rows and columns, like an Excel spreadsheet). Each row is a record or entry, and each column is a field (also called features, or variable). The structure is part of the database design (and not each database entry, as in the semi-structured format), and so every new record has to follow that same format. For example, here is the same work of art in a structured format.\n\n\n\nExample of structured data\n\n\nTitle\nAuthor\nSection\nContent\n\n\n\n\nA data story\nPhilippe Mongeon\nChapter 1\nOnce upon a time a data was created by a random internet user and abandoned in a see of unstructured data.\n\n\nA data story\nPhilippe Mongeon\nChapter 2\nAn algorithm passed by and decided to add xml tags to the text.\n\n\nA data story\nPhilippe Mongeon\nChapter 3\nA data manager stumbled upon the data, thought it would be relevant for the database users, and so she decided to add it to include it in her database. The end.\n\n\n\n\n\n\n\nHere, the structure is determined by the columns I chose for my table (title, author, section, and content). But, again, this was an arbitrary choice, and I could have chosen a completely different structure (although the realm of possibilities is somewhat limited by the data and by common sense). The point is that there is rarely an absolute best way of structuring data, and the best structure is the one that best suits the needs of the users. Sometimes the same data may be duplicated and structured differently to suit different users and uses. There are however data structure standards that you should follow when using certain established data models and DBMS, but these are beyond the scope of this course.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "ch2.html#the-power-of-structure",
    "href": "ch2.html#the-power-of-structure",
    "title": "2  Data structures",
    "section": "2.2 The power of structure",
    "text": "2.2 The power of structure\nThe more structured a dataset is, the more easily it will be to use to extract knowledge by an end-user. Unfortunately, data is not always found in easily exploitable structures and will often require some cleaning and processing. There is a well-known saying that data scientists spend 80% of their time cleaning their data. Clean data that fits a clear purpose is actually extremely easy to analyze… the problem is that the purpose isn’t always clear in the end of the user, and data does not always come in the exact format needed for that purpose.\nIn this course, we will not deal with extremely complex structures and very messy data, but we will get comfortable with a few basic data processing tasks in Excel that will help you make that most of the imperfect data that you are likely to encounter in your professional life.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "ch2.html#practice",
    "href": "ch2.html#practice",
    "title": "2  Data structures",
    "section": "2.3 Practice",
    "text": "2.3 Practice\nSince this course is intended to be accessible to students with little to no experience working with data, this week is dedicated to developing or polishing your Microsoft Excel skills.\nWhile Excel has some data management capabilities, that’s not what it is designed and most commonly used for. You can still think of it as a very rudimentary form of DBMS. However, Excel is a flexible tool that you can use to quickly explore, manipulate and structure data before creating those structures in an actual DBMS. Excel will also always be relevant in your data management workflow because it remains one of the most accessible ways to work with data for downstream processes (before data gets stored in a DMBS) and upstream processes (after the data is retrieved from the DBMS).\n\n2.3.0.1 Formatting exercise\n\nDownload the Easy_Excel.xlsx file (provided by Julie Marcoux, a data librarian at the Killam Library)\nFollow the instructions in the instructions sheet.\nThe exercise sheet contains the data to format.\nAt the end of the exercise, your table should look similar to the one found in the Results sheet.\nThe Useful functions sheet contains tips on using some Excel formatting functions.\n\n\n\n2.3.0.2 Analyzing exercise\n\nWatch the demo on pivot tables in Excel below.\nCreate a pivot table to explore the data in the _The exercise sheet of the Easy_Excel.xlsx used for the first exercise.\nExplore ways of structuring your pivot table and combining and filtering columns.\nWhich publisher has the most books in the dataset?\nOn average, are cloth-bound books more or less expensive than paper-bound books?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "ch5.html#data-integrity-validation-and-cleaning",
    "href": "ch5.html#data-integrity-validation-and-cleaning",
    "title": "5  Assessing data",
    "section": "5.2 Data Integrity, Validation, and Cleaning",
    "text": "5.2 Data Integrity, Validation, and Cleaning\nWhen data is created using an information system, such as Shopify, steps are often taken to ensure its integrity. For example, user identities are usually authenticated through secure web infrastructure, a username, a password, or another authentication process. Today, user identities are often managed through commercial identity access management cloud infrastructure, such as Okta’s Auth0 or Ping Identity. These operate behind the scenes in many enterprise information systems [14]. Identity access management software helps ensure that data generated by the information systems accurately reflect a user’s activity. Similarly, if a cloud information system is configured correctly, it usually ensures data integrity with secure exchange protocols. Even if the right people are using a system and the system is secure, data can still be invalid. Data entered through a web form, for example, can be false. A user might also mistype an address or could, for example, input an avocado price of $10.10 instead of the $1.10 intended. A city field may be configured incorrectly to read Lüneburg (i.e., Germany) instead of Lunenberg (i.e., Canada). Systems can be configured to conduct automatic validation, though this can’t prevent all errors. In larger organizations that use more complex data resources (e.g., databases, repositories), data engineers and data analysts spend a lot of their time ensuring the integrity of data, so that the organization’s information is not flawed.\nThis is why it is very important to check and validate data before analyzing it. Some tangible steps that you can take to validate your data include:\n\nIdentifying outliers; analyze the distribution to see if there are data that are way different from the others.\nFinding inconsistent or missing values; investigate the data manually or with an automated check to see if anything is missing.\nHave consistent encoding and formatting; determine whether there are inconsistencies in the way that the data are recorded.\nCommon sense checks; ask yourself whether the values make sense based on the data description or metadata provided. Once clean, it is a best practice to save cleaned data and keep it separate from raw data. Be sure to keep a record of how you created a clean dataset, or it will be very difficult to translate information about the cleaning to another person.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing data</span>"
    ]
  },
  {
    "objectID": "ch5.html#open-data-sources-and-data-credibility",
    "href": "ch5.html#open-data-sources-and-data-credibility",
    "title": "5  Assessing data",
    "section": "5.3 Open Data Sources and Data Credibility",
    "text": "5.3 Open Data Sources and Data Credibility\nMany organizations that generate data keep their data securely controlled and inaccessible to the public. There are many reasons for this, but one of the biggest is that knowledge is a strategic resource that gives companies a competitive advantage [15]. Organizations may also wish to secure sensitive data to help protect their stakeholders’ privacy.\nYet, there is also a widespread movement to release data publicly. Often referred to as the open data movement [16], many advocates believe that data should be publicly available to help advance education, accountability, and transparency. Throughout the 2010s and 2020s, governments have increasingly released data through their open data portals, while other organizations have often contributed their data to open data competitions such as those offered by Kaggle [17]. These have offered new opportunities for people to learn data management, data science, and machine learning skills.\nHowever, some data sources, even those offered through government data portals, may contain errors, omissions, or may be intentionally misleading. When working with data, it is essential to think critically about the source of the data and its credibility. One way to do this is to apply the CRAAP test, which is often used to ensure the credibility of information [18]. Before engaging with a dataset, ask yourself:\n\nIs this data current? Is it outdated, or has it been revised?\nHow relevant is the data? Is the data well-suited to your analytical task? Is it too summarized, or overly verbose?\nDoes the source of the data have a clear authority? Is there a clear and qualified author responsible for ensuring the data’s quality?\nIs the data accurate? Is the source of the data likely to be biased?\nWhy does this information exist? Does the data have a clear purpose?\n\nBy answering these questions, we can mitigate the risk of generating misinformation or suboptimal decisions with inadequate data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing data</span>"
    ]
  }
]